<html>
<head>
<title>Talismane User's Manual</title>
<style>
   body{
     counter-reset: h1Counter;
     font-family: serif;
	}
	/*
  h1{counter-reset: h2Counter}
   h2{counter-reset: h3Counter}
   h3{counter-reset: h4Counter}
   h4{counter-reset: h5Counter}
   
   h1:before{
     counter-increment: h1Counter;
     content: counter(h1Counter) " ";
   }
   h2:before{
     counter-increment: h2Counter;
     content: counter(h1Counter) "." counter(h2Counter) " ";
   }
   h3:before{
     counter-increment: h3Counter;
     content: counter(h1Counter) "." counter(h2Counter) "." counter(h3Counter) " ";
   }
   h4:before{
     counter-increment: h4Counter;
     content: counter(h1Counter) "." counter(h2Counter) "." counter(h3Counter) "." counter(h4Counter) " ";
   }
   h5:before{
     counter-increment: h5Counter;
     content: counter(h1Counter) "." counter(h2Counter) "." counter(h3Counter) "." counter(h4Counter) "." counter(h5Counter) " ";
   }
   */
   
   .info {
	  border-style: outset;
	  border-width: 3px;
	  border-color: #993300;
	  background-color: #FFFF99;
	  page-break-inside:avoid !important;
	}
	
	.alert {
	  border-style: outset;
	  border-width: 3px;
	  border-color: #993300;
	  background-color: #FF9966;
	  page-break-inside:avoid !important;
	  }
	  
	.reference {
	  font-family: monospace;
	  border-style: solid;
	  border-width: 1px;
	  padding: 3px;
	}
	pre {
	 white-space: pre-wrap;       /* css-3 */
	 white-space: -moz-pre-wrap;  /* Mozilla, since 1999 */
	 white-space: -pre-wrap;      /* Opera 4-6 */
	 white-space: -o-pre-wrap;    /* Opera 7 */
	 word-wrap: break-word;       /* Internet Explorer 5.5+ */
	}
	
	pre.shell {
	  background-color : black;
	  color: white;
	  padding-left: 5px;
	  page-break-inside:avoid !important;
	}
	
	.sans-serif {
	  font-family: monospace;
	}
	
	.smallcaps {
	  font-variant:small-caps;
	}
	
	.imageCaption {
	  font-size: small;
	  font-style: italic;
	}
	
	table {
	page-break-inside:avoid !important;
	}
	
	.mainTitle {
	  font-size: xx-large;
	  font-weight: bold;
	  text-align: center;
	  }
	  
	 .tocTitle {
	  font-size: x-large;
	  font-weight: bold;
	  text-align: left;
	  }
	  
  div#toc ul {
	  list-style-type: none;
	}
   </style>
<script src="toc.js" type="text/javascript"></script>
</head>
<body>
<div class="mainTitle">Talismane User's Manual</div>
<div align="center">Version 1.5.3b</div>
<br/>
<div align="center"><b>Assaf Urieli</b> - CLLE-ERSS: CNRS & University of Toulouse. <tt>assaf.urieli@univ-tlse2.fr</tt></div>
<div align="center"><b>Jean-Philippe Fauconnier</b> - IRIT: CNRS & University of Toulouse. <tt>jean-philippe.fauconnier@irit.fr</tt></div>
<br/>
<div class="tocTitle">Table of Contents</div>
<div id="toc"></div>

<h1>The Talismane Suite</h1>

<h2>Introduction</h2>
<p>Talismane is a statistical transition-based dependency parser for natural languages written in Java. It was developed within the framework of <a href="http://w3.erss.univ-tlse2.fr/textes/pagespersos/urieli">Assaf Urieli</a>'s doctoral thesis at the <a href="http://w3.erss.univ-tlse2.fr/">CLLE-ERSS</a> laboratory in the Université de Toulouse, France, under the direction of <a href="http://w3.erss.univ-tlse2.fr/textes/pagespersos/tanguy">Ludovic Tanguy</a>. Many aspects of Talismane's behaviour can be tuned via the available configuration parameters. Furthermore, Talismane is based on an open, modular architecture, enabling a more advanced user to easily replace and/or extend the various modules, and, if required, to explore and modify the source code. It is distributed under an <a href="http://www.gnu.org/licenses/agpl.html">AGPL open-source license</a> in order to encourage its non-commercial redistribution and adaptation.</p>
<p>Talismane stands for "Traitement Automatique des Langues par Inférence Statistique Moyennant l'Annotation de Nombreux Exemples" in French, or "Tool for the Analysis of Language, Inferring Statistical Models from the Annotation of Numerous Examples" in English.</p>
<p>Talismane should be considered as a <b>framework</b> which could potentially be adapted to any natural language. The present document presents a default implementation of Talismane for French, which uses the <a href="#ftb">French Treebank</a> as a training corpus, the <a href="#lefff">Lefff</a> as a lexicon, and a <a href="#tagset">tagset</a>, <a href="#features">features</a> and <a href="#rules">rules</a> specific to French.</p>
<p>Talismane is a statistical toolset and makes heavy use of a probablistic classifier (currently either Linear SVM, Maximum Entropy or Perceptrons). Linguistic knowledge is incorporated into the system via the selection of features and rules specific to the language being processed.</p>
<p>The portability offered by Java enables Talismane to function on most operating systems, including Linux, Unix, MacOS and Windows.</p>
<p>Talismane consists of four main modules which transform a raw unannotated text into a series of syntax dependency trees. It also contains a number of pre-processing and post-processing filters to manage and transform input and output. In sequence, these modules are:</p>
<ul>
<li><b>sentence detector</b></li>
<li><b>tokeniser</b></li>
<li><b>pos-tagger</b></li>
<li><b>syntax parser</b></li>
</ul>
<p>Each of the modules in the processing chain can be used independently if desired.</p>
<div align="center"><img src="./pics/TalismaneOverview.png" alt="Talismane Processing Chain"/></div>
<div class="imageCaption" align="center">Talismane Processing Chain</div>

<h2>Sentence Detector</h2>
<p>The Talismane Sentence Detector takes a stream of raw unannotated text as input, and breaks it up into sentences as output. It is based on a statistical approach, where each potential sentence boundary character is tested and marked as either a true or a false boundary.</p>
<p>In the default implementation for French, the Sentence Detector was trained using the <a href="#ftb">French Treebank</a> [Abeillé and Clément, 2003].</p>
<p>More details on Talismane Sentence Detector configuration and usage can be found <a href="#sentenceDetector">here</a>.</p>

<h2>Tokeniser</h2>
<p>A <b>token</b> is a single syntaxic unit. While the vast majority of tokens are simply words, there can be exceptions: for example, in English, the two words "of course" form a single token, while in French, the single word "au" is composed of two tokens, "à" and "le". The Talismane Tokeniser takes a sentence as input, and transforms it into a sequence of tokens as output. It combines a pattern-based approach with a statistical approach. The patterns are language specific, and ensure that only areas where any doubt exists concerning the tokenisation are actually tested statistically.</p>
<p>The default French implementation thus comes with a list of patterns for French. The statistical model was trained using a modified version of the <a href="#ftb">French Treebank</a> [Abeillé and Clément, 2003]. This modified version  includes specific tokenising decisions about each pattern-matching sequence in the treebank, which don't necessarily match the French Treebank's initial decision on what consists in a "compound word".</p>
<p>More details on Talismane Tokeniser configuration and usage can be found <a href="#tokeniser">here</a>.</p>

<h2>Pos-Tagger</h2>
<p>The Talismane Pos-Tagger takes a sequence of tokens (forming a sentence) as input, and adds a part-of-speech tag to each of these tokens as output, along with some additional sub-specified information (lemma, morpho-syntaxic details). It combines a statistical approach based on the use of a probabilistic classifier with a linguistic approach via the features used to train the classifier and the symbolic rules used to override the classifier.</p>
<p>In the default French implementation, the Pos-Tagger uses a <a href="#tagset">tagset</a> based on the one proposed by [Crabbé and Candito 2008] in their work on statistical dependency parsing for French, with some minor alterations. However, Talismane's architecture makes it fairly straightforward to replace this tagset with another tagset, as long as a statistical model is retrained for the new tagset.</p>
<p>After pos-tagging, the Pos-Tagger assigns lemmas and detailed morpho-syntaxic information. In the current version, this information is sub-specified, meaning it is only provided if it is found in the lexicon, in this case the <a href="#lefff">Lefff</a> [Sagot et al, 2006]. The <tt>Lefff</tt> can be replaced and/or extended by other lexical resources.</p>
<p>In the default French implementation, the Pos-Tagger was trained using the <a href="#ftb">French Treebank</a> [Abeillé and Clément, 2003].</p>
<p>More details on Talismane Pos-Tagger configuration and usage can be found <a href="#posTagger">here</a>.</p>

<h2>Syntax parser</h2>
<p>The Talismane Syntax Parser is a transition-based statistical dependency parser, based on the work described by [Nivre, 2008], but modified in several significant ways.</p>
<p>It takes a sequence of pos-tagged tokens (with additional sub-specified lemmas and morpho-syntaxic information) as input, and generates a dependency tree as output, or a dependency forest if any nodes were left unconnected.</p>
<p>The default French Implmentation was trained using the <a href="#ftb">French Treebank</a> [Abeillé and Clément, 2003], automatically converted to a projective dependency treebank by [Candito et al., 2010A].</p>
<p>More details on Talismane Parser configuration and usage can be found <a href="#parser">here</a>.</p>

<h2>Licenses</h2>

<h3>The Talismane suite</h3>
<p>Talismane is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.</p>
<p>Talismane is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.</p>
<p>You should have received a copy of the GNU Affero General Public License
along with Talismane.  If not, see <a href="http://www.gnu.org/licenses/">this page</a>.</p>

<h3>The documentation</h3>
<p>The present document is distributed udner the <a href="http://creativecommons.org/licenses/by-nc/3.0/">Creative Commons BY-NC 3.0 license</a>. 
You are free to copy, distribute and transmit the work, and to adapt the work under the conditions that:</p>
<li>You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work).</li>
<li>You may not use this work for commercial purposes.</li>
<img src="./pics/logo.png" alt="Logo CC BY-NC 3.0"/>

<h2>How to cite us?</h2>
<p>To cite us, please use the following notation:</p>
<div class="reference">Urieli, A. and Fauconnier, J., <i>Talismane User Manual</i>, Toulouse, CLLE-ERSS, 2012.</div>

<h2>Typographic conventions</h2>
<p>Various typographic conventions will be used in the present document.</p>
<li>Since Talismane is not currently available with a graphical user interface, it needs to be called from a command prompt (or shell). Commands will follow a unix standard, although most commands (especially Java commands) are immediately transposable to a Windows environment. These commands are shown as follows:</li>
<pre class="shell">
user@computer:~$ command</pre>
<li>Complementary information regarding a piece of functionality or an underlying principle is shown as follows:</li>
<div class="info">Complementary information</div>
<li>Any elements which are critical to the proper functioning of the Talismane suite are shown as follows:</li>
<div class="alert">Critical information</div>

<h1>Basic usage</h1>

<h2>Installation</h2>
<li>Talismane requires Java version 6 or higher. Various versions of the Java JDK are available <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">here</a>.
In order to check your Java version, type:</li>
<pre class="shell">
java -version</pre>

<h3>From a precompiled executable</h3>
<p>The simplest way to use Talismane is by downloading the pre-compiled executable. The latest stable Talismane suite is on the CLLE-ERSS REDAC site at: <a href="http://redac.univ-tlse2.fr/applications/talismane.html">http://redac.univ-tlse2.fr/applications/talismane.html</a>.</p>
<li>Assuming you want to use Talismane to pos-tag or parse French texts, simply download the latest version of the default French implementation, <tt>talismane-fr-<i>version</i>-allDeps.jar</tt> from the website above.</li>
<li>Place the jar file in a directory of your choice, here <tt>Talismane</tt>:</li>
<pre class="shell">
mkdir Talismane
mv /path/talismane-fr-<i>version</i>-allDeps.jar /path/Talismane/
cd Talismane</pre>
<li>In the same directory create a file, <tt>test.txt</tt>, containing the text "Les poules du couvent couvent."</li>
<li>Attempt to pos-tag this file using <b>talismane-fr-<i>version</i>-allDeps.jar</b>:</li>
<pre class="shell">
java -Xmx1024M -jar talismane-fr-<i>version</i>-allDeps.jar command=analyse endModule=postag inFile=test.txt outFile=output.txt</pre>
<li>Check that the file <tt>output.txt</tt> was generated, and that is contains the pos-tagged text.</li>

<h3>From the sources using Git</h3>
<p>The Talismane suite is available on the open-source project management platform <tt>GitHub</tt>. More advanced users with some knowledge of Java may wish to compile the source code directly from Git.</p>
<li>Install the version control system <tt>Git</tt> on your system. Download Git at <a href="http://git-scm.com/">http://git-scm.com/</a> and follow the installation instructions for your operating system.</li>
<li>Create a directory in which to install the project, here <tt>Talismane</tt>, and then, clone the Talismane source code from gitHub at <a href="https://github.com/urieli/talismane">https://github.com/urieli/talismane</a>.</li>
<pre class="shell">
mkdir Talismane
cd Talismane
git clone https://github.com/urieli/Talismane</pre>
<li>Create an executable <b>.jar</b> file from the Talismane source code using <tt>Ant</tt>. First of all, make sure you have <tt>Ant</tt> on your system:</li>
<pre class="shell">
ant -version</pre>
<li>Next, if <tt>Ant</tt> is correctly installed, go to the Talismane folder and generate the jar. On Linux, issue the following command:</li>
<pre class="shell">
ant -lib ant/</pre>
<li>On Windows, issue the following command:</li>
<pre class="shell">
ant -lib=ant</pre>
<li>Check that the <b>talismane-fr-<i>version</i>.jar</b> file has been generated in the <tt>dist</tt> directory.</li>
<pre class="shell">
cd dist
ls</pre>
<li>In the <tt>dist</tt> directory, create a file, <tt>test.txt</tt>, containing the text "Les poules du couvent couvent."</li>
<li>Attempt to pos-tag this file using <b>talismane-fr-<i>version</i>-allDeps.jar</b>:</li>
<pre class="shell">
java -Xmx1024M -jar talismane-fr-<i>version</i>-allDeps.jar command=analyse endModule=postag inFile=test.txt outFile=output.txt</pre>
<li>Check that the file <tt>output.txt</tt> was generated in the <tt>dist</tt> directory, and that is contains the pos-tagged text.</li>

<h2>Quick start</h2>
<p>This section helps you get up and running quickly with Talimane's default French implementation, which is contained in the file <tt>talismane-fr-<i>version</i>.jar</tt> (where <i>version</i> in the shell commands below should be replaced by the version you downloaded).</p>

<h3>Parsing a sentence</h3>
<p>First, you will parse a French sentence.</p>
<p>Talismane can either work with input and/or output files, or directly from the shell using the standard streams <a href="http://en.wikipedia.org/wiki/Standard_streams">STDIN and STDOUT</a>.</p>
<pre class="shell">
echo "Les poules du couvent couvent." | java -Xmx1024M -jar talismane-fr-<i>version</i>-allDeps.jar command=analyse
1       Les     les     DET     det     n=p     2       det     _       _
2       poules  poule   NC      nc      g=f|n=p 5       suj     _       _
3       du      de      P+D     P+D     g=m|n=s 2       dep     _       _
4       couvent couvent NC      nc      g=m|n=s 3       obj     _       _
5       couvent couver  V       v       n=p|p=3|t=pst   0       root    _      _
6       .       .       PONCT   PONCT   _       5       ponct   _       _</pre>

<div class="alert">
<b>Memory management</b>: in order to increase execution speed, Talismane first loads all lexical resources into memory. For the default French implementation, at least <tt>1Gb</tt> of runtime memory needs to be allocated to the java executable. Hence the addition of the parameter <b>-Xmx1024M</b>.
</div>

<h4>stdin and stdout</h4>
<p>Whether or not Talismane uses stdin and stdout depends on the inclusion of the arguments <b>inFile</b> and <b>outFile</b>.</p>
<p>The following command takes a text file as entry, <tt>test.txt</tt>, and outputs the result into the current shell.</p>
<pre class="shell">
java -Xmx1024M -jar talismane-fr-<i>version</i>-allDeps.jar command=analyse inFile=test.txt </pre>
<p>Conversely, the following command takes the value of <tt>echo</tt> command as input, processes it, and stores the result in <tt>output.txt</tt>.</p>
<pre class="shell">
echo "Les poules du couvent couvent." | java -Xmx1024M -jar talismane-fr-<i>version</i>-allDeps.jar command=analyse outFile=output.txt</pre>
<p>This command has an identical result to the previous command, but uses <a href="http://en.wikibooks.org/wiki/Bourne_Shell_Scripting/Files_and_streams">stream redirection</a> to write the output to the file <tt>output.txt</tt>:</p>
<pre class="shell">
echo "Les poules du couvent couvent." | java -Xmx1024M -jar talismane-fr-<i>version</i>-allDeps.jar command=analyse > output.txt</pre>
<p>Finally, this command reads the sentences from one file, and writes the parsed result to another:</p>
<pre class="shell">
java -Xmx1024M -jar talismane-fr-<i>version</i>-allDeps.jar command=analyse inFile=test.txt outFile=output.txt</pre>

<div class="alert">
<b>File management</b>: if inFile is specified, it must actually exist on the file system. On the other hand, outFile need not exist. If it does exist, the existing outFile will be deleted, and replaced by a new one.
</div>

<a name="unixtools"></a>
<h4>Integration with Unix Tools</h4>

<p>By using the standard input and output streams, Talismane integrates well with the basic Unix tools, enabling the user to easily process the output data.</p>

<p>For example, the following command allows us to recuperate the <b>2nd</b> column (the token) and the <b>4th</b> column (the postag) from the parser's default output:</p>
<pre class="shell">
echo "Les poules du couvent couvent." | java -Xmx1024M -jar talismane-fr-<i>version</i>-allDeps.jar command=analyse | cut -f2,4
Les       DET
poules    NC
du        P+D
couvent   NC
couvent   V
.         PONCT</pre>

<p>While this command replaces <tt>DET</tt> by <tt>DETERMINANT</tt>.</p>
<pre class="shell">
echo "Les poules du couvent couvent." | java -Xmx1024M -jar talismane-fr-<i>version</i>-allDeps.jar command=analyse | perl -pe 's/DET/DETERMINANT/g</pre>
<p>Finally, this command gives the total number of tokens and sentences processed:</p>
<pre class="shell">
echo "Les poules du couvent couvent." | java -Xmx1024M -jar talismane-fr-<i>version</i>-allDeps.jar command=analyse | perl -pe 'END{print "tokens:$i\nsentences:$p\n"} if (/^\r$/) {$p++} else {$i++}'</pre>

<a name="arguments"></a>
<h3>Arguments</h3>
<p>Talismane requires several arguments in order to function correctly. If Talismane is called without any arguments, it will display a concise online help, simply listing the required argments.</p>
<p>Talismane provides several basic modes of operation:<p>
<li><tt>command=analyse</tt>: processes raw text and generates an annotated result. This is the mode use in all of the examples above.</li>
<li><tt>command=evaluate</tt>: allows the user to evaluate Talismane's performance on a text that has been manually annotated. This advanced usage is described in the section on <a href="#auto-evaluation">auto-evaluation</a>, below.</li>
<li><tt>command=process</tt>: simply use Talismane to read an annotated file and process it in a way defined by sub-classes. By default, the file will simply be outputted again. The input/output format can be changed to transform from one format to another.</li>
<li><tt>command=compare</tt>: similar to the <tt>evaluate</tt> command, but uses one annotated file as a "reference", and another annotated file as an "evaluation". This advanced usage is described in the section on <a href="#auto-evaluation">auto-evaluation</a>, below.</li>
<p>In all operating modes, various additional arguments can be used to modify Talismane's behaviour. Details on these arguments are provided in the following sections.</p>

<h4>Arguments shared by all modes</h4>
<li><b>inFile</b>: the input file to be processed. If not specified, <tt>stdin</tt> will be used as input.</li>
<li><b>startModule/endModule</b>: the variables <tt>startModule</tt> and <tt>endModule</tt> designate the sequence of operations to perform on the input data. The options are <tt>sentence</tt>, <tt>tokenise</tt>, <tt>postag</tt> and <tt>parse</tt>. For example, if you start with text that has already been separated into one sentence per line, and you want to tokenise and pos-tag these sentences, you would choose <tt>startModule=tokenise</tt> and <tt>endModule=postag</tt>. By default, the <tt>startModule</tt> is <tt>sentence</tt> (break up raw text into sentences) and the <tt>endModule</tt> is <tt>parse</tt> (generate a dependency tree for each sentence).
Note that, if you choose a start module other than <tt>sentence</tt>, a certain default data format is assumed for the input file. This default data format can be overridden by using the optional <tt>inputPatternFile</tt> argument described in the section on <a href="#inputTemplate">input templates</a>.</li>
<li><b>module</b>: A shortcut for indicating that the <tt>startModule</tt> and <tt>endModule</tt> are the same. Often used for evaluation, when evaluating a single module only.</li>
<li><b>encoding</b>: the encoding used to process the input and generate the output. By default, Talismane uses the default encoding for your system, but any Java encoding string can be used, including <tt>UTF-8</tt>, <tt>LATIN1</tt>, <tt>UTF-16</tt>, <tt>UTF-32</tt>. Incompatible with the parameters <tt>inputEncoding</tt> and <tt>outputEncoding</tt>.</li>
<li><b>inputEncoding</b>: like encoding, but specifies the encoding for the input only. Allows a different encoding for input and output.</li>
<li><b>outputEncoding</b>: like encoding, but specifies the encoding for the output only. Allows a different encoding for input and output.</li>
<li><b>inputPatternFile</b>: this variable allows you to override the default input data format assumed for each module, by providing template file describing how the input data is formatted. More information can be found in the section on <a href="#inputTemplate">input templates</a>.</li>
<li><b>inputPattern</b>: like <tt>inputPatternFile</tt>, but the pattern is specified directly (between double-quotes) instead of being specified in a separate file.</li>
<li><b>logStats</b>: if "true", log the total time (in milliseconds) to a CSV file in the output directory.</li>
<li><b>logConfigFile</b>: allows the user to specify a <a href="http://logging.apache.org/log4j/2.x/manual/configuration.html">log4j configuration file</a> that will override the default log4j config file included in the JARs.</li>
<li><b>performanceConfigFile</b>: allows the user to monitor performance, using a performance configuration file. See <a href="#performanceMonitoring">performance monitoring</a> below.</li>

<h4>Arguments shared by analysis and process mode</h4>
<li><b>outFile</b>: the path to the output file. If not specified, <tt>stdout</tt> will be used for output. If the file already exists, it will first be deleted.</li>
<li><b>template</b>: designates the path of a <a href="http://freemarker.sourceforge.net/">Freemarker</a> template file to be used for formatting Talismane's output. If not provided, there is a default internal template for each endModule type. More details can be found in the section on <a href="#outputTemplate">output templates</a>.</li>
<li><b>builtInTemplate</b>: one of the available built-in templates for output - currently the only available template is <tt>with_location</tt>, which adds token location information (file name, row, column) to the standard output templates.</li>

<h4>Arguments used by analysis mode only</h4>
<li><b>textFilters</b>: a path to a file containing various filters (generally regular-expression based) used to mark sections of the input data for processing or skipping. Other marks are available as well (e.g. output as raw text, insert a sentence break, etc.). More information can be found in the section on <a href="#filters">text filters</a>.</li>
<li><b>tokenFilters</b>: a path to a file containing various filters (generally regular-expression based) used to mark parts of the input data that should be processed as single tokens (e.g. URLs). More information can be found in the section on <a href="#tokenFilters">token filters</a>.</li>
<li><b>processByDefault</b>: if true, the input file/stream will be processed by default (and text marker filters can be used to indicate sections to be skipped). If false, the input file/stream will NOT be processed by default (and text marker filters cna be used to indicate sections to be processed). More details on text maker filters can be found in the section on <a href="#filters">text maker filters</a>. The default is "true".</li>
<li><b>newline</b>: how to handle newlines in the document. Either <tt>SENTENCE_BREAK</tt>, <tt>SPACE</tt> or <tt>NONE</tt>. If <tt>SENTENCE_BREAK</tt>, newlines are skipped and replaced by a sentence break (the default behaviour). If <tt>SPACE</tt>, newlines are skipped and replaced by a space. If <tt>NONE</tt>, Talismane does nothing to newlines - the user will have to handle them via text marker filters. The default is <tt>SENTENCE_BREAK</tt>.</li>
<li><b>fileName</b>: the file name to be tagged against each token during analysis - especially useful if <tt>stdin</tt> is used instead of <tt>inFile</tt>.</li>
<li><b>endBlockCharCode</b>: a character (typically non-printing) which will mark a stop in the input stream and set-off analysis. Expects the decimal integer Unicode code of the character. The default value is the form-feed character (code=12).</li>
<li><b>blockSize</b>: the minimum block size, in characters, to process by the sentence detector. Filters are applied to a concatenation of the previous block, the current block,
and the next block prior to sentence detection, in order to ensure that a filter which crosses block boundaries is correctly applied.
It is not legal to have a filter which matches text greater than a block size, since this could result in a filter which stops analysis but doesn't start it again correctly,
or vice versa. Block size can be increased if really big filters are really required. Default is 1000.</li>

<h4>Arguments used by evaluate mode only</h4>
<li><b>crossValidationSize</b>: the number of parts to use for a cross-validation.</li>
<li><b>includeIndex</b>: if crossValidationSize&gt;0, the index of the sentence to include, if we start counting at 0 and reset at zero after reaching the crossValidationSize. Typically used for evaluation only.</li>
<li><b>excludeIndex</b>:  if crossValidationSize&gt;0, the index of the sentence to exclude, if we start counting at 0 and reset at zero after reaching the crossValidationSize. Typically used for training only.</li>

<h4>Arguments shared by evaluate and compare modes</h4>
<li><b>outDir</b>: The path to the output directory. When auto-evaluation is complete, this directory will contain two files: <tt><i>inFileName</i>.fscores.csv</tt>, a confusion matrix showing the error counts for each possible classification, and <tt><i>inFileName</i>_sentences.csv</tt>, containing the full set of evaluated sentences. More details are available in the section on <a href="#auto-evaluation-output">auto-evaluation output</a>.</li>
<li><b>includeDistanceFScores</b>: If "true", when evaluating the parser, will output a CSV containing the accuracy at each attachment distance.</li>

<h4>Arguments shared by analyse and evaluate modes</h4>
<li><b>includeDetails</b>: enables the user to obtain a very detailed analysis on how Talismane obtained the results it displays. More information can be found in the section on <a href="#includeDetails">detailed analysis output</a>.</li>
<li><b>posTaggerRules</b>: a path to a file containing a set of rules that systematically override decisions made by the pos-tagger's statistical model. More information can be found in the section on <a href="#rules">pos-tagger rules</a>.</li>
<li><b>parserRules</b>: a path to a file containing a set of rules that systematically override decisions made by the parser's statistical model. More information can be found in the section on <a href="#rules">parser rules</a>.</li>
<li><b>sentenceModel, tokeniserModel, posTaggerModel, parserModel</b>: enables the user to override the default statistical model included with the default implementation, with another model. This is very advanced functionality, as it assumes the user has been able to train the model using Talismane's training interface. More information can be found in the section on <a href="#training">training a new model</a>.</li>
<li><b>posTagSet</b>: enables the user to override the default tagset used by Talismane. For very advanced usage only, as changing the tagset implies using a statistical model that has been trained against the new tagset using Talismane's training interface.</li>
<li><b>beamWidth</b>: the beam width to apply to the pos-tagger and parser. The default is 1.</li>
<li><b>tokeniserBeamWidth</b>: the beam width to apply to the tokeniser. The default is 1.</li>
<li><b>posTaggerBeamWidth</b>: the beam width to apply to the pos-tagger, if separate beam width required for pos-tagger and parser. The default is 1.</li>
<li><b>parserBeamWidth</b>: the beam width to apply to the parser, if separate beam width required for pos-tagger and parser. The default is 1.</li>
<li><b>propagateBeam</b>: if true, the entire output beam of the pos-tagger is used as the input for the parser. If false, only the best option is sent to the parser. Default is true.</li>
<li><b>propagateTokeniserBeam</b>: if true, the entire output beam of the tokeniser is used as the input for the pos-tagger. If false, only the best option is sent to the pos-tagger. Default is false.</li>
<li><b>maxParseAnalysisTime</b>: the maximum amount of time the parser will spend analysing any single sentence, in seconds. If it exceeds this time, the parser will return a partial analysis, or a "dependency forest", where certain nodes are left unattached (no governor). A value of 0 indicates that there is no maximum time - the parser will always continue until sentence analysis is complete. The default value is 60.</li>
<li><b>minFreeMemory</b>: The minimum amount of remaining free memory to continue a parse, in kilobytes.<br/>
Will be ignored is set to 0.<br/>
If analysis jumps out because of free memory descends below this limit,
there will be a parse-forest instead of a parse-tree, with several nodes left unattached. Default is 64k.</li>

<h4>Arguments used by the compare module only</h4>
<li><b>evaluationFile</b>: wherease <tt>inFile</tt> refers to the reference file, <tt>evaluationFile</tt> refers to the file to be evaluated compared to this reference</li>
<li><b>evaluationPatternFile</b>: like <tt>inputPatternFile</tt> for the evaluation file, if a different pattern is required. Default: get value from <tt>inputPatternFile</tt>.</li>
<li><b>evaluationPattern</b>: like <tt>inputPattern</tt> for the evaluation file, if a different pattern is required. Default: get value from <tt>inputPattern</tt>.</li>

<h2>Default configuration</h2>
<h3>Introduction</h3>

<p>The default French implementation of Talismane is setup with certain configuration options, and uses a set of default statistical models.</p>
<p>This section describes these default options and models, and explains which areas can be modified easily, and which require more advanced usage.</p>

<h3>Statisical models, features and rules</h3>
<p>In order to provide an off-the-shelf solution, each module in the Talismane suite comes with a statistical model specifically trained for the task in question (sentence detection, tokenising, pos-tagging, parsing). Terminologicially, these models were built using <b>machine learning</b> techniques for automatic statistical classification. Thus, each of the tasks listed above has been transformed into a classification task. The case of pos-tagging is the most emblamatic: we take a given word in a sentence as input, and classify it by assigning a single part-of-speech out of a closed set.</p>
<p>In practice, we begin with a <b>training corpus</b>, which is a large body of text which has been manually annotated for the phenomenon that we wish to annotate automatically. Then, a statistical model is trained by projecting various <b>features</b> onto the training corpus, in an attempt to recognise patterns within the language. When asked to annotate a new text, the machine first projects the same set of features onto the new text, then feeds the feature results into the statistical model, and finally selects a classification which is statistically the most likely, in view of the patterns seen in the training corpus. The actual nature of features is described in <a href="#features">this section</a>.</p>
<p>Now, for certain tasks, Talismane also allows the user to override the decision taken by the statistical model, using pre-configured <b>rules</b>. Rules are particularly useful for encoding phenomena which are statistically rare or absent from the training corpus, but about which the linguist is quite sure. They can also be used to hard-code decisions to be taken in a specialised corpus (e.g. named entities). Thus, <b>features</b> are used in statistical model training, while <b>rules</b> are only used when actually analysing a text.</p>

<a name="features"></a>
<h4>Features</h4>
<p>In automatic classification, the machine tries to correctly classify a given linguistic context. The <b>linguistic context</b> to classify depends on the task: in the case of pos-tagging, for example, the linguistic context is a single word (or more precisely, a single token) within a sentence. A <b>feature</b> is an attribute of the linguistic context, which can provide information about the correct classification for this context. For example, when building a French pos-tagger, we can create a feature called <tt>*ait</tt> which returns true if a word ends with the suffix "-ait" and false otherwise. It can be seen that many words for which the <tt>*ait</tt> feature returns true (e.g. "parlait") should be classified as verbs in the imperfect tense.</p>
<p>In Talismane, a feature can return a boolean value (i.e. true or false), a numeric value (integer or decimal), or a string. There is a standard <a href="#syntax">feature definition syntax</a> which is identical for all modules. A simple example for the pos-tagger is the feature <tt>FirstWordInSentence()</tt>, which returns true if the word in question is the first one in the sentence, and false otherwise.</p>
<p>Feature selection is the manner in which the linguist brings his linguistic knowledge to bear on Talismane's statistical models. The list of features used by the default French implementation can be found in the Git directory <tt>talismane_trainer_fr/features</tt>.

<a name="rules"></a>
<h4>Rules</h4>
<p>Rules are applied when Talismane analyses new data, allowing the system to bypass the statistical model and automatically assign a certain classification to a certain phenomenon. Any boolean feature can be transformed into a rule: if the boolean feature returns <tt>true</tt> for a given context, the rule is applied, and the statistical model is skipped.</p>
<p>Rules thus add a symbolic dimension to the statistical approach. This makes it possible for Talismane to adapt to certain specific cases which are either under-represented or not represented at all in the training corpus. Rules, however, should be used with caution, as a large number of rules can make maintenance difficult.</p>
<p>Information on how to define the rules used for analysis can be found <a href="#rulesFile">below</a>.

<h3>Resources</h3>

<a name="lefff"></a>
<h4>Lefff</h4>

<p>The default French implementation of Talismane uses the <b>Lefff</b> (see [Sagot et al. 2006]) as its main lexical resource. More information about the Lefff can be found at <a href="http://www.labri.fr/perso/clement/lefff/">http://www.labri.fr/perso/clement/lefff/</a>. Every time Talismane is started, it loads the entire Lefff into memory, which typically takes at least 20 seconds (depending on system specs of course). This slows down performance for short text analysis, but speeds it up for longer text analysis. Note that Talismane is designed to enable the user to replace or extend the lexical resource by any other lexical resource implementing the required <tt>com.joliciel.talismane.posTagger.PosTaggerLexiconService</tt> interface. The Lefff lexicon, which is incorporated in electronic format in the default French implementation for Talismane, is distributed under the <a href="http://www.labri.fr/perso/clement/lefff/licence-LGPLLR.html">LGPLLR license</a>.

<p>In practice, the Lefff is used to project features onto the corpus, either as training features or as rules. For example, it can be used to create a feature checking if a certain word is listed in the Lefff as a verb, thus increasing the probability that the word is actually a verb. It can also be used in negative rules, excluding words from a closed-class parts-of-speech unless they are explicitly listed as belonging to that part-of-speech. The following negative rule, for example, does not allow Talismane to classify a word as a subordinating conjunction unless it is listed in the Lefff as a subordinating conjunction:</p>
<pre>!CS	Not(LexiconPosTag("CS"))</pre>

<a name="ftb"></a>
<h4>French TreeBank</h4>
<p>The <b>French Treebank</b> is a syntaxically and morphologically annoted corpus (see [Abeillé and Clément, 2003]). It is annotated for syntaxic constituents (rather than dependencies), and includes some functional data about the relationship between these constituents. It is used indirectly by Talismane, as the training corpus on which its default statistical models are built for sentence detection, tokenising, and pos-tagging. The default parser model is trained on a version of the French Treebank converted to dependencies by [Candito et al. 2010].</p>
<p>Unlike the Lefff lexicon, the French Treebank is not distributed under a license which permits its redistribution with Talismane. Anybody wishing to construct new statistical models must either find another annotated corpus for French, or contact <a href="http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php">Anne Abeillé</a> to request a copy of the French Treebank.</p>
 
<a name="tagset"></a>
<h3>The tagset</h3>
<p>The tagset used in the default French implementation of Talismane is based on [Crabbé and Candito, 2008], with slight modification. The tags used are listed below.</p>

<table border="1">
<tr><th>Tag</th><th>Part of speech</th></tr>
<tr><td>ADJ</td><td>Adjective</td></tr>
<tr><td>ADV</td><td>Adverb</td></tr>
<tr><td>ADVWH</td><td>Interrogative adverb</td></tr>
<tr><td>CC</td><td>Coordinating conjunction</td></tr>
<tr><td>CLO</td><td>Clitic (object)</td></tr>
<tr><td>CLR</td><td>Clitic (reflexive)</td></tr>
<tr><td>CLS</td><td>Clitic (subject)</td></tr>
<tr><td>CS</td><td>Subordinating conjunction</td></tr>
<tr><td>DET</td><td>Determinent</td></tr>
<tr><td>DETWH</td><td>Interrogative determinent</td></tr>
<tr><td>ET</td><td>Foreign word</td></tr>
<tr><td>I</td><td>Interjection</td></tr>
<tr><td>NC</td><td>Common noun</td></tr>
<tr><td>NPP</td><td>Proper noun</td></tr>
<tr><td>P</td><td>Preposition</td></tr>
<tr><td>P+D</td><td>Preposition and determinant combined (e.g. "du")</td></tr>
<tr><td>P+PRO</td><td>Preposition and pronoun combined (e.g. "duquel")</td></tr>
<tr><td>PONCT</td><td>Punctuation</td></tr>
<tr><td>PRO</td><td>Pronoun</td></tr>
<tr><td>PROREL</td><td>Relative pronoun</td></tr>
<tr><td>PROWH</td><td>Interrogative pronoun</td></tr>
<tr><td>V</td><td>Indicative verb</td></tr>
<tr><td>VIMP</td><td>Imperative verb</td></tr>
<tr><td>VINF</td><td>Infinitive verb</td></tr>
<tr><td>VPP</td><td>Past participle</td></tr>
<tr><td>VPR</td><td>Present participle</td></tr>
<tr><td>VS</td><td>Subjunctive verb</td></tr>
</table>

<p>In Talismane, the tagset is contained in the file <tt>talismane_trainer_fr/postags/CrabbeCanditoTagset.txt</tt>. The many-to-many mapping between the morpho-syntaxic categories used by the French Treebank and this tagset is found in the file <tt>talismane_trainer_fr/postags/ftbCrabbeCanditoTagsetMap.txt</tt>. The many-to-many mapping between the morpho-syntaxic categories used by the Lefff and this tagset are found in the file <tt>lefff/resources/lefffCrabbeCanditoTagsetMap.txt</tt>.</p>

<h3>Output format</h3>
<p>Talismane's default output format for parsing is based on the <a href="http://nextens.uvt.nl/depparse-wiki/DataFormat">CoNLL format</a>. This format was defined for the CoNLL evaluation campaign for multilingual morpho-syntaxic annotation. The elements are separated into columns by tab characters. Other formats may be defined in templates, as described in the <a href="#outputTemplate">advanced usage section</a>.</p>
<p>The CoNLL format used by Talismane outputs the following information in each row:<p>
<li>The token number (starting at 1 for the first token)</li>
<li>The original word form (or _ for an empty token)</li>
<li>The lemma found in the lexicon (or _ when unknown)</li>
<li>The part-of-speech tag</li>
<li>The grammatical category found in the lexicon</li>
<li>The additional morpho-syntaxic information found in the lexicon.</li>
<li>The token number of this token's governor (or 0 when the governor is the root)</li>
<li>The label of the dependency governing this token</li>
<p>For example:</p>
<pre>6	couvent	couver	V	v	n=s|p=3|t=pst	0	root	_	_</pre>

<p>The additional morpho-syntaxic information includes:</p>
<li><b>g</b>=<i>m|f</i>: gender = male or female</li>
<li><b>n</b>=<i>p|s</i>: number = plural or singluar</li>
<li><b>p</b>=<i>1|2|3|12|23|123</i>: person = 1st, 2nd, 3rd (or a combination thereof if several can apply)</li>
<li><b>poss</b>=<i>p|s</i>: possessor number = plural or singular</li>
<li><b>t</b>=<i>pst|past|imp|fut|cond</i>: tense = present, past, imperfect, future or conditional. Verb mood is not included, since it is already in the postag.</li>

<h1>Advanced usage</h1>

<p>Talismane comes with a default French implementation. It is possible to modify the behaviour of this implementation in three ways (ordered by increasing complexity):</p>
<li>Modifying the configuration files</li>
<li>Training new statistical models</li>
<li>Modifying the source code</li>
<p>The present section concentrates on the configuration files, though some information in this section (e.g. feature syntax) has a direct bearing on training new statistical models. There is a short section in the end explaining how to train new models.</p>

<a name="outputTemplate"></a>
<h2>Modifying the output format</h2>
<p>In order to define the output format, Talismane defines an output "object" for each module, and processes this object using a <a href="http://freemarker.sourceforge.net">Freemarker</a> template. More information on writing these templates can be found in the Freemarker's <a href="http://freemarker.sourceforge.net/docs/index.html">Template Author's Guide</a>. The default French implementation of Talismane comes with a default template for each module. These default templates can be found in the source code of the <tt>talismane_core</tt> project, in the package <tt>com.joliciel.talismane.output</tt>.</p>
<p>By using the <tt>template</tt> argument in <tt>talismane-fr-<i>version</i>.jar</tt>, the user can override the default template with a template of his choice.</p>
<p>The following is a schematic view of the output interface for the parser:</p>
<table border="1">
<tr><th>class</th><th>class description</th><th>field</th><th>field description</th></tr>
<tr><td><b>sentence</b></td><td colspan="3">a top-level object, which is simply a sequence of syntaxic units, see <b>unit</b> below</td></tr>
<tr><td><b>configuration</b></td><td colspan="3">a top-level object giving more parsing details, see com.joliciel.talismane.parser.ParseConfiguration for details</td></tr>
<tr><td rowspan="6"><b>unit</b></td><td rowspan="6">a single syntaxic unit, i.e. a pos-tagged token with a dependency</td><td>token</td><td>the actual <b>token</b> (see below)</td></tr>
<tr><td>tag</td><td>it's pos-tag</td></tr>
<tr><td>lexicalEntry</td><td>the <b>lexical entry</b> found for this token/pos-tag combination</td></tr>
<tr><td>governor</td><td>the token's governor (another <b>unit</b>)</td></tr>
<tr><td>label</td><td>the dependency label between the governor and the token</td></tr>
<tr><td>lemmaForCoNLL</td><td>the "best" lemma for this pos-tagged token, formatted for CoNLL (spaces and blanks replaced by underscores)</td></tr>
<tr><td rowspan="8"><b>token</b><td rowspan="8">a single token</td><td>text</td><td>the token that was processed by Talismane</td></tr>
<tr><td>originalText</td><td>the original token that was in the raw text</td></tr>
<tr><td>textForCoNLL</td><td>the original text with spaces replaced by underscores, and an empty entry replaced by an underscore</td></tr>
<tr><td>index</td><td>the token's placement in this sentence (starting at 1)</td></tr>
<tr><td>fileName</td><td>the file name in which the token was found</td></tr>
<tr><td>lineNumber</td><td>the line number within the file (starting at 1)</td></tr>
<tr><td>columnNumber</td><td>the column number within the line (starting at 1)</td></tr>
<tr><td>precedingRawOutput</td><td>any text in the original file that has been marked for raw output just before this token</td></tr>
<tr><td rowspan="5"><b>lexical entry</b></td><td rowspan="5">a lexical entry from the lexicon</td><td>word</td><td>the lexical form of this entry</td></tr>
<tr><td>lemma</td><td>the lemma for this entry</td></tr>
<tr><td>category</td><td>the main grammatical category for this entry</td></tr>
<tr><td>morphology</td><td>additional morpho-syntaxic details for this entry</td></tr>
<tr><td>morphologyForCoNLL</td><td>additional morpho-syntaxic details for this entry, in CoNLL format (separated by a vertical pipe)</td></tr>
</table>
<p>Thus, the default parser output uses the following Freemarker template to produce the CoNLL output:</p>
<pre>
[#ftl]
[#list sentence as unit]
[#if unit.token.precedingRawOutput??]
${unit.token.precedingRawOutput}
[/#if]
[#if unit.token.index>0]
${unit.token.index?c}	${unit.token.textForCoNLL}	${unit.posTaggedToken.lemmaForCoNLL}	${unit.tag.code}	${(unit.lexicalEntry.category)!"_"}	${(unit.lexicalEntry.morphologyForCoNLL)!"_"}	${(unit.governor.token.index?c)!"0"}	${unit.label!"_"}	_	_
[/#if]
[/#list]</pre>
<p>If we wanted to add on the original location, we could add the following:</p>
<pre>${unit.token.fileName}	${(unit.token.lineNumber)?c}	${(unit.token.columnNumber)?c}</pre>

<p>Similarly, the following is a schematic view of the output interface for the pos-tagger:</p>
<table border="1">
<tr><th>class</th><th>class description</th><th>field</th><th>field description</th></tr>
<tr><td><b>sentence</b></td><td colspan="3">a top-level object, which is simply a sequence pos-tagged units, see <b>unit</b> below</td></tr>
<tr><td rowspan="3"><b>unit</b></td><td rowspan="3">a single pos-tagged token</td><td>token</td><td>the actual <b>token</b> (see below)</td></tr>
<tr><td>tag</td><td>it's pos-tag</td></tr>
<tr><td>lexicalEntry</td><td>the <b>lexical entry</b> found for this token/pos-tag combination</td></tr>
<tr><td rowspan="8"><b>token</b><td rowspan="8">a single token</td><td>text</td><td>the token that was processed by Talismane</td></tr>
<tr><td>originalText</td><td>the original token that was in the raw text</td></tr>
<tr><td>textForCoNLL</td><td>the original text with spaces replaced by underscores, and an empty entry replaced by an underscore</td></tr>
<tr><td>index</td><td>the token's placement in this sentence (starting at 0)</td></tr>
<tr><td>fileName</td><td>the file name in which the token was found</td></tr>
<tr><td>lineNumber</td><td>the line number within the file (starting at 1)</td></tr>
<tr><td>columnNumber</td><td>the column number within the line (starting at 1)</td></tr>
<tr><td>precedingRawOutput</td><td>any text in the original file that has been marked for raw output just before this token</td></tr>
<tr><td rowspan="5"><b>lexical entry</b></td><td rowspan="5">a lexical entry from the lexicon</td><td>word</td><td>the lexical form of this entry</td></tr>
<tr><td>lemma</td><td>the lemma for this entry</td></tr>
<tr><td>category</td><td>the main grammatical category for this entry</td></tr>
<tr><td>morphology</td><td>additional morpho-syntaxic details for this entry</td></tr>
<tr><td>morphologyForCoNLL</td><td>additional morpho-syntaxic details for this entry, in CoNLL format (separated by a vertical pipe)</td></tr>
</table>
<p>The default Freemarker template for the pos-tagger is:</p>
<pre>
[#ftl]
[#list sentence as unit]
[#if unit.token.precedingRawOutput??]
${unit.token.precedingRawOutput}
[/#if]
${unit.token.index?c}	${unit.token.textForCoNLL}	${unit.lemmaForCoNLL}	${unit.tag.code}	${(unit.lexicalEntry.category)!"_"}	${(unit.lexicalEntry.morphologyForCoNLL)!"_"}	
[/#list]</pre>

<p>This template would produce the following output:</p>
<pre>
0       Les     les     DET     det     n=p
1       poules  poule   NC      nc      g=f|n=p
2       du      de      P+D     P+D     g=m|n=s
3       couvent couvent NC      nc      g=m|n=s
4       couvent couver  V       v       n=p|p=3|t=pst
5       .       .       PONCT   PONCT   _</pre>
<div class="info"><b>Token indexes: </b> Note that the pos-tagger indexes start at 0, while the parser indexes start at 1, since the parser has inserted an artificial "root" token at position 0.</div>

<a name="inputTemplate"></a>
<h2>Modifying the input format</h2>
<p>You may sometimes need to apply Talismane to text that has already been processed elsewhere - either broken up into sentences, or tokenised, or pos-tagged.
In this case, you would set the <tt>startModule</tt> argument to <tt>tokeniser</tt>, <tt>postag</tt> or <tt>parse</tt>, indicating that sentence boundary detection is not the first required task. Thus, Talismane is no longer being used to analyse raw text, but rather annotated text, and has to be told how to identify the annotations.</p>
<p>Each startModule has its own default format.</p>
<h3>Input format for the tokeniser</h3>
<p>The tokeniser will always assume there is one sentence per line.</p>
<h3>Input format for the pos-tagger</h3>
<p>The pos-tagger will always assume there is one token per line, and and empty line between sentences. It thus needs to find a token in each line of input. The default input format, <tt>.+\\t(.+)</tt> assumes an index, a tab and a token on each line, e.g.</p>
<pre>1	Je</pre>
<p>To override this format, use the <tt>inputPattern</tt> or <tt>inputPatternFile</tt> arguments to indicate a new regex. The regex must contain a single group in parentheses, indicating the location of the token, and is a regular Java regular expression pattern in all other respects.</p>
<h3>Input format for the parser</h3>
<p>The parser assumes one pos-tagged token per line, and an empty line between sentences. It thus needs to find a token and a pos-tag in each line of input. The default input format, <tt>.*\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t</tt>, corresponds to Talimane's default output format for the pos-tagger.</p>
<p>This pattern indicates that the <tt>%TOKEN%</tt> will be found after the first tab, and the <tt>%POSTAG%</tt> after the third tab. To override this format, use the <tt>inputPattern</tt> or <tt>inputPatternFile</tt> arguments to indicate a new regex. The regex must contain the strings <tt>%TOKEN%</tt> and <tt>%POSTAG%</tt> to indicate the positions of the token and pos-tag respectively, and is a regular Java regular expression pattern in all other respects.</p>
<h3>Input formats for evaluation</h3>
<p>When evaluating the pos-tagger, the input format needs to indicate the expected pos-tag for each token. It is thus identical to the input format for the parser above.</p>
<p>When evaluating the parser, the input format must indicate the <tt>%INDEX%</tt>, <tt>%TOKEN%</tt>, <tt>%POSTAG%</tt>, <tt>%LABEL%</tt> and <tt>%GOVERNOR%</tt>, as follows:</p>
<li><tt>%INDEX%</tt>: a unique index for a given token (typically just a sequential index)</li>
<li><tt>%TOKEN%</tt>: the token</li>
<li><tt>%POSTAG%</tt>: the token's pos-tag</li>
<li><tt>%LABEL%</tt>: the dependency label governing this token</li>
<li><tt>%GOVERNOR%</tt>: the index of the token governing this token - a value of 0 indicates an invisible "root" token as a governor</li>
<p>The default format corresponds to the CONLL format: <tt>%INDEX%\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t%GOVERNOR%\t%LABEL%\t_\t_</tt>, with the assumption that the pos-tag which interests us is the "coarse" CoNLL pos-tag (rather than the fine-grained one).</p>

<a name="filters"></a>
<h2>Filtering the input: text marker filters</h2>
<p>Many input texts have sections that need to be parsed and other sections that should be skipped or replaced. For example, in XML, it may only be necessary to parse text contained between certain tags, and to skip certain tags inside that text.</p>
<p>The simplest way to do this is by applying regular expression filters to the raw text, using the <tt>textFilters</tt> argument.</p>
<p>The <tt>textFilters</tt> file has the following tab-delimited format per line:</p>
<pre><i>FilterType</i>	<i>Markers</i>	<i>Regex</i>	<i>GroupNumber*</i>	<i>Replacement</i></pre>
<p>The meaning of these fields is given below:</p>
<li><b>FilterType:</b> currently the only allowable value is RegexMarkerFilter</li>
<li><b>Markers:</b> a comma-delimited list of markers to be applied by this filter. See marker types below.</li>
<li><b>Regex:</b> the regular expression to be found in the text. This follows the <a href="http://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html">standard Java format</a>.</li>
<li><b>GroupNumber*:</b> the group number within the regular expression, that indicates the actual text to be marked.
This is useful when some context is required to determine which text needs to be marked, but the context itself should not be marked.
More information about groups can be found <a href="http://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html">in the Groups and Capturing section of the Java Pattern class</a></li>
<li><b>Replacement:</b> Only required if the FilterType is REPLACE. Like in Java Patterns, can include placeholders $1, $2, etc., which get filled in from the groups in the regex.</li>
<br/>
<div class="info"><b>Default group </b> By default, if a regex contains groups (marked by parentheses), and the <tt>GroupNumber</tt> parameter is omitted, Talismane will assume that the entire expression will be marked. To select a specific group for marking, explicitly enter the group number. Groups are numbered starting at 1 in the order in which their opening parenthesis appears, with group 0 always referring to the entire expression.</div>

<p>Below is a table of allowable markers. Markers are either stack based or unary. Stack-based markers mark both the beginning and end of section of text, and can be nested. Unary markers apply a single action at a given point in the text: if unary markers (e.g. start and end markers) are placed inside an area marked by a stack-based marker, their action will only affect this area. For maximum robustness, the best strategy is to reserve stack-based markers for very short segments, and use unary markers instead of excessive nesting.<br/></p>
<table border="1">
<tr><th>Marker type</th><th>Description</th></tr>
<tr><td>SKIP</td><td>Skip any text matching this filter (stack-based).</td></tr>
<tr><td>INCLUDE</td><td>Include any text matching this filter (stack-based).</td></tr>
<tr><td>OUTPUT</td><td>Skip any text matching this filter, and output its raw content in any output file produced by Talismane (stack-based).</td></tr>
<tr><td>SENTENCE_BREAK</td><td>Insert a sentence break.</td></tr>
<tr><td>SPACE</td><td>Replace the text with a space (unless the previous segment ends with a space already). Only applies if the current text is marked for processing.</td></tr>
<tr><td>REPLACE</td><td>Replace the text with another text. Should only be used for encoding replacements which don't change meaning - e.g. replace "&amp;eacute;" by "é". Only applies if the current text is marked for processing.</td></tr>
<tr><td>STOP</td><td>Mark the beginning of a section to be skipped (without an explicit end).<br/>
Note that the processing will stop at the beginning of the match.<br/>
If this marker is placed inside an area marked by SKIP, INCLUDE or OUTPUT, it will only take effect within this area. It can be reversed by a START marker.</td></tr>
<tr><td>START</td><td>Mark the beginning of a section to be processed (without an explicit end).<br/>
Note that the processing will begin AFTER the end of the match.<br/>
If this marker is placed inside an area marked by SKIP, INCLUDE or OUTPUT,
it will only take effect within this area. It can be reversed by a START marker.</td></tr>
<tr><td>OUTPUT_START</td><td>Mark the beginning of a section to be outputted (without an explicit end).<br/>
Will only actually output if processing is stopped.<br/>
Stopping needs to be marked separately (via a STOP marker).<br/>
Note that the output will begin at the beginning of the match.<br/>
If this marker is placed inside an area marked by OUTPUT, it will only take effect within this area. It can be reversed by a OUTPUT_STOP marker.</td></tr>
<tr><td>OUTPUT_STOP</td><td>Mark the end of a section to be outputted (without an explicit beginning).<br/>
Starting the processing needs to be marked separately.<br/>
Note that the output will stop at the end of the match.<br/>
If this marker is placed inside an area marked by OUTPUT, it will only take effect within this area. It can be reversed by a OUTPUT_START marker.</td></tr>
</table>
<p>The text marked for raw output will only be included if the output template explicitly includes it using the <tt>precedingRawOutput</tt> field (as is the case for the default templates). More information can be found in the <a href="#outputTemplate">output template</a> section.</p>

<div class="info"><b>Default behaviour for processing: </b> By default, Talismane will assume that the input file/stream should be processed from the very beginning. If this is not the case (e.g. for an XML file), the user should set the parameter <tt>processByDefault=false</tt>.</div>

<h3>Text marker filter examples</h3>
<li>To skip the XML tag <tt>&lt;skip&gt;</tt> and its contents:</li>
<pre>RegexMarkerFilter	SKIP	&lt;skip&gt;.*&lt;/skip&gt;</pre>
<li>To skip the XML tag <tt>&lt;b&gt;</tt>, but not its contents:</li>
<pre>RegexMarkerFilter	SKIP	&lt;b&gt;
RegexMarkerFilter	SKIP	&lt;/b&gt;</pre>
<li>To skip the text "Figure 2:" at the beginning of a paragraph. Note that the "\n" character is used to enforce the start-of-paragraph constraint, and we explicitly indicate that the group to be marked is group 1:</li>
<pre>RegexMarkerFilter	SKIP	\n(Figure \d+:)	1</pre>
<li>To include the text INSIDE the XML tag <tt>&lt;text&gt;</tt>:
<pre>RegexMarkerFilter	INCLUDE	&lt;text&gt;(.*)&lt;/text&gt;	1</pre>
<li>To mark the text inside the XML tag <tt>&lt;marker&gt;</tt> for output along with Talismane's analysis. Note that the output will be placed just prior to the token closest to the marker. Note that the token itself is included in the output:</li>
<pre>RegexMarkerFilter	OUTPUT	&lt;marker&gt;.*&lt;/marker&gt;</pre>
<li>Same as above, but excluding the marker tag itself from the output:</li>
<pre>RegexMarkerFilter	OUTPUT	&lt;marker&gt;(.*)&lt;/marker&gt;	1</pre>
<li>To replace a double-newline by a sentence break, and a single newline by a space, except when the line ends with a hyphen. Note that the regex below handles correctly both Unix, Windows and Mac OS9- newline characters:</li>
<pre>RegexMarkerFilter SKIP,SENTENCE_BREAK	(\r\n|[\r\n]){2}	0
RegexMarkerFilter SPACE	[^-\r\n](\r\n|[\r\n])	1</pre>
<li>To replace any occurrence of <tt>&amp;eacute;</tt> by é:</li>
<pre>RegexMarkerFilter	REPLACE	&eacute;	é</pre>
<li>To start processing when we reach the XML tag <tt>&lt;text&gt;</tt>:</li>
<pre>RegexMarkerFilter	START	&lt;text&gt;</pre>
<li>To stop processing when we reach the XML tag <tt>&lt;/text&gt;</tt>:</li>
<pre>RegexMarkerFilter	STOP	&lt;/text&gt;</pre>
<li>To start outputting raw text when we reach the XML tag <tt>&lt;marker&gt;</tt>:</li>
<pre>RegexMarkerFilter	STOP,OUTPUT_START	&lt;marker&gt;</pre>
<li>To stop outputting raw text when we reach the XML tag <tt>&lt;/marker&gt;</tt>:</li>
<pre>RegexMarkerFilter	START,OUTPUT_STOP	&lt;/marker&gt;</pre>

<a name="filters"></a>
<h2>Token filters</h2>
<p>Unlike text filters, token filters are used to mark certain portions of the text as "indissociable", and optionally to replace them with a replacement text when performing analysis. These portions will never be broken up into several different tokens. The tokeniser may still join them with other atomic tokens to create larger tokens containing them.</p>
<p>For example, the user may wish to indicate that e-mail addresses are indissociable, and to replace them by the word "EmailAddress" for analysis. He may then wish to indicate that EmailAddress should always be treated as a proper noun, using a pos-tagger rule.</p>
<p>Token filters are provided to Talismane in a configuration file using the <tt>tokenFilters</tt> command-line parameter. Lines beginning with a # will be skipped. Other lines will have the following format:</p>
<pre><i>FilterType</i>	<i>Regex</i>	<i>GroupNumber*</i>	<i>Replacement*</i></pre>
<p>The meaning of these fields is given below:</p>
<li><b>FilterType:</b> currently the only allowable value is TokenRegexFilter</li>
<li><b>Regex:</b> the regular expression to be found in the text. This follows the <a href="http://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html">standard Java format</a>.</li>
<li><b>GroupNumber*:</b> the group number within the regular expression, that indicates the actual text to be marked for tokenising.
This is useful when some context is required to determine which text needs to be marked, but the context itself should not be marked.
More information about groups can be found <a href="http://docs.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html">in the Groups and Capturing section of the Java Pattern class</a>. By default, the entire regex is marked.</li>
<li><b>Replacement:</b> The replacement for this token, when analaysing. Like in Java Patterns, can include placeholders $1, $2, etc., which get filled in from the groups in the regex.</li>
<br/>

For example, the following filter would replace all e-mail addresses by the word EmailAddress:
<pre>TokenRegexFilter	\b([\w.%-]+@[-.\w]+\.[A-Za-z]{2,4})\b	EmailAddress</pre>

<a name="rulesFile"></a>
<h2>Modifying the rules</h2>
<p>As stated previously, while features are applied during statistical model training, rules are applied during text analysis in order to bypass the statistical model. As such, rules are independent of the statistical model, and can be configured differently for each analysis. Rules are simply boolean features associated with either a positive classification (i.e. you must assign this classification if the features returns true) or a negative classification (i.e. you cannot assign this classification if the feature returns true).</p>
<div class="info">Rules files can also include "templates" (see feature syntax below). If templates are included, the template name must be preceded by a tab (but no postag/transitionCode)</div>

<h3>Pos-tagger rules</h3>
<p>Pos-tagger rules are configured using the <tt>posTaggerRules</tt> argument, which points at a file containing the rules. The file format is as follows:</p>
<pre>[postag|!postag]\t[boolean feature]</pre>
<p>Where \t is that tab character. A ! before the postag implies a negative rule (or constraint). The boolean feature syntax is described in greater detail in the <a href="#syntax">Feature definition syntax</a> section below. The rules used by the default French implementation of Talismane can be found in the <tt>talismane_fr</tt> project, at <tt>src/com/joliciel/talismane/fr/resources/posTaggerConstraints_fr.txt</tt>.</p>

<p>Negative rules, or <b>constraints</b>, are typically used around closed classes. The following constrint, for example, does not allow the pos-tagger to classify a word as a subordinating conjunction if it isn't listed in the lexicon as a subordinating conjunction:</p>
<pre>!CS	Not(LexiconPosTag("CS"))</pre>
<p>Alternatively, the following constraint ensures that a word which is listed only under closed classes in the lexicon (such as "le" in French) should never be classified as a common noun:</p>
<pre>!NC	HasClosedClassesOnly()</pre>
<div class="info"><b>Eliminating all classifications:</b> If negative rules result in all classifications being eliminated for a certain context, then the negative rules are ignored, rather than forcing the system not to assign any classification.</div>
<p>Positive rules are typically used for very specific cases that are under-represented in the training corpus, but which the linguist feels should always result in a certain classification. For example, the positive rule below tells Talsimane to classify as an adjective any word representing a cardinal, when preceded by a determinent and followed by a token which, according to the lexicon, can be classified as a noun:</p>
<pre>ADJ	PosTag(History(-1))=="DET" & Word("deux","trois","quatre","cinq","six","sept","huit","neuf","dix") & LexiconPosTag(Offset(1),"NC")</pre>

<h3>Parser rules</h3>
<p>Parser rules are configured using the <tt>parserRules</tt> argument, which points at a file containing the rules. The file format is as follows:</p>
<pre>[transitionCode|!transitionCode]\t[boolean feature]</pre>
<p>Where \t is the tab character. A ! before the transition code implies a negative rule (or constraint). The boolean feature syntax is described in greater detail in the <a href="#syntax">Feature definition syntax</a> section below.</p>
<p>Positive parser rules should be used with extreme caution, as they may easily degrade parser performance or even force the parser to abandon parsing mid-way.</p>
<p>For example, a user may wish to automatically attach a punctuation mark to its first potential governor, so as to avoid cluttering the beam with useless punctuation attachment options.</p>
<p>A first attempt might be:</p>
<pre>RightArc[ponct]	PosTag(Buffer[0])=="PONCT"
LeftArc[ponct]	PosTag(Stack[0])=="PONCT"</pre>
<p>However, there are two issues here: first of all, a comma can have two labels: "coord" when it's a coordinant, and "ponct" otherwise. Secondly, after performing a RightArc we need to reduce the stack, so as to allow for further parsing. So, a better implementation would be:</p>
<pre>RightArc[ponct]	PosTag(Buffer[0])=="PONCT" & Not(LexicalForm(Buffer[0])==",")
Reduce	DependencyLabel(Stack[0])=="ponct" & Not(LexicalForm(Stack[0])==",")
# If after all that we still don't have a dependent, we apply a LeftArc on the punctuation (should only occur with punct at start of sentence).
LeftArc[ponct]	PosTag(Stack[0])=="PONCT" & Not(LexicalForm(Stack[0])==",")</pre>
<p>However, evaluation shows that while these rules radically speed up parsing at higher beams (about 50%), they also reduce accuracy, since the order of attachment in a shift-reduce algorithm determines the order in which other elements are compared, and attaching punctuation early makes it more likely to attach elements accross this punctuation mark.</p>
<p>Negative rules are far less risky, as they simply constrain choices. For example, you may wish to indicate that a verb should only have one subject, as follows:</p>
<pre>!LeftArc[suj]	PosTag(Buffer[0])=="V" & DependentCountIf(Buffer[0],DependecyLabel=="suj")>0</pre>
<p>However, even this should be done with caution, as some configurations may justify two subjects (without coordination), as in French questions, when a pronoun clitic is repeated after the verb: <i>"Mon copain est-il arrivé ?"</i></p>

<a name="syntax"></a>
<h2>Feature definition syntax</h2>
<p>Talismane uses a standard feature definition syntax for all modules, although each module supports specific functions for use within this syntax.</p>
<p>This syntax allows for the use of certain operators and parentheses for grouping, as well as certain generic functions. The operators are listed in the table below:</p>
<table border="1">
<tr><th>Operator</th><th>Result</th><th>Description</th></tr>
<tr><td>+</td><td>integer/decimal</td><td>integer or decimal addition</td></tr>
<tr><td>-</td><td>integer/decimal</td><td>integer or decimal substraction</td></tr>
<tr><td>*</td><td>integer/decimal</td><td>integer or decimal multiplication</td></tr>
<tr><td>/</td><td>integer/decimal</td><td>integer or decimal division</td></tr>
<tr><td>%</td><td>integer</td><td>integer modulus</td></tr>
<tr><td>==</td><td>boolean</td><td>integer, decimal, string or boolean equality</td></tr>
<tr><td>!=</td><td>boolean</td><td>integer, decimal, string or boolean inequality</td></tr>
<tr><td>&lt;</td><td>boolean</td><td>integer or decimal less than operator</td></tr>
<tr><td>&gt;</td><td>boolean</td><td>integer or decimal greater than operator</td></tr>
<tr><td>&lt;=</td><td>boolean</td><td>integer or decimal less than or equal to</td></tr>
<tr><td>&gt;=</td><td>boolean</td><td>integer or decimal greater than or equal to</td></tr>
<tr><td>&amp;</td><td>boolean</td><td>boolean AND</td></tr>
<tr><td>|</td><td>boolean</td><td>boolean OR</td></tr>
<tr><td>(...)</td><td>n/a</td><td>grouping parenthesis</td></tr>
<tr><td>[...]</td><td>n/a</td><td>grouping brackets</td></tr>
<tr><td>"..."</td><td>string</td><td>encloses a string - double quotes can be escaped via \"</td></tr>
</table>
<br/>
<div class="info"><b>Parentheses and square brackets</b> may be used interchangeably, but must be matched with the correct closing symbol.</div>
<p>The following are the generic functions supported by Talismane for all modules:</p>
<table border="1">
<tr><th>Function</th><th>Type</th><th>Description</th></tr>
<tr><td>And(boolean,boolean,...)</td><td>boolean</td><td>Performs a boolean AND of any number of boolean features</td></tr>
<tr><td>Concat(string,string,...)</td><td>string</td><td>Merges two or more string features by concatenating their results and adding a | in between. Includes the string "null" if any of the results is null.</td></tr>
<tr><td>ConcatNoNulls(string,string,...)</td><td>string</td><td>Merges two or more string features by concatenating their results and adding a | in between. If any of the results is null, returns a null.</td></tr>
<tr><td>Graduate(decimal feature, integer n)</td><td>decimal</td><td>Takes a feature with a value from 0 to 1, and converts it to a graduated value of 0, 1/n, 2/n, 3/n, ..., 1.</td></tr>
<tr><td>ExternalResource(string name, string keyElement1, string keyElement2...)</td><td>string</td><td>Returns the class indicated in the named external resource for the set of key elements. See <a href="#externalResources">External Resources</a> below.</td></tr>
<tr><td>MultivaluedExternalResource(string name, string keyElement1, string keyElement2...)</td><td>string collection</td><td>Returns the collection of classes/weights indicated in the named external resource for the set of key elements. See <a href="#externalResources">External Resources</a> below.</td></tr><tr><td>IfThenElse(boolean condition, any thenResult, any elseResult)</td><td>any</td><td>Standard if condition==true then return one result else return another result. The results must both be of the same type.</td></tr>
<tr><td>IndexRange(integer from, integer to)</td><td>integer</td><td>Creates n separate features, one per index in the range going from "from" to "to".
<br/>Note: unlike other functions, index range can ONLY take actual numbers (e.g. 1, 2, 3) as parameters - you cannot pass it an integer function as a parameter.</td></tr>
<tr><td>Inverse(decimal)</td><td>decimal</td><td>Inverts a normalised double feature (whose values go from 0 to 1), giving 1-result. If the result is &lt; 0, returns 0.</td></tr>
<tr><td>IsNull(any)</td><td>boolean</td><td>Returns true if a feature returns null, false otherwise.</td></tr>
<tr><td>Normalise(decimal feature, decimal minValue, decimal maxValue)</td><td>decimal</td><td>Changes a numeric feature to a value from 0 to 1, where any value <= minValue is set to 0, and any value >= maxValue is set to 1, and all other values are set to a proportional value between 0 and 1.</td></tr>
<tr><td>Normalise(decimal feature, decimal maxValue)</td><td>decimal</td><td>Like Normalise(decimal, decimal, decimal) above, but minValue is defaulted to 0.</td></tr>
<tr><td>Not(boolean)</td><td>boolean</td><td>Performs a boolean NOT of a boolean feature</td></tr>
<tr><td>NullIf(boolean condition, any feature)</td><td>any</td><td>If the condition returns true, return null, else return the result of the feature provided.</td></tr>
<tr><td>NullToFalse(boolean)</td><td>boolean</td><td>If the wrapped boolean feature returns null, will convert it to a false.</td></tr>
<tr><td>OnlyTrue(boolean)</td><td>boolean</td><td>If the boolean feature returns false, will convert it to a null. Useful to keep the feature sparse, so that only true values return a result.</td></tr>
<tr><td>Or(boolean,boolean,...)</td><td>boolean</td><td>Performs a boolean OR of any number of boolean features</td></tr>
<tr><td>Round(decimal)</td><td>integer</td><td>Rounds a double to the nearest integer.</td></tr>
<tr><td>ToString(any)</td><td>string</td><td>Converts a non-string feature to a string feature. If the feature result is null, will return the string "null".</td></tr>
<tr><td>ToStringNoNulls(any)</td><td>string</td><td>Converts a non-string feature to a string feature. If the feature result is null, will return null (rather than the string "null").</td></tr>
<tr><td>Truncate(decimal)</td><td>integer</td><td>Truncates a double down to an integer.</td></tr>
</table>
<p>The specific functions that can be used with each of the Talismane modules are listed in the section concerning this module, below.</p>
<div class="info">
<b>The OnlyTrue function</b> makes a boolean feature return true or null. This allows us to include features where the value "false" wouldn't add much value and would uselessly burden training. For example, in a literary text, the vast majority of tokens doesn't contain a number. Thus, knowing that a token doesen't contain a number doesn't really help determine its part-of-speech. We would write our feature something like this: <tt>OnlyTrue(ContainsNumber())</tt>, in order to exclude the class returning a negative result from the calculations.
<br/>A very few features have "OnlyTrue" behaviour by default, and if the opposite behaviour is desired, the user needs to use <tt>NullToFalse</tt>. This is notably the example for the tokeniser feature Word().</div>

<h3>Named features</h3>
<p>Features in Talismane can be named, which allows them to be re-used by other features, and also simplifies the interpretation of any output files referring to features,
as the name will be used instead of the full feature.
This is done by giving the features a unique name, followed by a tab, followed by the feature itself, e.g.</p>
<pre>IsMonthName	Word("janvier","février","mars","avril","mai","juin","juillet","août","septembre","octobre","novembre","décembre")
IsDayOfMonth	Word("31") & AndRange(NullToFalse(Word("31","au",",","et","à")), 1, ForwardLookup(IsMonthName()))</pre>
<p>Note that IsMonthName is used by the IsDayOfMonth feature.</p>

<h3>Parametrised features</h3>
<p>It is also possible to pass parameters to named features, thus creating a "template" of sorts.
Note that the parametrised features aren't analysed in and of themselves - they are only used as components for other features.
To create a parametrised feature, add parentheses after the feature name containing a comma-separated list of parameter names, and re-use these parameters in the feature description,
as follows:</p>
<pre>IfThenElseNull(X,Y)	NullIf(Not(X),Y)
IsVerb(X)	PosTag(X)=="V" | PosTag(X)=="VS" | PosTag(X)=="VIMP" | PosTag(X)=="VPP" | PosTag(X)=="VINF" | PosTag(X)=="VPR"
VerbPosTag	IfThenElseNull(IsVerb(Stack[0]),PosTag(Stack[0]))</pre>
<p>Note that the VerbPosTag feature uses the feature templates IfThenElseNull and IsVerb.</p>
<p>A special case is the parametrised feature with zero parameters, generated by adding empty parentheses after the feature name: <tt>StackIsVerb()</tt>.
This simply indicates to Talismane that the feature should not be computed on its own as a separate feature, but only when called by other features.</p>

<h3>Feature groups</h3>
<p>Features may be grouped together, by adding an additional feature group name separated by tabs from the feature name and the feature code, e.g.</p>
<pre>LemmaStack0()	PairGroup	Lemma(Stack[0])
LemmaBuffer0()	PairGroup	LemmaOrWord(Buffer[0])
LemmaBuffer1()	PairGroup	LemmaOrWord(Buffer[1])</pre>
<p>This allows us to construct additional features for an entire group (e.g. when we want to concatenate additional information for an entire group), as follows:</p>
<pre>PairGroup_P	ConcatNoNulls(PosTagPair(), PairGroup())
PairGroup_L	ConcatNoNulls(LexicalisedPair(), PairGroup())
PairGroup_V	ConcatNoNulls(LexicalisedVerbPair(), PairGroup())</pre>

<h3>String Collection features</h3>
<p>String collection features are a special kind of feature, which evaluates to a collection of weighted strings at runtime (instead of evaluating to a single result).</p>
<p>They can be used anywhere a regular StringFeature is expected</p>
<p>However, in order to avoid a cross-product of all instances of the collection feature, each collection feature is evaluated up-front, and a single value is inserted at a time.</p>
<p>For example, take the String collection feature <tt>LexiconPosTags</tt> (returning each postag found in the lexicon for the current token). Imagine a crazy feature which returns the lexicon postags for the current token, but replaces the postag "V" with the word "Verb". It might look something like: <tt>IfThenElse(LexiconPosTags=="V","Verb",LexiconPosTags)</tt>. If each instance of LexiconPosTags were evaluated separately, and the current token has 3 postags in the lexicon ("DET","P","CLO"), this feature would return a cross-product of the 2 calls (or 9 results) instead of the expected 3 results. Instead, Talismane evaluates LexiconPosTags up front, and then runs the feature 3 times, once for each result in the collection, filling in the result in the appropriate place as follows: <tt>IfThenElse("DET"=="V","Verb","DET")</tt>, <tt>IfThenElse("P"=="V","Verb","P")</tt>, <tt>IfThenElse("CLO"=="V","Verb","CLO")</tt>.</p>
<p>In this document, these are indicated by with a return type of <tt>string collection</tt>.</p>
<div class="alert">If a top-level feature contains a string collection feature, the top-level feature will be converted into <i>n</i> separate features,
one per string-collection result.<br/>
If the top-level feature returns a boolean or an integer, it will be converted to a string, and concatenated to current the string collection result.<br/>
If the top-level feature returns a double, the string collection result will be returned, and its weight will be multiplied by the double result to give the final result.<br/>
If the top-level feature returns a string, the string collection result <b>is not</b> concatenated to the top-level feature result. It is assumed that the string already includes the
string-collection result. If the user needs to include the string-collection result, it needs to be concatenated manually (typically via <tt>ConcatNoNulls</tt>).<br/>
See example feature files below fore details (e.g. Tokeniser feature file, which concatenates the <tt>TokeniserPatterns</tt> feature to the results).</div>

<h2>Talismane modules</h2>

<a name="sentenceDetector"></a>
<h3>The Sentence Detector</h3>

<p>The Talismane Sentence Detector examines each possible sentence boundary, and takes a binary decision on whether or not it is a true sentence boundary.
Any of the following characters will be considered as a possible sentence boundary:</p>
<p><tt>. ? ! " ) ] } » — ― ”</tt></p>
<p>The sentence detector features also look at the <b>atomic tokens</b> surrounding this possible boundary - see <a href="#tokeniser">tokeniser</a> below for more details.</p>
<div class="alert">The newline character is always assumed by the Sentence Detector to be a sentence boundary. If another behaviour is desired, the user needs to use <a href="#filters">filters</a>.</div>

<p>The following feature functions are available for the sentence detector:</p>
<table border="1">
<tr><th>Function</th><th>Type</th><th>Description</th></tr>
<tr><td>BoundaryString()</td><td>string</td><td>Returns the actual text of the possible sentence boundary being considered.</td></tr>
<tr><td>Initials()</td><td>boolean</td><td>Returns true if the current token is "." and the previous token is a capital letter, false otherwise..</td></tr>
<tr><td>InParentheses()</td><td>string</td><td>Returns "YES" if the current sentence break is between a "(" and ")" character without any intervening characters ".", "?" or "!".
<br/>Returns "OPEN" if a parenthesis has been open but not closed.<br/>Return "CLOSE" if a parenthesis has not been opened but has been closed.</td></tr>
<tr><td>IsStrongPunctuation()</td><td>boolean</td><td>Returns true if the current boundary is ".", "?" or "!". Returns false otherwise.</td></tr>
<tr><td>NextLetterCapital()</td><td>string</td><td>In the following descriptions, the current boundary is surrounded by square brackets.<br/>
Returns "CapitalAfterInitial" for any pattern like: W[.] Shakespeare<br/>
Returns "CapitalAfterQuote" for any pattern like: blah di blah[.]" Hello <i>or</i> blah di blah[.] "Hello<br/>
Returns "CapitalAfterDash" for any pattern like: blah di blah[.] - Hello<br/>
Returns "true" for any other pattern like: "blah di blah[.] Hello<br/>
Returns "false" otherwise.<br/>
Note that there MUST be whitespace between the separator and the capital letter for it to be considered a capital letter.</td></tr>
<tr><td>NextLetters(integer n)</td><td>string</td><td>Returns the <i>n</i> exact characters immediately following the current boundary.</td></tr>
<tr><td>NextTokens(integer n)</td><td>string</td><td>Returns the <i>n</i> atomic tokens immediately following the current boundary.</td></tr>
<tr><td>PreviousLetters(integer n)</td><td>string</td><td>Returns the <i>n</i> exact characters immediately preceding the current boundary.</td></tr>
<tr><td>PreviousTokens(integer n)</td><td>string</td><td>Returns the <i>n</i> atomic tokens immediately preceding the current boundary.</td></tr>
<tr><td>Surroundings(integer n)</td><td>string</td><td>Examines the atomic tokens from <i>n</i> before the boundary to <i>n</i> after the boundary.<br/>
For each token, if it is whitespace, adds " " to the result.<br/>
If it is a separator, adds the original separator to the result.<br/>
If it is a capitalised word, adds, "W", "Wo" or "Word", depending on whether the word is 1 letter, 2 letters, or more.<br/>
Otherwise adds "word" to the result.</td></tr>
</table>

<p>The initial release of Talismane for French contained the following sentence detector feature file:</p>
<pre>
BoundaryString()
IsStrongPunctuation()
NextLetterCapital()
InParentheses()
Initials()
PreviousLetters(IndexRange(1,4))
NextLetters(IndexRange(1,4))
PreviousTokens(IndexRange(1,3))
NextTokens(IndexRange(1,3))
Surroundings(IndexRange(1,3))</pre>

<a name="tokeniser"></a>
<h3>The Tokeniser</h3>
<p>An <b>atomic token</b> is defined as a contiguous character string which is either a single separator, or contains no separators.
The list of separators considered for this definition is as follows:</p>
<p><tt>! " # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \ ] ^ _ ` { | } ~ « » ‒ – — ―‛ “ ” „ ‟ ′ ″ ‴ ‹ › ‘ ’ *</tt> as well as the various whitespace characters: the space bar, the tab character, the newline, etc.</p>
<p>Thus, a text such as the following:<p>
<pre>A-t-elle mangé de l'avoine ?</pre>
<p>Will be separated into the following atomic tokens:</p>
<pre>[A][-][t][-][elle][ ][mangé][ ][de][ ][l]['][avoine][ ][?]</pre>
<p>In the Talismane suite, the tokeniser's role is to determine, for the interval between any two atomic tokens, whether it is separating or non-separating.</p>
<p>The default tokeniser provided with Talismane is a pattern-based tokeniser. This tokeniser assigns a default decision to each interval, unless the text within the interval matches a certain pattern. The default values are provided for each separator. If no value is provided, it is assumed the separator separates from both tokens before and after it. Otherwise, the following values can be provided:</p>
<li>IS_NOT_SEPARATOR: is connected to both the previous and next atomic tokens (e.g. for French, the dash <tt>-</tt>)</li>
<li>IS_SEPARATOR_AFTER: is connected to the previous atomic token, but separated from the next one (e.g. for French, the apostrophe <tt>'</tt> as in "l'avoine"</li>
<li>IS_SEPARATOR_BEFORE: is separated from the previous atomic token, but separated from the next one (as in the case of "-t-elle" in French)</li>
<p>Talismane's PatternTokeniser takes a configuration file in the specific format to list the default separator decisions and the patterns that can override these decisions.
First, the file contains a line for each default decision, followed by the separators to which it applies.
All other separators are assumed to separate tokens on both sides (IS_SEPARATOR)<br/>
Next, it should contain a list of patterns, using a syntax very similar to the Java pattern class, but somewhat modified - see the <tt>com.joliciel.talismane.tokeniser.patterns.TokenPattern</tt> class for details.<br/>
Optionally, each pattern can be preceded by a user-friendly name and a tab. Any line starting with a # is ignored.</p>
<p>For example, the following file could define patterns for French:</p>
<pre>
# Default decisions for separators
IS_NOT_SEPARATOR -
IS_SEPARATOR_AFTER '
IS_SEPARATOR_BEFORE
# List of patterns
ellipses	\.\.\.
Mr.	\D+\.[ ,\)/\\]
-t-elle	.+-t-(elle|elles|en|il|ils|on|y)
-elle	(?!t\z).+-(ce|elle|elles|en|il|ils|je|la|le|les|leur|lui|moi|nous|on|toi|tu|vous|y)
-t'	.+-[mt]'
celui-ci	(celui|celle|ceux|celles)-(ci|là)
^celui-ci	(?!celui\z|celle\z|ceux\z|celles\z).+-(ci|là)
1 000	\d+ \d+
1,000	\d+,\d+
1.000	\d+\.\d+</pre>

<h4>Specific feature functions available to the tokeniser</h4>
<p>The tokeniser defines the following specific feature functions for any token:</p>
<table border="1">
<tr><th>Function</th><th>Type</th><th>Description</th></tr>
<tr><td>AndRange(tokenAddress token*, boolean criterion, integer start, integer end)</td><td>boolean</td><td>Tests all tokens within a certain range for a certain criterion, and returns true only if all of them satisfy the criterion.<br/>
If (start&gt;end) will return null.<br/>
Start or end are relative to the current token's index.<br/>If either refer to a postion outside of the token sequence, will test all valid tokens only.<br/>
If no tokens are tested, will return null.<br/>
If any test returns null, will return null.<br/></td></tr>
<tr><td>BackwardLookup(tokenAddress token*, boolean criterion, integer offset*)</td><td>integer</td><td>Returns the offset of the first token to the left of this one
which matches a certain criterion, or null if no such token is found.<br/>
If an initial offset is provided as a second argument (must be a negative integer), will only look
to the left of this initial offset.<br/>
Will always return a negative integer.</td></tr>
<tr><td>BackwardSearch(tokenAddress token*, boolean criterion, integer startOffset*, integer endOffset*)</td><td>tokenAddress</td>
<td>Returns the first token preceding this one which matches a certain criterion, or null if no such token is found.<br/>
If a start offset is provided as a second argument (must be &lt;=0), will start looking at this offset.<br/>
If an end offset is provided as a third argument (must be &lt;=0), will continue until the end offset and then stop.<br/>
Note that, by default, it doesn't look at the current token (e.g. default start offset = -1) - to include the current
token, set start offset = 0.<br/>
Will always return a negative integer.</td></tr>
<tr><td>FirstWordInCompound(tokenAddress token*)</td><td>string</td><td>Returns the first word in a compound token. If not a compound token, returns null.</td></tr>
<tr><td>FirstWordInSentence(tokenAddress token*)</td><td>boolean</td><td>Returns true if this is the first word in the sentence.<br/>
Will skip initial punctuation (e.g. quotation marks) or numbered lists, returning true for the
first word following such constructs.</td></tr>
<tr><td>ForwardLookup(tokenAddress token*, boolean criterion, integer offset*)</td><td>integer</td><td>Returns the offset of the first token to the right of this one
which matches a certain criterion, or null if no such token is found.<br/>
If an initial offset is provided as a second argument, will only look
to the right of this initial offset.<br/>
Will always return a positive integer.</td></tr>
<tr><td>ForwardSearch(tokenAddress token*, boolean criterion, integer startOffset*, integer endOffset*)</td><td>tokenAddress</td>
<td>Returns the first token following this one which matches a certain criterion, or null if no such token is found.<br/>
If a start offset is provided as a second argument (must be &gt;=0), will start looking at this offset.<br/>
If an end offset is provided as a third argument (must be &gt;=0), will continue until the end offset and then stop.<br/>
Note that, by default, it doesn't look at the current token (e.g. default start offset = 1) - to include the current
token, set start offset = 0.</td></tr>
<tr><td>HasClosedClassesOnly(tokenAddress token*)</td><td>boolean</td><td>Returns true if all of this tokens classes in the lexicon are closed, false otherwise.</td></tr>
<tr><td>LastWordInCompound(tokenAddress token*)</td><td>string</td><td>Retrieves the last word in a compound token. Returns null if token isn't compound.</td></tr>
<tr><td>LastWordInSentence(tokenAddress token*)</td><td>boolean</td><td>Returns true if this is the last word in the sentence (including punctuation).</td></tr>
<tr><td>LemmaForPosTag(tokenAddress token*)</td><td>string</td><td>The "best" lemma of a given token and postag (or set of postags), as supplied by the lexicon.</td></tr>
<tr><td>LexiconAllPosTags(tokenAddress token*)</td><td>string</td><td>Returns a comma-separated concatenated string of all lexicon pos-tags for this token.</td></tr>
<tr><td>LexiconPosTag(tokenAddress token*, string posTag)</td><td>boolean</td><td>Returns true if the token has a lexical entry for the PosTag provided.</td></tr>
<tr><td>LexiconPosTagForString(tokenAddress token*, string testString, string posTag)</td><td>boolean</td><td>Returns true if the string provided has a lexicon entry for the PosTag provided.</td></tr>
<tr><td>LexiconPosTags(tokenAddress token*)</td><td>string collection</td><td>Returns each of the postags of the current token, according to the lexicon, as a collection of strings.</td></tr>
<tr><td>LexiconPosTagsForString(tokenAddress token*, string testString)</td><td>string collection</td><td>Returns each of the postags of a given string, according to the lexicon, as a collection of strings.</td></tr>
<tr><td>NLetterPrefix(tokenAddress token*, integer n)</td><td>string</td><td>Retrieves the first N letters of the first entire word in the present token, as long as N &lt; the length of the first entire word.</td></tr>
<tr><td>NLetterSuffix(tokenAddress token*, integer n)</td><td>string</td><td>Retrieves the last N letters of the last entire word in the current token, as long as N &lt; the length of the last word.</td></tr>
<tr><td>Offset(tokenAddress token*, integer offset)</td><td>tokenAddress</td><td>Returns a token offset from the current token by a certain offset.<br/>
Returns null if the offset goes outside the token sequence.</td></tr>
<tr><td>OrRange(tokenAddress token*, boolean criterion, integer start, integer end)</td><td>boolean</td><td>Tests all tokens within a certain range for a certain criterion,
and returns true if any one of them satisfies the criterion.<br/>
If (start&gt;end) will return null.<br/>
Start or end are relative to the current token's index.<br/>If either refer to a postion outside of the token sequence, will test all valid tokens only.<br/>
If no tokens are tested, will return null.<br/>
If any test returns null, will return null.</td></tr>
<tr><td>PosTagSet()</td><td>string collection</td><td>A StringCollectionFeature returning all of the postags in the current postagset.</td></tr>
<tr><td>Regex(tokenAddress token*, string pattern)</td><td>boolean</td><td>Returns true if the token matches a given regular expression.</td></tr>
<tr><td>TokenIndex(tokenAddress token*)</td><td>integer</td><td>Returns the current token's index in the sentence.</td></tr>
<tr><td>UnknownWord(tokenAddress token*)</td><td>boolean</td><td>Returns true if the token is unknown in the lexicon.</td></tr>
<tr><td>Word(tokenAddress token*, string, string, ...)</td><td>boolean</td><td> Returns true if token word is any one of the words provided.<br/>
Important: returns null (NOT false) if the the token word is not one of the words provided.</td></tr>
<tr><td>WordForm(tokenAddress token*)</td><td>string</td><td>Simply returns the current token's text.</td></tr>
</table>
<p>Note that all of the above features can be used by the pos-tagger as well.</p>
<div class="info">Most of the above features have an optional <tt>tokenAddress</tt> argument as the first argument.
A tokenAddress function is a function that, given a reference token, can return another token.
If provided, the feature will apply to the token returned by the token address function. Otherwise, it will apply to the current token.</div>

<p>The tokeniser defines certain features for patterns only.<br/>
For these features to work correctly, they should be fed the TokeniserPatterns() feature in place of the patternName. InsidePatternNgram should be fed TokeniserPatternsAndIndexes. See the example features below for details</p>
<table border="1">
<tr><th>Function</th><th>Type</th><th>Description</th></tr>
<tr><td>TokeniserPatterns()</td><td>string collection</td><td>Returns a collection of pattern names for each pattern where current token is the FIRST TOKEN in a sequence of tokens matching the pattern.</td></tr>
<tr><td>TokeniserPatternsAndIndexes()</td><td>string collection</td><td>Returns a collection of pattern names and indexes for each pattern where current token is NOT the FIRST TOKEN in a sequence of tokens matching the pattern.<br/>
The pattern name and index are separated by the character ¤ (as expected by the InnerPatternNgram feature).</td></tr>
<tr><td>PatternWordForm(string patternName)</td><td>string</td><td>Returns the actual text of the tokens matching the current pattern.</td></tr>
<tr><td>PatternIndexInSentence(string patternName)</td><td>int</td><td>Returns the index of the first token within the current pattern.</td></tr>
<tr><td>InsidePatternNgram(string patternNameAndIndex)</td><td>string</td><td>Gives the previous tokeniser decision for the atomic token just preceding the one indicated by a given index in the given pattern.<br/>
Useful for ensuring that inner-pattern decisions are always respected (unless two patterns overlap in the same sequence),
thus ensuring that multi-token compound words are either made compound as a whole, or not at all.<br/>
The patternNameAndIndex should give a pattern name, followed by the character ¤, followed by the index to test.</td>
<tr><td>PatternOffset(string patternName, integer offset)</td><td>tokenAddress</td><td>Returns a token
offset from the TokeniserPattern containing the present token.<br/>
This allows us to find the word preceding a given compound candidate, or following a given compound candidate.<br/>
Returns null if the offset goes outside the token sequence.<br/></td></tr>
</table>

<p>The current release of Talismane for French contained the following pattern tokeniser features:</p>
<pre>CurrentPattern	TokeniserPatterns
CurrentPatternWordForm	PatternWordForm(CurrentPattern)
PatternNgram	ConcatNoNulls(TokeniserPatternsAndIndexes,InsidePatternNgram(TokeniserPatternsAndIndexes))
PrevTokenPosTag	OnlyTrue(LexiconPosTag(PatternOffset(CurrentPattern, -1), PosTagSet))
NextTokenPosTag	OnlyTrue(LexiconPosTag(PatternOffset(CurrentPattern, 1), PosTagSet))
PrevTokenUnknown	OnlyTrue(UnknownWord(PatternOffset(CurrentPattern, -1)))
TokenP2Unknown|TokenP1WordForm	ConcatNoNulls(CurrentPattern,ToStringNoNulls(OnlyTrue(UnknownWord(PatternOffset(CurrentPattern, -2)))), WordForm(PatternOffset(CurrentPattern, -1)))
NextTokenUnknown	OnlyTrue(UnknownWord(PatternOffset(CurrentPattern, 1)))
TokenN1WordForm|TokenN2Unknown	ConcatNoNulls(CurrentPattern,WordForm(PatternOffset(CurrentPattern, 1)), ToStringNoNulls(OnlyTrue(UnknownWord(PatternOffset(CurrentPattern, 2)))))
PrevTokenAllPosTags	ConcatNoNulls(CurrentPattern,LexiconAllPosTags(PatternOffset(CurrentPattern, -1)))
TokenP2AllPosTags|TokenP1WordForm	ConcatNoNulls(CurrentPattern,LexiconAllPosTags(PatternOffset(CurrentPattern, -2)), WordForm(PatternOffset(CurrentPattern, -1)))
TokenP2AllPosTags|TokenP1AllPosTags	ConcatNoNulls(CurrentPattern,LexiconAllPosTags(PatternOffset(CurrentPattern, -2)), LexiconAllPosTags(PatternOffset(CurrentPattern, -1)))
TokenP2WordForm|TokenP1AllPosTags	ConcatNoNulls(CurrentPattern,WordForm(PatternOffset(CurrentPattern, -2)), LexiconAllPosTags(PatternOffset(CurrentPattern, -1)))
NextTokenAllPosTags	ConcatNoNulls(CurrentPattern,LexiconAllPosTags(PatternOffset(CurrentPattern, 1)))
TokenN1WordForm|TokenN2AllPosTags	ConcatNoNulls(CurrentPattern,WordForm(PatternOffset(CurrentPattern, 1)), LexiconAllPosTags(PatternOffset(CurrentPattern, 2)))
TokenN1AllPosTags|TokenN2AllPosTags	ConcatNoNulls(CurrentPattern,LexiconAllPosTags(PatternOffset(CurrentPattern, 1)), LexiconAllPosTags(PatternOffset(CurrentPattern, 2)))
TokenN1AllPosTags|TokenN2WordForm	ConcatNoNulls(CurrentPattern,LexiconAllPosTags(PatternOffset(CurrentPattern, 1)), WordForm(PatternOffset(CurrentPattern, 2)))
PrevTokenSuffix	NullIf(Not(UnknownWord(PatternOffset(CurrentPattern, -1))),ConcatNoNulls(CurrentPattern,NLetterSuffix(PatternOffset(CurrentPattern, -1),IndexRange(3,5))))
NextTokenSuffix	NullIf(Not(UnknownWord(PatternOffset(CurrentPattern, 1))),ConcatNoNulls(CurrentPattern,NLetterSuffix(PatternOffset(CurrentPattern, 1),IndexRange(3,5))))
PrevTokenWordForm	ConcatNoNulls(CurrentPattern,WordForm(PatternOffset(CurrentPattern, -1)))
TokenP2WordForm|TokenP1WordForm	ConcatNoNulls(CurrentPattern,WordForm(PatternOffset(CurrentPattern, -2)), WordForm(PatternOffset(CurrentPattern, -1)))
NextTokenWordForm	ConcatNoNulls(CurrentPattern,WordForm(PatternOffset(CurrentPattern, 1)))
TokenN1WordForm|TokenN2WordForm	ConcatNoNulls(CurrentPattern,WordForm(PatternOffset(CurrentPattern, 1)), WordForm(PatternOffset(CurrentPattern, 2)))
FirstWord	OnlyTrue(PatternIndexInSentence(CurrentPattern)==0)
FirstWordPerWordForm	ConcatNoNulls(CurrentPattern,CurrentPatternWordForm(CurrentPattern),ToStringNoNulls(OnlyTrue(PatternIndexInSentence(CurrentPattern)==0)))
FirstWordOrAfterPunct	PatternIndexInSentence(CurrentPattern)==0 | NullToFalse(LexiconPosTag(PatternOffset(CurrentPattern, -1),"PONCT"))
PrevTokenVerbLemma	ConcatNoNulls(CurrentPattern,LemmaForPosTag(PatternOffset(CurrentPattern,-1),"V","VS","VIMP","VPP","VINF","VPR"))
PrevVerbLemma	ConcatNoNulls(CurrentPattern,LemmaForPosTag(BackwardSearch(PatternOffset(CurrentPattern,-1),LexiconPosTag("V")|LexiconPosTag("VS")|LexiconPosTag("VIMP")|LexiconPosTag("VPP")|LexiconPosTag("VINF")|LexiconPosTag("VPR"), 0, -4),"V","VS","VIMP","VPP","VINF","VPR"))</pre>

<a name="posTagger"></a>
<h3>The Pos-Tagger</h3>
<p>The Pos-Tagger takes a token within a sequence of tokens representing the sentence, and assigns it a pos-tag. The <a href="#tagset">tagset</a> is fixed by the statistical model, which has to be trained to produce results in a particular tagset. Changing the tagset thus involves the following steps:</p>
<li>defining the tagset in a file with a specific format</li>
<li>mapping the morpho-syntaxic categories found in the training corpus to tags in the tagset, using a file in a specific format. Note that this is a many-to-many mapping.</li>
<li>mapping the morpho-syntaxic categories found in the lexicon to tags in the tagset, using a file in a specific format. Note that this is a many-to-many mapping.</li>
<li>possibly, rewriting pos-tagger features and/or rules to refer to the new set of postags</li>
<li>retraining the pos-tagger's statistical model</li>
<li>possibly, rewriting parser features to refer to the new set of postags</li>
<li>retraining the parser's statistical model</li>
<p>The pos-tagger is currently the only module to allow for rules that bypass the statistical model. Writing a new set of rules is described <a href="#rulesFile">here</a>.</p>

<h4>Specific feature functions available to the pos-tagger</h4>
<p>Pos-tagger rules and features can include any functions defined by the tokeniser, except for those that are specific to patterns. In this case, functions will be applied to the tokens found by the tokeniser (whereas the tokeniser applied them to atomic tokens). In addition, the pos-tagger defines the following pos-tagger specific functions:</p>
<table border="1">
<tr><th>Function</th><th>Type</th><th>Description</th></tr>
<tr><td>Ngram(integer n)</td><td>string</td><td> Retrieves and concatenates the tags assigned to the previous N tokens.<br/>
Will only return results if the current index >= N-2 (to avoid multiple start tokens).<br/>
This ensures that we don't repeat exactly the same information in 4-grams, trigrams, bigrams, etc...</td></tr>
<tr><td>History(integer offset)</td><td>posTaggedTokenAddress</td><td>Looks into the current history of analysis, and retrieves the pos-tagged token at position n with respect to the current token, where n is a negative integer.</td></tr>
</table>
<br/>
<p>The following is a list of PosTaggedTokenFeatures - features that can only be applied to a pos-tagged token, and therefore can only be applied to tokens already pos-tagged in the current analysis (available via the <tt>History</tt> feature above). Note that these features can also be used by the parser, if an additional address function is added as a first parameter.</p>
<table border="1">
<tr><th>Function</th><th>Type</th><th>Description</th></tr>
<tr><td>Category(posTaggedTokenAddress)</td><td>string</td><td>The main grammatical category of a given token as supplied by the lexicon.</td></tr>
<tr><td>ClosedClass(posTaggedTokenAddress)</td><td>boolean</td><td>Whether or not the pos-tag assigned to this token is a closed-class category.</td></tr>
<tr><td>Gender(posTaggedTokenAddress)</td><td>string</td><td>The grammatical gender of a given token as supplied by the lexicon.</td></tr>
<tr><td>Index(posTaggedTokenAddress)</td><td>integer</td><td>The index of a given token in the token sequence.</td></tr>
<tr><td>Lemma(posTaggedTokenAddress)</td><td>string</td><td>The "best" lemma of a given token as supplied by the lexicon.</td></tr>
<tr><td>LexicalForm(posTaggedTokenAddress)</td><td>string</td><td>The actual text of a given token.</td></tr>
<tr><td>Morphology(posTaggedTokenAddress)</td><td>string</td><td>The detailed morpho-syntaxic information of a given token as supplied by the lexicon.</td></tr>
<tr><td>Number(posTaggedTokenAddress)</td><td>string</td><td>The grammatical number of a given token as supplied by the lexicon.</td></tr>
<tr><td>Person(posTaggedTokenAddress)</td><td>string</td><td>The grammatical person of a given token as supplied by the lexicon.</td></tr>
<tr><td>PossessorNumber(posTaggedTokenAddress)</td><td>string</td><td>The grammatical number of the possessor of a given token as supplied by the lexicon.</td></tr>
<tr><td>PosTag(posTaggedTokenAddress)</td><td>string</td><td>The pos-tag assigned a given token.</td></tr>
<tr><td>PredicateHasFunction(posTaggedTokenAddress, string functionName)</td><td>boolean</td><td>For this pos-tagged token's main lexical entry, does the predicate have the function provided?</td></tr>
<tr><td>PredicateFunctionHasRealisation(posTaggedTokenAddress, string functionName, string realisationName)</td><td>boolean</td><td>For this pos-tagged token's main lexical entry, assuming the function name provided is in the list, does it have the realisation provided? If the function name provided is not in the list, returns null.</td></tr>
<tr><td>PredicateFunctionIsOptional(posTaggedTokenAddress, string functionName)</td><td>boolean</td><td>For this pos-tagged token's main lexical entry, assuming the function name provided is in the list, is it optional? If the function name provided is not in the list, returns null.</td></tr>
<tr><td>PredicateFunctionPosition(posTaggedTokenAddress, string functionName)</td><td>integer</td><td>For this pos-tagged token's main lexical entry, assuming the function name provided is in the list, what is its index? If the function name provided is not in the list, returns null.</td></tr>
<tr><td>PredicateHasMacro(posTaggedTokenAddress, string macroName)</td><td>boolean</td><td>For this pos-tagged token's main lexical entry, does the predicate have the macro provided?</td></tr>
<tr><td>Tense(posTaggedTokenAddress)</td><td>string</td><td>The tense of a given token as supplied by the lexicon.</td></tr>
</table>

<div class="info">The above features have a <tt>posTaggedTokenAddress</tt> argument as the first argument.
A posTaggedTokenAddress function is a function that, given a reference posTaggedToken, can return another posTaggedToken.
The results of a posTaggedTokenAddress function can be used in place of a tokenAddress function for a tokeniser feature.
The parser can also provide posTaggedTokenAddress functions to feed into pos-tagger features - see below for details.
</div>

<br/>
<div class="info">
<b>PosTag(History(n)) vs. LexiconPosTag</b>
<p>Although <tt>PosTag(History(n))=="x"</tt> and <tt>LexiconPosTag(Offset(n),"x")</tt> are similar, there is a fundamental difference between them:</p>
<ul>
<li><tt>PosTag(History(n))=="x"</tt> examines the actual pos-tag assigned by the pos-tagger to a given token in the current analysis.
As such, it can <b>only</b> be used with tokens preceding the current one, since they have already been assigned a pos-tag.</li>
<li><tt>LexiconPosTag(Offset(n),"x")</tt> takes a pos-tag as a parameter, and checks if a given token is listed in the lexicon as having that pos-tag.
As such, it can be used for any token, including tokens following the current one which have not yet been analysed.</li>
</ul>
<p>When a feature deals with a previous token to the current one, <tt>PosTag(History(n))=="x"</tt> is almost always preferable to <tt>LexiconPosTag(Offset(n),"x")</tt>. Thus, we would typically use <tt>PosTag(History(n))=="x"</tt> for all negative offsets, and LexiconPosTag for all offsets >= 0, as in: <tt>PosTag(History(-1))=="DET" & Word("deux","trois","quatre") & LexiconPosTag(Offset(1),"NC")</tt></p>
</div>

<p>The current release of Talismane for French contained the following pos-tagger features:</p>
<pre>
LemmaOrWord(X)	IfThenElse(IsNull(Lemma(X)),LexicalForm(X),Lemma(X))
WordFormRange	WordForm(Offset(IndexRange(-1,1)))
Ngram(2)
Ngram(3)
NLetterPrefix(IndexRange(2,5))
NLetterSuffix(IndexRange(2,5))
LexiconPosTagF1	LexiconPosTags(Offset(1))
LexiconAllPosTags
LexiconAllPosTagsF1	LexiconAllPosTags(Offset(1))
WordFormF1LexiconAllPosTagsF2	ConcatNoNulls(WordForm(Offset(1)), LexiconAllPosTags(Offset(2)))
WordFormF1WordFormF2	ConcatNoNulls(WordForm(Offset(1)), WordForm(Offset(2)))
LemmaB1	LemmaOrWord(History(-1))
LemmaB1WordForm	ConcatNoNulls(LemmaOrWord(History(-1)),WordForm())
LemmaB2LemmaB1	ConcatNoNulls(LemmaOrWord(History(-2)),LemmaOrWord(History(-1)))
PosTagB2LemmaB1	ConcatNoNulls(PosTag(History(-2)),LemmaOrWord(History(-1)))
LemmaB2PosTagB1	ConcatNoNulls(LemmaOrWord(History(-2)),PosTag(History(-1)))
LemmaB2LemmaB1WordForm	ConcatNoNulls(LemmaOrWord(History(-2)),LemmaOrWord(History(-1)),WordForm())
PosTagB2LemmaB1WordForm	ConcatNoNulls(PosTag(History(-2)),LemmaOrWord(History(-1)),WordForm())
LemmaB2PosTagB1WordForm	ConcatNoNulls(LemmaOrWord(History(-2)),PosTag(History(-1)),WordForm())
PosTagB2PosTagB1WordForm	ConcatNoNulls(PosTag(History(-2)),PosTag(History(-1)),WordForm())
UnknownB2PosTagB1	ConcatNoNulls(ToStringNoNulls(OnlyTrue(UnknownWord(History(-2)))),PosTag(History(-1)))
PosTagB2UnknownB1	ConcatNoNulls(PosTag(History(-2)),ToStringNoNulls(OnlyTrue(UnknownWord(History(-1)))))
OnlyTrue(LastWordInSentence())
OnlyTrue(FirstWordInSentence() & LexiconPosTag("CLS"))
IsMonthName()	Word("janvier","février","mars","avril","mai","juin","juillet","août","septembre","octobre","novembre","décembre","jan.","jan","fév.","fév","avr.","avr","juil.","juil","juill.","juill","sept.","sept","oct.","oct","nov.","nov","déc.","déc")
IsDayOfMonth	Word("31") & AndRange(NullToFalse(Word("31","au","[[au]]",",","et","à")), 0, ForwardLookup(IsMonthName())-1)
ContainsSpace	OnlyTrue(Regex(".+ .+"))
ContainsPeriod	OnlyTrue(Regex(".*[^.]\.[^.].*"))
EndsWithPeriod	OnlyTrue(Regex("(.*[^.])\."))
ContainsHyphen	OnlyTrue(Regex(".+\-.+"))
ContainsNumber	OnlyTrue(Regex(".*\d.*"))
FirstLetterCapsSimple	OnlyTrue(Regex("[A-Z].*"))
FirstLetterCaps	OnlyTrue(Regex("[A-Z][^A-Z].*"))
FirstWordInCompoundOrWord	IfThenElse(ContainsSpace(),FirstWordInCompound(),WordForm())
LastWordInCompound()
IsNumeric	OnlyTrue(Regex("\d+(,\d+)?"))
AllCaps	OnlyTrue(Regex("[A-Z '\-]+") & Regex(".*[A-Z][A-Z].*"))
UnknownWordB1	OnlyTrue(UnknownWord(Offset(-1)) & Not(ContainsSpace(Offset(-1))))
UnknownWordF0	OnlyTrue(UnknownWord() & Not(ContainsSpace()))
UnknownWordF1	OnlyTrue(UnknownWord(Offset(1)) & Not(ContainsSpace(Offset(1))))
NegativeAdverb()	NullToFalse(Word("aucun","aucune","jamais","guère","ni","pas","personne","plus","point","que","qu'","rien"))
QueFollowingNeg	Word("que","qu'") & OrRange(NegativeAdverb, BackwardLookup(Word("ne","n'")), -1)
LexiconPosTagCurrentWord	LexiconPosTagsForString(IfThenElse(UnknownWord&NullToFalse(ContainsSpace),FirstWordInCompound,WordForm))
LeOrLaFollowedByUpperCase	Word("Le","La","Les","L'") & FirstLetterCapsSimple(Offset(1))</pre>

<a name="parser"></a>
<h3>The Parser</h3>
<p>In shallow <b>dependency parsing</b>, a labelled dependency arc is drawn between each token in the sentence and at most one governor. Tokens without a governor are attached to an artificial "root" node. Circular dependencies are not permitted. Converting dependency parsing to a classification problem is done in Talismane via a shift-reduce algorithmas per [Nivre, 2008]. Thus, the parser is given a <b>parse configuration</b> consisting of three structures:</p>
<li><b>the stack:</b> a list of partially analysed pos-tagged tokens, which are handled in a LIFO (last-in-first-out) fashion</li>
<li><b>the buffer:</b> a list of yet unanalysed pos-tagged tokens, which are handled in the order in which they appear in the sentence</li>
<li><b>the dependencies:</b> a set of dependency arcs which have already been created, where each arc is of the form (dependent, governor, label)</li>
<p>Given a particular parse configuration, the parser must select a <b>transition</b> from the set of allowable transitions, which will modify the parse configuration in a specific way, thus generating a new parse configuration. Typical transitions perform actions such as:</p>
<li>create a "left" dependency from the head of the stack (its top-most token) to the head of the buffer (the next token to be analysed)</li>
<li>create a "right" dependency from the head of the buffer to the head of the stack</li>
<li>shift the head of the buffer to the head of the stack</li>
<p>Depending on the transition scheme, transitions can also perform actions such as removing the top-most element in the stack or in the buffer. Transitions which generate dependencies are generally represented by <i>n</i> different transitions, one per allowable dependency label. Parsing may now be restated as a classification task as follows: given a parse configuration, which transition (selected from a closed pre-defined set) is the correct one to apply.</p>
<p>Parser features thus apply to parse configurations, and can draw upon any information included within the parse configuration. This includes information concerning the pos-tagged token in a specific position in the stack or buffer, information about the existing dependencies of this token, or information relating two tokens (e.g. top-of-stack and top-of-buffer) such as the distance between them.</p>

<h4>Specific feature functions available to the parser</h4>
<p>The following are the list of "address" functions which take a parse configuration and return a particular pos-tagged token:</p>
<table border="1">
<tr><th>Function</th><th>Type</th><th>Description</th></tr>
<tr><td>Buffer(integer index)</td><td>address</td><td>Retrieves the nth item from the buffer.</td></tr>
<tr><td>Dep(address referenceToken, integer index)</td><td>address</td><td>Retrieves the nth dependent of the reference token.</td></tr>
<tr><td>Head(address referenceToken)</td><td>address</td><td>Retrieves the head (or governor) of the reference token.</td></tr>
<tr><td>Offset(address referenceToken, integer offset)</td><td>address</td><td>Retrieves the token offset from the current token by <i>n</i> (in the linear sentence),
where <i>n</i> can be negative (before the current token) or positive (after the current token).<br/>
The "current token" is returned by the address function.</td></tr>
<tr><td>LDep(address referenceToken)</td><td>address</td><td>Retrieves the left-most left-hand dependent of the reference token.</td></tr>
<tr><td>RDep(address referenceToken)</td><td>address</td><td>Retrieves the right-most right-hand dependent of the reference token.</td></tr>
<tr><td>Seq(integer index)</td><td>address</td><td>Retrieves the nth pos-tagged token in the sequence of tokens.</td></tr>
<tr><td>Stack(integer index)</td><td>address</td><td>Retrieves the nth item from the stack.</td></tr>
<tr><td>ForwardSearch(address referenceToken, boolean criterion)</td><td>address</td><td>Looks at all pos-tagged tokens following the reference token in sequence, and returns the first one matching certain criteria.</td></tr>
<tr><td>BackwardSearch(address referenceToken, boolean criterion)</td><td>address</td><td>Looks at all pos-tagged tokens preceding the reference token in sequence (starting at the one closest to the reference token), and returns the first one matching certain criteria.</td></tr>
</table>

<p>The following are the list of feature functions which act directly on a pos-tagged token returned by an address function:</p>
<table border="1">
<tr><th>Function</th><th>Type</th><th>Description</th></tr>
<tr><td>DependencyLabel(address)</td><td>string</td><td>The dependency label of a given token's governing dependency,
where the token is referenced by address.</td></tr>
</table>

<p>In addition, all PosTaggedToken features from the pos-tagger above may be used by the parser, as long as an address is provided as the first parameter. The address will retrieve a pos-tagged token to apply the feature to.</p>

<p>The following additional functions allow comparison between two tokens or information about token dependents:</p>
<table border="1">
<tr><th>Function</th><th>Type</th><th>Description</th></tr>
<tr><td>DependentCountIf(address referenceToken, boolean criterion)</td><td>integer</td><td>Returns the number of dependents already matching a certain criterion.</td></tr>
<tr><td>BetweenCountIf(address token1, address token2, boolean criterion)</td><td>integer</td><td>Returns the number of pos-tagged tokens between two pos-tagged tokens (and not including them) matching a certain criterion.</td></tr>
<tr><td>Distance(address token1, address token2)</td><td>integer</td><td>Returns the distance between the token referred to by addressFunction1 and the token
referred to by addressFunction2, as an absolute value from 0 to n.</td></tr>
</table>

<p>The current "baseline" parser features for Talismane for French are the following:</p>
<pre>
# Functions for use below
IfThenElseNull(X,Y)	NullIf(Not(X),Y)
IsVerb(X)	PosTag(X)=="V" | PosTag(X)=="VS" | PosTag(X)=="VIMP" | PosTag(X)=="VPP" | PosTag(X)=="VINF" | PosTag(X)=="VPR"
LemmaOrWord(X)	IfThenElse(IsNull(Lemma(X)), LexicalForm(X), Lemma(X))
Lexicalised(X)	IfThenElse(ClosedClass(X), LemmaOrWord(X), PosTag(X))
LexicalisedVerb(X)	IfThenElse(ClosedClass(X) | IsVerb(X), LemmaOrWord(X), PosTag(X))

# Main features
PosTagStack0	PosTag(Stack[0])
PosTagBuffer0	PosTag(Buffer[0])
LemmaStack0PosTagBuffer0	Concat(LemmaOrWord(Stack[0]),PosTag(Buffer[0]))
PosTagStack0LemmaBuffer0	Concat(PosTag(Stack[0]),LemmaOrWord(Buffer[0]))
LemmaStack0LemmaBuffer0	Concat(LemmaOrWord(Stack[0]),LemmaOrWord(Buffer[0]))
PosTagBuffer1()	PairGroup	PosTag(Buffer[1])
PosTagBuffer12()	PairGroup	ConcatNoNulls(PosTag(Buffer[1]), PosTag(Buffer[2]))
PosTagBuffer123()	PairGroup	ConcatNoNulls(PosTag(Buffer[1]), PosTag(Buffer[2]), PosTag(Buffer[3]))
PosTagStack1()	PairGroup	PosTag(Stack[1])
PosTagBeforeStack()	PairGroup	PosTag(Offset(Stack[0],-1))
PosTagAfterStack()	PairGroup	PosTag(Offset(Stack[0],1))
PosTagBeforeBuffer()	PairGroup	PosTag(Offset(Buffer[0],-1))
PosTagAfterBuffer()	PairGroup	PosTag(Offset(Buffer[0],1))
DepLabelStack()	PairGroup	DependencyLabel(Stack[0])
DepLabelLDepStack()	PairGroup	DependencyLabel(LDep(Stack[0]))
DepLabelRDepStack()	PairGroup	DependencyLabel(RDep(Stack[0]))
DepLabelLDepBuffer()	PairGroup	DependencyLabel(LDep(Buffer[0]))
LexicalFormStack0()	PairGroup	LexicalForm(Stack[0])
LexicalFormBuffer0()	PairGroup	LexicalForm(Buffer[0])
LexicalFormBuffer1()	PairGroup	LexicalForm(Buffer[1])
LexicalFormStackHead()	PairGroup	LexicalForm(Head(Stack[0]))
LemmaStack0()	PairGroup	Lemma(Stack[0])
LemmaBuffer0()	PairGroup	LemmaOrWord(Buffer[0])
LemmaBuffer1()	PairGroup	LemmaOrWord(Buffer[1])
LemmaStackHead()	PairGroup	LemmaOrWord(Head(Stack[0]))
GenderMatch()	PairGroup	Concat(Gender(Stack[0]),Gender(Buffer[0]))
NumberMatch()	PairGroup	Concat(Number(Stack[0]),Number(Buffer[0]))
TenseStack0()	PairGroup	Tense(Stack[0])
TenseBuffer0()	PairGroup	Tense(Buffer[0])
TenseBuffer1()	PairGroup	Tense(Buffer[1])
TenseStackHead()	PairGroup	Tense(Head(Stack[0]))
TenseMatch()	PairGroup	NullIf(IsNull(Tense(Stack[0])) & IsNull(Tense(Buffer[0])), Concat(Tense(Stack[0]),Tense(Buffer[0])))
PersonMatch()	PairGroup	NullIf(IsNull(Person(Stack[0])) & IsNull(Person(Buffer[0])), Concat(Person(Stack[0]),Person(Buffer[0])))
MorphologyMatch()	PairGroup	Concat(Morphology(Stack[0]),Morphology(Buffer[0]))

# Complete features above with info from the top-of-stack and top-of-buffer
PosTagPair	Concat(PosTag(Stack[0]),PosTag(Buffer[0]))
LexicalisedPair	IfThenElseNull(ClosedClass(Stack[0]) | ClosedClass(Buffer[0]), Concat(Lexicalised(Stack[0]),Lexicalised(Buffer[0])))
LexicalisedVerbPair	IfThenElseNull(IsVerb(Stack[0]) | IsVerb(Buffer[0]), Concat(LexicalisedVerb(Stack[0]),LexicalisedVerb(Buffer[0])))
PairGroup_P	ConcatNoNulls(PosTagPair(), PairGroup())
PairGroup_L	ConcatNoNulls(LexicalisedPair(), PairGroup())
PairGroup_V	ConcatNoNulls(LexicalisedVerbPair(), PairGroup())</pre>

<h2>Analysis mechanism</h2>
<a name="beamSearch"></a>
<h3>Beam search</h3>
<p>Almost all of Talismane's modules make use of the <b>beam search</b> algorithm to reduce the search space. In a beam search, at each step of classification, the machine sorts the classifications by descending probability. Only the top <i>n</i> classifications are then considered for the next step, where <i>n</i> is called the <b>beam width</b>. For example, assume a beam width of 2. While pos-tagging, we determine that the first token is 50% likely to be a noun, 30% likely to be an adjective, and 20% likely to be a verb. Because of the beam width, we will only retain the 2 most likely classifications (noun, adjective) when analysing the second token. The pos-tag assigned to the first token affects the pos-tag probabilities for the second token (via n-gram and similar features). Note that we have excluded the possibility starting with a verb up front, even if it ended up being the correct choice in view of tokens downstream. Thus, the beam search is a method of maintaining linear complexity, at the cost of heavy pruning at each stage of analysis.</p>
<p>For a beam search to function, there are several requirements:</p>
<li>a classifier which assigns a probability to each classification</li>
<li>a method for scoring an analysis as a whole - typically the sum of the logs of probabilities for each step</li>
<li>a method of grouping analyses by "degree of progress", so that we are comparing "like with like"</li>
<p>The first point above is dealt with in the <a href="#maxent">probabilistic classifier</a> section below.</p>
<p>The second and third points are straightforward when we compare sequential analyses, as in the case of a simple left-to-right pos-tagger. It is more subtle when we compare parser analyses, as they are not handling the tokens in a sequential order. We now have to decide whether to compare analyses by the number of transitions, the number of dependencies created, etc., and how to calculate a total score without necessarily favouring the shortest path to a solution. One option is discussed in [Sagae and Lavie, 2006].</p>

<a name="maxent"></a>
<h3>Probabilistic classifier</h3>
<p>As mentioned above, the beam search algorithm used by Talismane requires a probabilistic classifier, which can generate a probability distribution of classifications for each context to be classified.</p>
<p>Talismane has three probabilistic classifier implementations currently available:</p>
<ul>
<li><b>Maximum Entropy</b>:  also known as <b>MaxEnt</b> (see [Ratnaparkhi, 1998]), using the <a href="http://opennlp.apache.org/">Apache OpenNLP</a> Maxent implementation.</li>
<li><b>Perceptrons</b>: also using the <a href="http://opennlp.apache.org/">Apache OpenNLP</a> implementation.</li>
<li><b>Linear SVM</b>: using the <a href="http://liblinear.bwaldvogel.de/">LibLinear-Java</a> adaptation of the original <a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LibLinear</a> implementation [see Ho and Lin, 2012].</li>
</ul>
<p>In order to function, statistical probabilistic classifiers convert the training corpus into a series of <b>events</b>. Each event reduces a given linguistic context in the training corpus to a set of features (and feature results), and a correct classification for this context. Thus, if in the pos-tagger, our only two features are <tt>sfx3</tt>, the three-letter suffix of a token, and <tt>ngram2</tt>, the pos-tag of the previous token, we would reduce the second token in the sentence "Le/DET lait/NC a/V tourné/VPP" to the event ([sfx3:ait, ngram2:DET], class:NC) - and this is the full extent of the classifier's knowledge of this context. The linguist's role is to select features likely to be informative for the task at hand. More subtly, the linguist must try to define features as generically as possible while maintaining their information content, so that they will project well onto new contexts outside of the training corpus.</p>
<p>The statistical model generally assigns a weight to each feature/classification combination. Some features are found to be more informative for some classifications, while others are more informative for others. However, the a posteriori knowledge of weights assigned does not make it any easier to garner an understanding of the relative importance of features in the overall classification accuracy. First of all, we are typically dealing with tens of thousands of features, dozens of which act in combination on any linguistic context. Secondly, we could be dealing with numeric features (e.g. distance between two tokens), in which case the weight assigned to the feature interacts with its value in a given context to give the final result. Thus, the statistical model itself can be viewed as a <b>black box</b> for the linguist: it provides fairly accurate results, but looking inside it to try to gain a better understanding of the language is a fairly hopeless task.</p>

<a name="includeDetails" />
<h4>Looking inside the black box</h4>
<p>Still, Talismane does provide a way of reviewing the exact mathematics behind any specific decision made during analysis, through the argument <tt>includeDetails=true</tt>, which creates a file containing all of the mathematical details: the feature results returned for each context, the weight and value for each feature, and the final probability distribution. The command would look like:</p>
<pre class="shell">
java -Xmx1024M -jar talismane-fr-<i>version</i>.jar command=analyse inFile=corpus.txt outFile=postag_corpus.txt encoding=UTF-8 includeDetails=true</pre>
<div class="info"><b>Warning:</b> even for a very small analysis, the file produced can be huge - typically a few megabytes for just one sentence. A user running this command for anything more than a simple sentence is likely to drown in a sea of details.</div>

<a name="auto-evaluation"></a>
<h3>Auto-evaluation</h3>

<p>It is possible to evaluate the performance Talismane's pos-tagger and parser modules on any annotated data which uses the same tagset (and dependency labels for the parser). The following steps are typically involved:</p>
<ul>
<li>Analyse a new corpus</li>
<li>Correct the analysis manually</li>
<li>Evaluate the corrected corpus</li>
<li>Review the errors</li>
</ul>

<p>Auto-evaluation allows us to gauge Talismane's accuracy for corpora other than the training corpus, in this case the <a href="#ftb">French Treebank</a>. It also allows us to construct an evaluation corpus semi-automatically, and thus be able to measure improvement in Talismane's accuracy for a wide variety of corpora based on changes in training or analysis configuration.</p>

<p>To illustrate the auto-evaluation procedure, an example corpus (in French) will be used, taken from the <a href="http://fr.wikipedia.org/wiki/Discussion:Organisme_g%C3%A9n%C3%A9tiquement_modifi%C3%A9">discussion page</a> of the wikipedia article on <i>Organisme Génétiquement Modifié</i> (genetically modified organisms). In this example, the discussion will be placed in a file called <tt>corpus.txt</tt>. We will discuss auto-evaluation for the pos-tagger module, but a very similar procedure can be used for the parser module.</p>

<h4>Analysing a new corpus</h4>
<p>First of all, we need to create the file <tt>corpus.txt</tt> containing the text to be evaluated. In our case, the top of the file will be:</p>
<pre>
Nouvelle page de discussion

Pour plus de lisibilité sur cette page, les anciennes discussions ont été archivées. Pour relancer une discussion ancienne, il suffit de consulter les archives et de copier-coller le contenu souhaité dans la présente page.

Question de définition

Bonsoir. Afin de ne pas raconter n'importe quoi, l'insertion de gène d'une même espèce conduit-il à un OGM ? Exemple : transfert d'un gène d'une variété de blé résistant à un insecte à une autre variété de blé ? Si oui, cette précision manque dans la définition, car on a toujours l'impression d'avoir affaire à des chimères.--Manu (discuter) 29 avril 2008 à 22:53 (CEST)...</pre>

<p>Next, we need to get Talismane to analyse this corpus automatically, assigning pos-tags to the tokens. The pos-tagged corpus will be stored in a file called <tt>pre_postag_corpus.txt</tt>, as follows:</p>
<pre>
java -Xmx1024M -jar talismane-fr-<i>version</i>.jar command=analyse startModule=sentence endModule=postag inFile=corpus.txt outFile=pre_postag_corpus.txt encoding=UTF-8</pre>

<p>Note that the auto-evaluation is currently performed on the pos-tags only, and not on the lemmas and additional morpho-syntaxic information provided by the glossary.
The user may wish to correct the pos-tags only, in which case he can perform a Unix cut on the file to get rid of the additional columns as follows:</p>
<pre class="shell">
cut -f2,4 &lt; pre_postag_corpus.txt &gt; postag_corpus.txt</pre>
	
<p>The file <tt>postag_corpus.txt</tt> should now look something like:</p>
<pre>
Nouvelle	ADJ
page	NC
de	P
discussion	NC

Pour	P
plus	ADV
de	P
lisibilité	NC
sur	P
..	..</pre>

<h4>Correcting the analysis manually</h4>
<p>Given the file <tt>postag_corpus.txt</tt>, we now need to check manually whether the sentence detection and tokenisation have found all the correct tokens, and whether correct pos-tag has been assigned to each token. Regarding French pos-tags, the user is referred to the <a href="www.llf.cnrs.fr/Gens/Abeille/guide-morpho-synt.02.pdf">morpho-syntaxic guide</a> accompanying the French Treebank project. When correcting the pos-tag, the user has to be careful to choose a pos-tag from the pre-defined tagset, and to respect the tab characters within the file.</p>

<h4>Evaluating the corrected corpus</h4>
<p>We can now use Talismane to auto-evaluate its performance on the manually corrected <tt>postag_corpus.txt</tt> file, as follows:</p>
<pre class="shell">
java -Xmx1024M -jar talismane-fr-<i>version</i>.jar command=evaluate module=postag inFile=postag_corpus.txt outDir=eval/ encoding=UTF-8 inputPatternFile=posTagCapture.txt</pre>
<p>If the user has cut the file to keep only two columns, <tt>posTagCapture.txt</tt> would look like:</p>
<pre>TOKEN\tPOSTAG</pre>
<p>Otherwise, if the user has kept the original default pos-tagger output, <tt>posTagCapture.txt</tt> would look like:</p>
<pre>.*\tTOKEN\t.*\tPOSTAG\t.*</pre>
<p>The <tt>outDir</tt> parameter designates a path to the output directory, in this case the <tt>eval</tt> directory, in which the auto-evaluation files will be stored. These files will include:</p>
<ul>
<li><tt>postag_corpus.fscores.csv</tt>: a confusion matrix listing the error counts, precision, recall and f-score by pos-tag.</li>
<li><tt>postag_corpus_sentences.csv</tt>: the full set of evaluated sentences, including the correct answer and the first <i>n</i> responses returned by the beam search.</li>
</ul>

<h4>Reviewing the errors</h4>
<p>Two Perl scripts, available under a GPL license, are provided with the Talismane distribution in order to help extract useful information from the raw <tt>postag_corpus_sentences.csv</tt> file. These are:</p>
<ul>
	<li><b>sentence2errors.pl</b></li>
	<ul>
		<li>Generate statistics</li>
		<li>Create a separate file per error type, containing a .csv file with the sentences for this error type only.</li>
	</ul>
	<li><b>csv2ods.pl</b></li>
	<ul>
		<li>Convert the csv files into xls files in order to color-code the errors.</li>
	</ul>
</ul>
<p>These Perl scripts need to be copied into the <tt>eval</tt> directory, and run as follows:</p>
<pre class="shell">
cp /path/*.pl /path/eval/
cd /path/eval
perl sentence2errors.pl < postag_corpus_sentences.csv</pre>
<p>The new directories <tt>stat</tt> and <tt>corpus_csv</tt> are automatically created. The <tt>stat</tt> directory contains the various statistics files, while the <tt>corpus_csv</tt> directory contains the files with the specific sentences per error type.</p>

<p>The generated .csv files are named after their <tt>error label</tt>. This takes on the form <tt>PosTag1 - PosTAg2</tt>, where PosTag1 is the pos-tag that should have been assigned, and PosTag2 is the pos-tag that was actually assigned. For example, the file <tt>ADJ-ADV.csv</tt> contains all of the sentences where a token was classified as ADV, whereas the evaluation corpus claims it should have been an ADJ.</p>

<p>Inside the <tt>corpus_csv</tt> directory, we can transform the csv files into color-coded xls files, highlighting the errors. The color is red for errors concerning the current file's error label, and green for errors of another type.</p>
<p>A single file can be color-coded using the following command:</p>
<pre class="shell">
perl csv2ods.pl corpus_csv/PosTag1-PosTag2.csv</pre>
<p>All of the files can be color-coded in one fell swoop using the following command:</p>
<pre class="shell">
find corpus_csv/ -iname "*.csv" -exec csv2ods.pl '{}' \;</pre>

<a name="training"></a>
<h2>Training a new statistical model</h2>
<p>Information on how to write implementations of new training corpus readers or new lexicon readers is beyond the scope of the present user's manual. More details on the Java code and interfaces may be found in the project JavaDoc, to which you will find a reference on <a href="http://w3.erss.univ-tlse2.fr/textes/pagespersos/urieli">Assaf Urieli's home page</a> in the CLLE-ERSS laboratory.</p>
<p>However, a user with a license to use the <a href="#ftb">French Treebank</a> corpus may wish to define new feature files and train a new model using the built-in reader for the French Treebank. This is done using the <tt>talismane-trainer-fr-<i>version</i>.jar</tt> file.</p>

<div class="alert"><b>Modified training corpus:</b> Note that the sentence detector and tokeniser currently assume that the XML files of the original French Treebank have been modified to include a <tt>&lt;sentence&gt;</tt> tag immediately following the <tt>&lt;SENT&gt;</tt> tag for each sentence. This <tt>sentence</tt> tag needs to contain the original full sentence text. Anybody wishing to get a copy of this modified corpus for non-commercial purposes can write <a href="http://w3.erss.univ-tlse2.fr:8080/index.jsp?perso=urieli&subURL=index.html">Assaf Urieli</a>. A copy will be provided on proof that the user has an original French Treebank license.</div>
<div class="info"><b>Training corpus directory structure:</b> The training corpus should be separated by the user into training and evaluation subsets. In our case, we took 80% for training, 10% for dev, and 10% for test. This is done simply by creating <tt>training</tt>, <tt>dev</tt> and <tt>test</tt> sub-directories under a parent <tt>treebank</tt> directory, and placing the xml files into these sub-directories.</div>

<h3>Generic training parameters</h3>
<p>Some training parameters are shared by all of the various training modules. These are listed below.</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th></tr>
<tr><td>algorithm</td><td>The machine learning algorithm to use for training. Options are MaxEnt, LinearSVM and Perceptron.</td></tr>
<tr><td>cutoff</td><td>The number of times a feature must appear in the training corpus to be considered - default is 0</td></tr>
<tr><td>sentenceCount</td><td>the maximum number of sentences to process from the training corpus - default is all sentences</td></tr>
<tr><td>crossValidationSize</td><td>the number of parts to use for a cross-validation.</td></tr>
<tr><td>includeIndex</td><td>if crossValidationSize&gt;0, the index of the sentence to include, if we start counting at 0 and reset at zero after reaching the crossValidationSize. Typically used for evaluation only.</td></tr>
<tr><td>excludeIndex</td><td>if crossValidationSize&gt;0, the index of the sentence to exclude, if we start counting at 0 and reset at zero after reaching the crossValidationSize. Typically used for training only.</td></tr>
<tr><td>externalResources</td><td>a path to a directory or file containing external resources referred to by the features. See <a href="#externalResources">External Resources</a> below.</td></tr>
</table>

<p>Depending on the algorithm, different parameters can be included.</p>
<p>For MaxEnt, these are:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th></tr>
<tr><td>iterations</td><td>The maximum number of training iterations to perform</td></tr>
</table>

<p>For LinearSVM, these are:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th></tr>
<tr><td>linearSVMSolver</td><td>Available options are: L2R_LR (L2-regularized logistic regression - primal), L1R_LR (L1-regularized logistic regression), and L2R_LR_DUAL (L2-regularized logistic regression - dual). The default value is L2R_LR</td>
<tr><td>linearSVMCost</td><td>The cost of constraint violation (typically referred to as "C" in SVM literature). Default value: 1.0.</td></tr>
<tr><td>linearSVMEpsilon</td><td>The stopping criterion. Default value: 0.01.</td></tr>
</table>

<p>For Perceptron, these are:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th></tr>
<tr><td>iterations</td><td>The maximum number of training iterations to perform</td></tr>
<tr><td>perceptronAveraging</td><td>Use averaged weighting instead of standard (integer) weighting.</td>
<tr><td>perceptronSkippedAveraging</td><td>See the Apache OpenNLP documentation for more details on skipped averaging.</td></tr>
<tr><td>perceptronTolerance</td><td>If training set accuracy change &lt; tolerance, stop iterating.</td></tr>
</table>

<h3>Training and evaluating a new Sentence Detector model</h3>
<p>To train a new sentence detector model, the command would be something like:</p>
<pre class="shell">
java -Xmx4000M -jar talismane-trainer-fr-<i>version</i>-allDeps.jar trainer=sentenceDetector command=train treebank=treebank/training sentenceModel=sentenceDetectorModel.zip iterations=200 cutoff=5 sentenceFeatures=sentenceDetectorFeatures.txt</pre>
<p>The parameters are as follows:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th></tr>
<tr><td>sentenceModel</td><td>a path to the file where the new statistical model should be stored - MUST be a zip file</td></tr>
<tr><td>sentenceFeatures</td><td>a path to the file containing the sentence detector feature descriptors</td></tr>
<tr><td>corpus</td><td>a directory containing the training subset of modified XML files from the French Treebank corpus (see <a href="#training">note above</a>)</td></tr>
</table>
<p>To evaluate the new sentence detector model, the command would be something like:</p>
<pre class="shell">
java -Xmx4000M -jar talismane-trainer-fr-<i>version</i>-allDeps.jar trainer=sentenceDetector command=evaluate sentenceModel=sentenceDetectorModel.zip treebank=treebank/dev outDir=output</pre>
<p>The parameters are as follows:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th></tr>
<tr><td>sentenceModel</td><td>a path to the file where the new statistical model was previously stored by the train command</td></tr>
<tr><td>outdir</td><td>a path where to write the output files of the evaluation</td></tr>
<tr><td>corpus</td><td>a directory containing the evaluation subset of modified XML files from the French Treebank corpus (see <a href="#training">note above</a>)</td></tr>
</table>
<p>The evaluation will produce a file <tt><i>model_name</i>_errors.txt</tt>, listing the precision, recall, f-score and errors for each possible sentence boundary type, as well as the total accuracy.</p>

<h3>Training and evaluating a new Tokeniser model</h3>
<p>To train a new tokeniser model, the command would be something like:</p>
<pre class="shell">
java -Xmx4000M -jar talismane-trainer-fr-<i>version</i>-allDeps.jar trainer=tokeniser command=train tokeniserModel=tokeniserModel.zip tokeniserFeatures=tokeniserFeatures.txt tokeniserPatterns=tokeniserPatterns.txt iterations=200 cutoff=5 treebank=treebank/training posTagSet=resources/CrabbeCanditoTagset.txt lefff=resources/lefff.zip</pre>
<p>The parameters are as follows:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th><th>Git Location</th></tr>
<tr><td>tokeniserModel</td><td>a path to the file where the new statistical model should be stored - MUST be a zip file</td><td>&nbsp;</td></tr>
<tr><td>tokeniserFeatures</td><td>a path to the file containing the tokeniser feature descriptors</td><td><tt>talismane_trainer_fr/features</tt></td></tr>
<tr><td>tokeniserPatterns</td><td>a path to the file containing the tokeniser patterns</td><td><tt>talismane_trainer_fr/features</tt></td></tr>
<tr><td>corpus</td><td>a directory containing the training subset of modified XML files from the French Treebank corpus (see <a href="#training">note above</a>)</td><td>&nbsp;</td></tr>
<tr><td>posTagSet</td><td>a path to the file containing the posTagSet to be use. It is required by the Tokeniser for certain features, such as <tt>LexiconPosTag</tt></td><td><tt>talismane_trainer_fr/postags</tt></td></tr>
<tr><td>lexiconDir</td><td>a path to the directory containing a serialised digitised version of any lexicons being used for training.</td>
<td><tt>talismane_trainer_fr/resources<tr><td>tokenFilters</td><td>a path to a file containing any token filters to apply to the token reader before training this tokeniser model. These will automatically be applied before analysing any text with this model.</td><td><tt>talismane_trainer_fr/token_filters.txt</tt></td></tr>
<tr><td>tokenSequenceFilters</td><td>a path to a file containing any token sequence filters to apply to the token reader before training this tokeniser model. These will automatically be applied before analysing any text with this model.</td><td><tt>talismane_trainer_fr/token_sequence_filters.txt</tt></td></tr>
</tt></td></tr>
</table>
<p>To evaluate the new tokeniser model, the command would be something like:</p>
<pre class="shell">
java -Xmx4000M -jar talismane-trainer-fr-<i>version</i>-allDeps.jar trainer=tokeniser command=evaluate tokeniserModel=tokeniserModel.zip treebank=treebank/dev outdir=output posTagSet=resources/CrabbeCanditoTagset.txt lefff=resources/lefff.zip beamWidth=10</pre>
<p>The parameters are as follows:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th><th>Git Location</th></tr>
<tr><td>tokeniserModel</td><td>a path to the file where the new statistical model was previously stored by the train command</td><td>&nbsp;</td></tr>
<tr><td>outdir</td><td>a path where to write the output files of the evaluation</td><td>&nbsp;</td></tr>
<tr><td>corpus</td><td>a directory containing the evaluation subset of modified XML files from the French Treebank corpus (see <a href="#training">note above</a>)</td><td>&nbsp;</td></tr>
<tr><td>posTagSet</td><td>as in training, above</td><td><tt>talismane_trainer_fr/postags</tt></td></tr>
<tr><td>lexiconDir</td><td>a path to the directory containing a serialised digitised version of any lexicons being used for evaluation.</td>
<tr><td>beamWidth</td><td>the beam search beam width for this analysis</td><td>&nbsp;</td></tr>
</table>
<p>The evaluation will produce a file <tt><i>model_name</i>_errors.txt</tt>, listing the precision, recall, f-score and actual errors for each pattern, for all patterns tested by the statistical model, and for the all tokens (including default decisions). It will also produce a (fairly meaningless) confusion matrix giving total counts, precision, recall and f-score for the binary labels DOES_SEPARATE and DOES_NOT_SEPARATE (but which includes the millions of default decisions for tokens that were not even submitted to the statistical model).</p>

<h3>Training and evaluating a new Pos-Tagger model</h3>
<p>To train a new pos-tagger model, the command would be something like:</p>
<pre class="shell">
java -Xmx4000M -jar talismane-trainer-fr-<i>version</i>-allDeps.jar trainer=posTagger command=train posTaggerModel=posTaggerModel.zip posTaggerFeatures=features/posTaggerFeatures.txt iterations=200 cutoff=5 treebank=treebank/training posTagSet=postags/CrabbeCanditoTagset.txt posTagMap=postags/ftbCrabbeCanditoTagsetMap.txt lefff=resources/lefff.zip</pre>
<p>The parameters are as follows:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th><th>Git Location</th></tr>
<tr><td>posTaggerModel</td><td>a path to the file where the new statistical model should be stored - MUST be a zip file</td><td>&nbsp;</td></tr>
<tr><td>posTaggerFeatures</td><td>a path to the file containing the pos-tagger feature descriptors</td><td><tt>talismane_trainer_fr/features</td></tt></tr>
<tr><td>corpus</td><td>a directory containing the training subset of XML files from the French Treebank corpus. For the Pos-Tagger files can be the original FTB files.</td><td>&nbsp;</td></tr>
<tr><td>posTagSet</td><td>a path to the file containing the posTagSet to be used</td><td><tt>talismane_trainer_fr/postags</tt></td></tr>
<tr><td>posTagMap</td><td>a path to the file containing the mapping from FTB morpho-syntaxic labels to the selected tagset</td><td><tt>talismane_trainer_fr/postags</tt></td></tr>
<tr><td>lexiconDir</td><td>a path to the directory containing a serialised digitised version of any lexicons being used for training.</td>
<tr><td>tokenFilters</td><td>a path to a file containing any token filters to apply to the token reader before training this pos-tag model. These will only be applied if this pos-tag model is applied directly, without any tokeniser.</td><td><tt>talismane_trainer_fr/token_filters.txt</tt></td></tr>
<tr><td>tokenSequenceFilters</td><td>a path to a file containing any token sequence filters to apply to the token reader before training this pos-tag model. These will only be applied if this pos-tag model is applied directly, without any tokeniser.</td><td><tt>talismane_trainer_fr/token_sequence_filters.txt</tt></td></tr>
<tr><td>posTaggerPreprocessingFilters</td><td>a path to a file containing any token sequence filters to apply before training this pos-tag model. These will typically add empty tokens where required. These will automatically be applied before analysing any text with this model.</td><td><tt>talismane_trainer_fr/posTagger_preprocessing_filters.txt</tt></td></tr>
</table>
<p>To evaluate the new pos-tagger model, the command would be something like:</p>
<pre class="shell">
java -Xmx4000M -jar talismane-trainer-fr-<i>version</i>-allDeps.jar trainer=posTagger command=evaluate posTaggerModel=posTaggerModel.zip posTaggerRules=features/posTaggerConstraints_fr.txt treebank=treebank/dev outdir=output posTagSet=postags/CrabbeCanditoTagset.txt posTagMap=postags/ftbCrabbeCanditoTagsetMap.txt lefff=resources/lefff.zip beamWidth=10</pre>
<p>The parameters are as follows:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th><th>Git Location</th></tr>
<tr><td>posTaggerModel</td><td>a path to the file where the new statistical model was previously stored by the train command</td><td>&nbsp;</td></tr>
<tr><td>posTaggerRules</td><td>a path to the file containing any rules to be applied to the current analysis</td><td><tt>talismane_trainer_fr/features</tt></td></tr>
<tr><td>outdir</td><td>a path where to write the output files of the evaluation</td><td>&nbsp;</td></tr>
<tr><td>corpus</td><td>a directory containing the evaluation subset of XML files from the French Treebank corpus. For the Pos-Tagger files can be the original FTB files.</td><td>&nbsp;</td></tr>
<tr><td>posTagSet</td><td>as in training, above</td><td><tt>talismane_trainer_fr/postags</tt></td></tr>
<tr><td>posTagMap</td><td>as in training, above</td><td><tt>talismane_trainer_fr/postags</tt></td></tr>
<tr><td>lexiconDir</td><td>a path to the directory containing a serialised digitised version of any lexicons being used for evaluation.</td>
<tr><td>beamWidth</td><td>the beam search beam width for this analysis</td><td>&nbsp;</td></tr>
</table>
<p>The evaluation will produce a file <tt><i>model_name</i>.fscores.csv</tt> containing a confusion matrix and the precision, recall, f-score and error counts for each pos-tag. It will also produce a file, <tt><i>model_name</i>.sentences.csv</tt>, containing all of the sentences evaluated, the correct pos-tag, and the guessed pos-tags from the <i>n</i> evaluations made by the beam search, along with their probabilities.</p>

<h3>Training and evaluating a new Parser model</h3>
<p>To train a new parser model, the command would be something like:</p>
<pre class="shell">
java -Xmx4000M -jar talismane-trainer-fr-<i>version</i>-allDeps.jar trainer=parser train command=train parserModel=parserModel.zip parserFeatures=parserFeatures.txt corpus=resources/CanditoFTBDep/ftb6_1.conll iterations=100 cutoff=5 posTagSet=resources/CrabbeCanditoTagset.txt lefff=resources/lefff.zip</pre>
<p>The parameters are as follows:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th><th>Git Location</th></tr>
<tr><td>parserModel</td><td>a path to the file where the new statistical model should be stored - MUST be a zip file</td><td>&nbsp;</td></tr>
<tr><td>parserFeatures</td><td>a path to the file containing the parser feature descriptors</td><td><tt>talismane_trainer_fr/features</tt></td></tr>
<tr><td>iterations</td><td>the number of training iterations to perform</td><td>&nbsp;</td></tr>
<tr><td>cutoff*</td><td>the number of times a feature must appear in the training corpus to be considered - default is 0</td><td>&nbsp;</td></tr>
<tr><td>corpus</td><td>a training corpus file from the dependency treebank corpus created by [Candito et al, 2010]</td><td>&nbsp;</td></tr>
<tr><td>posTagSet</td><td>a path to the file containing the posTagSet to be used</td><td><tt>talismane_trainer_fr/postags</tt></td></tr>
<tr><td>lexiconDir</td><td>a path to the directory containing a serialised digitised version of any lexicons being used for training.</td>
<td>tokenFilters</td><td>a path to a file containing any token filters to apply to the token reader before training this parser model. These will only be applied if this pos-tag model is applied directly, without any tokeniser.</td><td><tt>talismane_trainer_fr/token_filters.txt</tt></td></tr>
<tr><td>tokenSequenceFilters</td><td>a path to a file containing any token sequence filters to apply to the token reader before training this parser model. These will only be applied if this pos-tag model is applied directly, without any tokeniser.</td><td><tt>talismane_trainer_fr/token_sequence_filters.txt</tt></td></tr>
<tr><td>posTaggerPreprocessingFilters</td><td>a path to a file containing any token sequence filters to apply before training this parser model. These will typically add empty tokens where required. These will only be applied if this pos-tag model is applied directly, without any pos-tagger.</td><td><tt>talismane_trainer_fr/posTagger_preprocessing_filters.txt</tt></td></tr>
</tt></td></tr>
</table>
<p>To evaluate the new parser model, the command would be something like:</p>
<pre class="shell">
java -Xmx4000M -jar talismane-trainer-fr-<i>version</i>-allDeps.jar trainer=parser command=evaluate parserModel=parserModel.zip corpus=resources/CanditoFTBDep/ftb6_2.conll outdir=output posTagSet=resources/CrabbeCanditoTagset.txt posTagMap=resources/ftbCrabbeCanditoTagsetMap.txt lefff=resources/lefff.zip beamWidth=10</pre>
<p>The parameters are as follows:</p>
<table border="1">
<tr><th>Parameter</th><th>Description</th><th>Git Location</th></tr>
<tr><td>parserModel</td><td>a path to the file where the new statistical model was previously stored by the train command</td><td>&nbsp;</td></tr>
<tr><td>outdir</td><td>a path where to write the output files of the evaluation</td><td>&nbsp;</td></tr>
<tr><td>corpus</td><td>an evaluation corpus file from the dependency treebank corpus created by [Candito et al, 2010]</td><td>&nbsp;</td></tr>
<tr><td>posTagSet</td><td>as in training, above</td><td><tt>talismane_trainer_fr/postags</tt></td></tr>
<tr><td>lexiconDir</td><td>a path to the directory containing a serialised digitised version of any lexicons being used for evaluation.</td>
<tr><td>beamWidth</td><td>the beam search beam width for this analysis</td><td>&nbsp;</td></tr>
</table>
<p>The evaluation will produce a file <tt><i>model_name</i>.fscores.csv</tt> containing a confusion matrix and the precision, recall, f-score and error counts for each dependency label. It will also produce a file, <tt><i>model_name</i>.sentences.csv</tt>, containing all of the sentences evaluated, the correct governor and dependency label, and the guessed governors and dependency labels from the <i>n</i> evaluations made by the beam search, along with their probabilities.</p>

<a name="externalResources"></a>
<h2>Custom external resources</h2>
<p>Talismane allows you to incorporate external resources in features.
These external resources are assumed to be files which tie a set of keys to one or more classes, and optionally weights.
For example, you may have a file which maps the set of keys [lemma, postag] to a single class [semanticGroup].
Or you may map the same set of keys [lemma, postag] to multiple [semanticGroup] classes, with different weights per class.</p>
<p>The default external resource file structure is as follows:<br/>
The default name will be the filename.<br/>
If a line starts with the string "Name: ", the default name will be replaced by this name.<br/>
If a line starts with the string "Multivalued: true", the resource will be considered multivalued with weights.<br/>
All lines starting with # are skipped.<br/>
Any other line will be broken up by tabs:<br/>
For multi-valued resources, the second-to-last tab is the class, the last tab is the weight.<br/>
For normal resoruces, the last tab is the class.<br/>
All previous tabs are considered to be key components.<br/>
The same set of key components can have multiple classes with different weights.</p>
<p>Thus, the following file is a legal external resource file:</p>
<pre>
Name: SemanticClass
Multivalued: false
NC	orange	fruit
NC	pomme	fruit
NC	vélo	véhicule
NC	voiture	véhicule
ADJ	bleu	couleur
ADJ	jaune	couleur</pre>
<p>A multivalued file might look like this:</p>
<pre>
Name: SemanticMultiClass
Multivalued: true
NC	orange	fruit	0.6
NC	orange	aliment	0.4
NC	pomme	fruit	0.7
NC	pomme	aliment	0.3
NC	pain	aliment	1.0</pre>
<p>It is recommended to normalise all values so they are between 0.0 and 1.0</p>
<p>External resources are specified via the <tt>externalResources</tt> argument in the trainer mechanism.</p>
<p>They are automatically stored in the statistical model, so that they can be re-used during analysis.</p>
<p>Two features can be used to make use of external resources: <tt>ExternalResource(string name, string keyElement1, string keyElement2, ...)</tt> and <tt>MultivaluedExternalResource(string name, string keyElement1, string keyElement2, ...)</tt>. Thus, we could add the following features to a feature file when training:</p>
<pre>
IsVerb(X)	PosTag(X)=="V" | PosTag(X)=="VS" | PosTag(X)=="VIMP" | PosTag(X)=="VPP" | PosTag(X)=="VINF" | PosTag(X)=="VPR"
LemmaOrWord(X)	IfThenElse(IsNull(Lemma(X)), LexicalForm(X), Lemma(X))
SemanticClass(X)	ExternalResource("SemanticClass",IfThenElse(IsVerb(X),"V",PosTag(X)),LemmaOrWord(X))
LemmaStack0SemClassBuffer0	ConcatNoNulls(LemmaOrWord(Stack[0]),SemanticClass(Buffer[0]))
SemanticClassPair	ConcatNoNulls(SemanticClass(Stack[0]),SemanticClass(Buffer[0]))</pre>
<p>In the above example, we can remplace "SemanticClass" by "SemanticMultiClass", in which case the string being returned will be replaced by a string collection.</p>

<a name="performanceMonitoring"></a>
<h2>Performance monitoring</h2>
<p>Talismane includes a built-in performance monitoring mechanism, via the optional <tt>performanceConfigFile</tt> argument.</p>
<p>If provided, the user can determine which packages or classes should be monitored for performance. A typical file would look like this:</p>
<pre>
talismane.monitoring.activated=true
talismane.monitoring.default=false
talismane.monitor.com.joliciel.talismane.lexicon=true
talismane.monitor.com.joliciel.talismane.parser=true
talismane.monitor.com.joliciel.talismane.parser.features=false
talismane.monitor.com.joliciel.talismane.parser.features.BetweenCountIf=true
</pre>
<p>In this file, we tell Talismane to switch on monitoring, but <b>not</b> to start monitoring by default. Monitoring will only be applied to any classes in the <tt>com.joliciel.talismane.lexicon</tt> and <tt>com.joliciel.talismane.lexicon.parser</tt> packages and sub-packages, with the exception of the <tt>com.joliciel.talismane.parser.features</tt> sub-package, but switched on again for the feature <tt>com.joliciel.talismane.parser.features.BetweenCountIf</tt>.</p>
<p>The result is a CSV file given total performance per class, and performance when we removed any monitored sub-classes.</p>

<h1>References</h1>
<li>Abeillé A. and Clément L., <i>Building a Treebank for French</i>, in TreeBanks, Springer, 2003.</li>
<li>Candito M.-H., Crabbé B., and Denis P., <i>Statistical French dependency parsing: treebank conversion and first results</i>, Proceedings of LREC'2010, La Valletta, Malta, 2010.</li>
<li>Candito M.-H., Nivre J., Denis P. and Henestroza Anguiano E., <i>Benchmarking of Statistical Dependency Parsers for French</i>, in Proceedings of COLING'2010, 2010, Beijing, China</li>
<li>Crabbé B. and Candito M.-H., <i>Expériences d'analyses syntaxique statistique du français</i>, in Proceedings of TALN 2008, 2008, Avignon, France.</li>
<li>Denis P. and Sagot B., <i>Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less human effort</i>, in Proceedings of PACLIC, 2009.</li>
<li>Ho C.-H. and Lin C.-J., <i>Large-scale Linear Support Vector Regression</i>, Technical report, 2012</li>
<li>Nivre J., <i>Algorithms for Deterministic Incremental Dependency Parsing</i>, in Computational Linguistics, 2008, 34(4), 513-553.</li>
<li>Ratnaparkhi, A, <i>Maximum entropy models for natural language ambiguity resolution</i>, University of Pennsylvania, 1998.</li>
<li>Sagae K. and Lavie A., <i>A best-first probabilistic shift-reduce parser</i>, in Proceedings of the COLING/ACL on Main conference poster sessions, pages 691-698, 2006.</li>
<li>Sagot B., Clément L., de La Clergerie E. and Boullier P., <i>The Lefff 2 syntactic lexicon for French: architecture, acquisition, use</i>, 2006</li>
<li>Tangy L. <i>Complexification des données et des techniques en linguistique : contributions du TAL aux solutions et aux problèmes.</i> Mémoire d'Habilitation à Diriger des Recherches, Université de Toulouse, 2012</li>

<p><i>End of document</i></p>

<script>
generateTOC(document.getElementById('toc'));
</script>
</body>
</html>