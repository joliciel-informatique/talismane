languagePack="languagePacks/french-UD/languagePack/"

input-ud {
  input-pattern = "%INDEX%\t%TOKEN%\t%LEMMA%\t%POSTAG%\t.*?\t.*?\t%GOVERNOR%\t%LABEL%\t.*?\t.*?"
  corpus-lexical-entry-regex = ${languagePack}"talismane_conll_morph_regex.txt"
  skip-line-patterns = ${talismane.core.generic.input.skip-line-patterns} ["\\d+\\-\\d+\\t.*"]

  line-rules = [
    {
      criteria = {
        LABEL = "advcl:.*"
      }
      actions = {
        LABEL = "advcl"
      }
    },
    {
      criteria = {
        LABEL = "csubj:.*"
      }
      actions = {
        LABEL = "csubj"
      }
    },
    {
      criteria = {
        LABEL = "expl:.*"
      }
      actions = {
        LABEL = "expl"
      }
    },
    {
      criteria = {
        LABEL = "flat:.*"
      }
      actions = {
        LABEL = "flat"
      }
    },
    {
      criteria = {
        LABEL = "iobj:.*"
      }
      actions = {
        LABEL = "iobj"
      }
    },
    {
      criteria = {
        LABEL = "nsubj:pass"
      }
      actions = {
        LABEL = "nsubj"
      }
    },
    {
      criteria = {
        LABEL = "obj:.*"
      }
      actions = {
        LABEL = "obj"
      }
    },
    {
      criteria = {
        LABEL = "obl:.*"
      }
      actions = {
        LABEL = "obl"
      }
    },
  ]
  
  sentence-rules = [
    {
      pattern = "(\\d+?)\\-(\\d+?)\\t(.+?)\\t.*"
      action = "merge $1 $2"
      conditions = {
        line1 = {
          POSTAG = "ADP" 
        }
        line2 = {
          POSTAG = "DET"
        }
      }
      result = {
        TOKEN = "$3"
        LEMMA = "$3"
        POSTAG = "ADP+DET"
        GOVERNOR = "${line1}"
        LABEL = "${line1}"
      }
    },
    {
      pattern = "(\\d+?)\\-(\\d+?)\\t(.+?)\\t.*"
      action = "merge $1 $2"
      conditions = {
        line1 = {
          POSTAG = "ADP" 
        }
        line2 = {
          POSTAG = "PRON"
        }
      }
      result = {
        TOKEN = "$3"
        LEMMA = "$3"
        POSTAG = "ADP+PRON"
        GOVERNOR = "${line2}"
        LABEL = "${line2}"
      }
    },
  ]
}

output-ud {
  rewrite-rules = [
    {
      conditions {
        POSTAG="ADP\\+DET"
      }
      action {
        type = split
        line1 = {
          POSTAG="ADP"
          GOVERNOR="${orig}"
          LABEL="${orig}"
          NON_PROJ_GOVERNOR="${orig}"
          NON_PROJ_LABEL="${orig}"
                    
          conditional = [
          	{
          	  TOKEN = "(?i)des|du"
          	  results = { TOKEN = "de", LEMMA="de" }
          	},
          	{
          	  TOKEN = "(?i)aux|au"
          	  results = { TOKEN = "à", LEMMA="à" }
          	},
          	{
          	  default = true
          	  results = { TOKEN = "de", LEMMA="de" }
          	}
          ]
        }
        line2 = {
          POSTAG="DET"
          conditional = [
          	{
          	  TOKEN = "(?i)des|aux"
          	  results = { TOKEN = "les", LEMMA="le" }
          	},
          	{
          	  TOKEN = "(?i)du|au"
          	  results = { TOKEN = "le", LEMMA="le" }
          	},
          	{
          	  default = true
          	  results = { TOKEN = "le", LEMMA="le" }
          	},
          	{
          	  relative-index = 1
          	  LABEL = "fixed"
          	  results = { LABEL = "fixed", GOVERNOR="${line1}", NON_PROJ_LABEL="fixed", NON_PROJ_GOVERNOR="${line1}" }
          	},
          	{
          	  default = true
          	  results = { LABEL = "det", GOVERNOR="${orig}", NON_PROJ_LABEL = "det", NON_PROJ_GOVERNOR="${orig}" }
          	}
          ]
        }
      }
    },
    {
      conditions {
        POSTAG="ADP\\+PRON"
      }
      action {
        type = split
        line1 = {
          conditional = [
          	{
          	  TOKEN = "(?i)duquel|desquels|desquelles"
          	  results = { TOKEN = "de", LEMMA="de" }
          	},
          	{
          	  TOKEN = "(?i)auquel|auxquels|auxquelles"
          	  results = { TOKEN = "à", LEMMA="à" }
          	},
          	{
          	  default = true
          	  results = { TOKEN = "de", LEMMA="de" }
          	}
          ]
          POSTAG="ADP"
          GOVERNOR="${line2}"
          LABEL="case"
          NON_PROJ_GOVERNOR="${line2}"
          NON_PROJ_LABEL="case"
        }
        line2 = {
          conditional = [
          	{
          	  TOKEN = "(?i)duquel|auquel"
          	  results = { TOKEN = "lequel", LEMMA="lequel" }
          	},
          	{
          	  TOKEN = "(?i)desquels|auxquels"
          	  results = { TOKEN = "lesquels", LEMMA="lequel" }
          	},
          	{
          	  TOKEN = "(?i)desquelles|auxquelles"
          	  results = { TOKEN = "lesquelles", LEMMA="lequel" }
          	},
          	{
          	  default = true
          	  results = { TOKEN = "lequel", LEMMA="lequel" }
          	}
          ]
          POSTAG="PRON"
          GOVERNOR="${orig}"
          LABEL="${orig}"
          NON_PROJ_GOVERNOR="${orig}"
          NON_PROJ_LABEL="${orig}"
        }
      }
    },
  ]
}
      
talismane {
  core {
    fr = ${talismane.core.generic} {
      locale = fr
      
      lexicons = [
        ${languagePack}"lexicons_fr.zip"
      ]
      
      lowercase-preferences = ${languagePack}"lowercasePreferences.txt"
      
      annotators {  
        text-annotators = [
          ${languagePack}"text_marker_filters.txt"
        ]
        
        sentence-annotators = [
          ${languagePack}"token_filters.txt"
        ]
      }
      
      sentence-detector {
        train = ${input-ud} {
          corpus-reader = com.joliciel.talismane.tokeniser.TokenRegexBasedCorpusReader
          features = "languagePacks/french-UD/features/sentenceDetector_fr_baseline.txt"
        }
      }
      
      tokeniser {
        type = simple

        filters = [
          com.joliciel.talismane.tokeniser.filters.QuoteNormaliser
          com.joliciel.talismane.tokeniser.filters.LowercaseKnownFirstWordFilter
          com.joliciel.talismane.tokeniser.filters.UppercaseSeriesFilter
        ]
        
        input = ${input-ud}
                
        train = ${input-ud} {
          features="languagePacks/french-UD/features/tokeniser_fr_baseline.txt"
          patterns="languagePacks/french-UD/features/tokeniserPatterns_fr.txt"
        }
      }
      
      pos-tagger {
        pos-tag-set = ${languagePack}"tagset.txt"
        
        pos-tag-map = {
          Lefff = ${languagePack}"lefff-ext-3.2_posTagMap.txt"
        }
        rules = [
          ${languagePack}"posTaggerConstraints_fr.txt"
        ]
        
        input = ${input-ud}
        
        train = ${input-ud} {
           features="languagePacks/french-UD/features/posTagger_fr_baseline.txt"
           machine-learning {
             cutoff=3
           }
        }
      }
      
      parser {
        dependency-labels = ${languagePack}"relations.txt"
        
        input = ${input-ud}
        
        output = ${output-ud} {
          processors = [
            com.joliciel.talismane.parser.output.ParseOutputRewriter
          ]
        }

        train = ${input-ud} {
          features="languagePacks/french-UD/features/parser_fr_baseline.txt" 
           machine-learning {
             cutoff=7
           }
         }

      }
    }
  }
}
