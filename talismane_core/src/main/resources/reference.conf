talismane {
	core {
		# The main command to execute:
		# - train: Train a model using a corpus, a feature set, a classifier + parameters, etc.
		# - analyse: Analyse a corpus and add annotations.
		# - evaluate: Evaluate an annotated corpus, by re-analysing the corpus and comparing the new annotations to the existing ones.
		# - process: Process an annotated corpus - Talismane simply reads the corpus using the appropriate corpus reader
		#    and passes the results to the appropriate processors.
		# - compare: Compare two annotated corpora.
		command = analyse
		
		# Which mode to run in:
		# - normal: Command line mode, reading from standard in or file, and writing to standard out or file.
		# - server: Server listening on port, and processing input as it comes.
		mode = normal
		
		# Modules are used by all commands, but can be overwritten by startModule and endModule for analyse
		# - language|languageDetector
		# - sentence|sentenceDetector
		# - tokenise|tokeniser
		# - postag|posTagger
		# - parse|parser
		module = parser
		
		# if inFile and inDir are not specified, Talismane will read from STDIN
		# use inFile if a specific file is being read
		inFile = ""
		# use inDir if a full directory needs to be processed
		inDir = ""
		
		# if outFile and outDir are not specified, Talismane will write to STDOUT
		# use outFile to write all output to a single file
		outFile = ""
		# use outDir to write output to a directory - in combination with inDir, will write one file out per file read in
		outDir = ""
				
		# if encoding is not provided, the default encoding for the system will be used
		encoding = "UTF-8"
		
		# inputEncoding and outputEncoding can override encoding
		inputEncoding = ""
		outputEncoding = ""
		
		# if set to true, the total time of talismane execution will be written to a separate file
		logStats = false
		
		analyse {
			# at which stage should analysis start
			startModule = sentence
			
			# at which stage should analysis end
			endModule = parser
			
			### Path to the pos-tag set (required if not in the language-pack)
			posTagSet = ""
			
			### Path to the transition system (required if not in the language-pack)
			transitionSystem = ""

			### various parameters defining beam search behaviour
			# how wide should the beam be in the beam search - wider beams give better analysis but slow the analysis linearly
			beamWidth = 1
			
			# we can override the beamWidth for individual models, by setting these to positive values
			tokeniserBeamWidth = 1
			posTaggerBeamWidth = -1
			parserBeamWidth = -1

			# whether or not to propagate the beam from the pos-tagger to the parser, enabling the parser to correct pos-tagging errors
			propagateBeam = true
			
			# whether or not to propagate the beam from the tokeniser to the pos-tagger and parser, enabling them to correct tokenisation errors
			propagateTokeniserBeam = false
			
			### Paths for finding the models
			languageModel = ""
			sentenceModel = ""
			tokeniserModel = ""
			posTaggerModel = ""
			parserModel = ""
			
			############################################
			### Parameters controlling how data is input
			# If one of these is provided, it will override the default handling of input
			# A regex for reading input, with various placeholders
			# The default for tokenised text (pos-tagger analysis or tokeniser evaluation) is: ".+\t%TOKEN%"
			# The default for pos-tagged text (parser analysis or pos-tagger evaluation) is: ".*\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t"
			# The default for parsed text (parser evaluation) is: "%INDEX%\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t%NON_PROJ_GOVERNOR%\t%NON_PROJ_LABEL%\t%GOVERNOR%\t%LABEL%"
			inputPattern = ""
			# File path to a regex
			inputPatternFile = ""
			
			### Paths to files containing filters
			# if these paths are preceded by the string "replace:", the default filters of the current language-pack are replaced, otherwise these tilters are added after the language-pack filters
			# All of these paths can refer to multiple filter files, separated by semicolons
			textFilters = ""
			tokenFilters = ""
			tokenSequenceFilters = ""
			posTagSequenceFilters = ""
			
			# How to handle the newline character, options are SPACE (will be replaced by a space) and SENTENCE_BREAK (will break sentences)
			newline = SENTENCE_BREAK
			
			# If true, the input file is processed from the very start (e.g. TXT files). If false, we wait until a text filter tells us to start processing (e.g. XML files).
			processByDefault = true
			
			# A character (typically non-printing) which will mark a stop in the input stream and set-off analysis immediately. Must be a single character.
			endBlockCharCode = "\f"
			
			# The block size to use when applying filters - if a text filter regex goes beyond the blocksize, Talismane will fail.
			# This is required because Talismane input is based on streaming, and in order to ensure filters are applied, we have to know when to stop reading and apply filters.
			# A larger block size will avoid errors due to regex matches that are too large, but will mean longer waits before Talismane returns results when streaming.
			blockSize = 1000

			############################################
			### Parameters controlling how data is output
			# a built-in output template. Options include: default, with_location, with_prob, with_comments
			builtInTemplate = "default"
			
			# if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
			template = ""

			# if true, will spew out a file full of low-level analysis details for certain model types
			includeDetails = false
			
			### Paths to files containing rules
			# if these paths are preceded by the string "replace:", the default rules of the current language-pack are replaced, otherwise these rules are added after the language-pack rules
			posTaggerRules = ""
			parserRules = ""
			
			# If output includes the filename (as in with_location_), we can give the name of the file to use.
			# This is only useful in server mode, when the filename is unknown. If an input file is processed, it's name is used by default.
			fileName = ""
			
			# If provided, will add this suffix to all output files (both in analysis and evaluation). Useful when comparing configurations.
			suffix = ""
			
			#############################################
			### Parameters controlling the analysis process itself
			# How long we will attempt to parse a sentence before leaving the parse as is, in seconds. A value of 0 means the parsing will continue indefinitely.
			# If analysis jumps out because of time-out, there will be a parse-forest instead of a parse-tree,
			# with several nodes left unattached
			maxParseAnalysisTime = 60
			
			# The minimum amount of remaining free memory to continue a parse, in kilobytes.
			# Will be ignored is set to 0.
			# If analysis jumps out because of free memory descends below this limit,
			# there will be a parse-forest instead of a parse-tree,
			# with several nodes left unattached
			minFreeMemory = 64
			
			# The max sentences to process. If <= 0 will be ignored.
			sentenceCount = 0
			
			# The first sentence index to process. Earlier sentences will be skipped.
			startSentence = 0
			
			
		}
		
		evaluate {
			# The file to evaluate against the inputFile
			evaluationFile = ""
			
			# If one of these is provided, it will override the default handling of evaluation input
			# Note that this is distinct from analyse.inputPattern, which indicates how the "gold" file will be read
			# A regex for reading input, with various placeholders
			evaluationPattern = ""
			# File path to a regex
			evaluationPatternFile = ""
			
			# Whether or not a separate file should be generated including the guessed output in a fairly compact format
			outputGuesses = false
			# If we do generate the output guesses, how many guesses (from the beam) to include per analysis
			outputGuessCount = 1
			
			parser {
				# If true, takes both governor and dependency label into account when determining errors. If false, only the governor is taken into account.
				labeledEvaluation = true
				
				# If true, calculates and writes the f-score for each separate governor to dependent distance during a parse evaluation
				includeDistanceFScores = false
				
				# If true, writes the list of transitions that were actually applied, one at a time.
				includeTransitionLog = false
				
				# if true, will output a file giving average time for sentences of different lengths, making it possible to see if analysis is performing in linear time
				includeTimePerToken = false
			}
			
			crossValidation {
				# If greater than 0, the input file will be split into foldCount folds
				# Typically, 10-fold validation is used, if cross-validation is desired
				foldCount = 0
				
				# Used when evaluating in cross-validation
				# If provided, only a single fold will be processed, numbered from 0 to foldCount -1
				includeIndex = 0
				
				# Used when training in cross-validation
				# If provided, one of the folds will be excluded, numbered from 0 to foldCount - 1
				excludeIndex = 0
			}
		}
		
		process {
			# processing options:
			# - output: Simply output what you read, usually changing the format
			# - loadParsingConstraints: Load the parsing constraints from a training corpus.
			# - posTagFeatureTester: Test pos-tag features on a subset of words in the training set.
			# - parseFeatureTester: Test parse features on the training set.
			option = output
		}
	}
}