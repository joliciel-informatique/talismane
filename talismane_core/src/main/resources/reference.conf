talismane {
	core {
		# The main command to execute:
		# - train: Train a model using a corpus, a feature set, a classifier + parameters, etc.
		# - analyse: Analyse a corpus and add annotations.
		# - evaluate: Evaluate an annotated corpus, by re-analysing the corpus and comparing the new annotations to the existing ones.
		# - process: Process an annotated corpus - Talismane simply reads the corpus using the appropriate corpus reader
		#    and passes the results to the appropriate processors.
		# - compare: Compare two annotated corpora.
		command = analyse
		
		# A language pack makes it possible to provide default models and lexicons for analysing in a certain language
		languagePack = ""
		
		# Which mode to run in:
		# - normal: Command line mode, reading from standard in or file, and writing to standard out or file.
		# - server: Server listening on port, and processing input as it comes.
		mode = normal
		
		# In server mode, which port to listen on
		port = 7272
		
		# Modules are used by all commands
		# If provided, it will override startModule and endModule for analyse/evaluate/compare
		# It is required for train|process
		# - language|languageDetector
		# - sentence|sentenceDetector
		# - tokenise|tokeniser
		# - postag|posTagger
		# - parse|parser
		module = ""
		
		# if inFile and inDir are not specified, Talismane will read from STDIN
		# use inFile if a specific file is being read
		inFile = ""
		# use inDir if a full directory needs to be processed
		inDir = ""
		
		# if outFile and outDir are not specified, Talismane will write to STDOUT
		# use outFile to write all output to a single file
		outFile = ""
		# use outDir to write output to a directory - in combination with inDir, will write one file out per file read in
		outDir = ""
				
		# if encoding is not provided, the default encoding for the system will be used
		encoding = "UTF-8"
		
		# inputEncoding and outputEncoding can override encoding
		inputEncoding = ""
		outputEncoding = ""
		
		# The locale in which we are analysing - required if not provided in the language pack
		locale = ""
		
		analyse {
			# at which stage should analysis start
			startModule = sentence
			
			# at which stage should analysis end
			endModule = parser
			
			### Path to the pos-tag set (required if not in the language-pack)
			posTagSet = ""
			
			### The transition system used: ArcEager or ShiftReduce
			transitionSystem = "ArcEager"
			
			### Path to the list of possible dependency labels (required if not in the language-pack)
			dependencyLabels = ""
			
			### Paths for finding the models
			languageModel = ""
			sentenceModel = ""
			tokeniserModel = ""
			posTaggerModel = ""
			parserModel = ""
			
			### Which lexicon to use when analysing. The lexicon is used to find lemmas and morphosyntaxic attributes for known words, and to feed various analysis features
			# if the path is preceded by the string "replace:", the default lexicon of the current language-pack are replaced, otherwise the lexicon is added after the language-pack lexicons
			lexicon = ""
			
			# Whether to preload the lexicon, or only load it the first time its needed
			# the latter option is only really useful to save time when we're sure the lexicon will not be needed (e.g. command=process)
			preloadLexicon = true
			
			### A path to a precompiled "diacriticizer", which adds diacritics to words in ALL UPPERCASE prior to analysis.
			# if left blank, a diacriticizer is automatically constructed from the lexicon, but this takes more time at startup.
			diacriticizer = ""
			
			############################################
			### Parameters controlling how data is input
			# If one of these is provided, it will override the default handling of input
			# A regex for reading input, with various placeholders
			# The default for tokenised text (pos-tagger analysis or tokeniser evaluation) is: ".+\t%TOKEN%"
			# The default for pos-tagged text (parser analysis or pos-tagger evaluation) is: ".*\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t"
			# The default for parsed text (parser evaluation) is: "%INDEX%\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t%NON_PROJ_GOVERNOR%\t%NON_PROJ_LABEL%\t%GOVERNOR%\t%LABEL%"
			inputPattern = ""
			# File path to a regex
			inputPatternFile = ""
			
			### Paths to files containing filters
			# if these paths are preceded by the string "replace:", the default filters of the current language-pack are replaced, otherwise these tilters are added after the language-pack filters
			# All of these paths can refer to multiple filter files, separated by semicolons
			textFilters = ""
			tokenFilters = ""
			tokenSequenceFilters = ""
			posTagSequenceFilters = ""
			
			# How to handle the newline character, options are SPACE (will be replaced by a space) and SENTENCE_BREAK (will break sentences)
			newline = SENTENCE_BREAK
			
			# If true, the input file is processed from the very start (e.g. TXT files). If false, we wait until a text filter tells us to start processing (e.g. XML files).
			processByDefault = true
			
			# A character (typically non-printing) which will mark a stop in the input stream and set-off analysis immediately. Must be a single character.
			endBlockCharCode = "\f"
			
			# The block size to use when applying filters - if a text filter regex goes beyond the blocksize, Talismane will fail.
			# This is required because Talismane input is based on streaming, and in order to ensure filters are applied, we have to know when to stop reading and apply filters.
			# A larger block size will avoid errors due to regex matches that are too large, but will mean longer waits before Talismane returns results when streaming.
			blockSize = 1000
			
			# The max sentences to process. If <= 0 will be ignored.
			sentenceCount = 0
			
			# The first sentence index to process. Earlier sentences will be skipped.
			startSentence = 0

			############################################
			### Parameters controlling how data is output
			# a built-in output template. Options include: default, with_location, with_prob, with_comments
			builtInTemplate = "default"
			
			# if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
			template = ""

			# if true, will spew out a file full of low-level analysis details for certain model types
			includeDetails = false
			
			### Paths to files containing rules
			# if these paths are preceded by the string "replace:", the default rules of the current language-pack are replaced, otherwise these rules are added after the language-pack rules
			posTaggerRules = ""
			parserRules = ""
			
			# If output includes the filename (as in builtInTemplate "with_location"), we can give the name of the file to use.
			# This is only useful in server mode, when the filename is unknown. If an input file is processed, its name is used by default.
			fileName = ""
			
			# If provided, will add this suffix to all output files (both in analysis and evaluation). Useful when comparing configurations.
			suffix = ""
			
			# A string to insert between sections marked for output (e.g. XML tags to be kept in the analysed output).
			# The String NEWLINE is interpreted as "\n". Otherwise, used literally.
			outputDivider = ""
			
			#############################################
			### Parameters controlling the analysis process itself
			
			### various parameters defining beam search behaviour
			# how wide should the beam be in the beam search - wider beams give better analysis but slow the analysis linearly
			beamWidth = 1
			
			# we can override the beamWidth for individual models, by setting these to positive values
			tokeniserBeamWidth = 1
			posTaggerBeamWidth = -1
			parserBeamWidth = -1

			# whether or not to propagate the beam from the pos-tagger to the parser, enabling the parser to correct pos-tagging errors
			propagateBeam = true
			
			# whether or not to propagate the beam from the tokeniser to the pos-tagger and parser, enabling them to correct tokenisation errors
			propagateTokeniserBeam = false
			
			# If true, feature descriptors will be converted into a single dynamically generated class
			dynamiseFeatures = false
			
			tokeniser {
				### The tokeniser type:
				# - simple: a deterministic tokeniser purely based on filters
				# - pattern: a probabilistic tokeniser using supervised machine learning to take tokenisation decisions in areas where certain patterns are matched
				type = "simple"
				
				### If using a pattern tokeniser, the method: Interval or Compound
				patternTokeniserType = "Compound"
			}
			
			parser {
				# How long we will attempt to parse a sentence before leaving the parse as is, in seconds. A value of 0 means the parsing will continue indefinitely.
				# If analysis jumps out because of time-out, there will be a parse-forest instead of a parse-tree,
				# with several nodes left unattached
				maxAnalysisTime = 60
				
				# The minimum amount of remaining free memory to continue a parse, in kilobytes.
				# Will be ignored is set to 0.
				# If analysis jumps out because of free memory descends below this limit,
				# there will be a parse-forest instead of a parse-tree,
				# with several nodes left unattached
				minFreeMemory = 64

				### During the beam search, how to determine which parse configurations to compare to each other in the same beam
				# * transitionCount: Comparison based on number of transitions applied.
				# * bufferSize: Comparison based on number of elements remaining on the buffer
				# * stackAndBufferSize: Comparison based on number of elements remaining on both the stack and the buffer.
				# * dependencyCount:  Comparison based on number of dependencies created.
				comparisonStrategy = bufferSize
				
				# If true, we stop as soon as the beam contains n terminal configurations,
				# where n is the beam width, rather than waiting for all paths to end.
				earlyStop = false

			}
		}
		
		evaluate {
			# The file to evaluate against the inputFile
			evaluationFile = ""
			
			# If one of these is provided, it will override the default handling of evaluation input
			# Note that this is distinct from analyse.inputPattern, which indicates how the "gold" file will be read
			# A regex for reading input, with various placeholders
			evaluationPattern = ""
			# File path to a regex
			evaluationPatternFile = ""
			
			csv {
				# Which character should separate cells in the CSV files generated
				separator = "\t"
			
				# Which encoding to generate CSV files in. If blank will use the default encoding for the current OS.
				encoding = ""
				
				# Which locale to use for CSV number formatting. If blank will use the default locale for the current OS.
				locale = ""
			}
			
			posTagger {
				# if true, will add files ending with "_unknown.csv" and "_known.csv" splitting pos-tagging f-scores into known and unknown words
				includeUnknownWordResults = false
				
				# if true, will add a file ending with ".lexiconCoverage.csv" giving lexicon word coverage
				includeLexiconCoverage = false
			}
			
			parser {
				# If true, takes both governor and dependency label into account when determining errors. If false, only the governor is taken into account.
				labeledEvaluation = true
				
				# if true, will output a file giving average time for sentences of different lengths, making it possible to see if analysis is performing in linear time
				includeTimePerToken = false
				
				### Distance f-score output
				# If true, calculates and writes the f-score for each separate governor to dependent distance during a parse evaluation
				includeDistanceFScores = false
				
				# Label to be skipped when calculating distance f-scores - for arbitrary distance attachments in the case of punctuation
				skipLabel = ""
				
				### Output the transitions applied
				# If true, writes the list of transitions that were actually applied, one at a time.
				includeTransitionLog = false
				
				# If included, will output in the transition log, at the end of each parse, the correct sentence and erroneous sentence
				# if the error concerned any one of these comma-separated dependency labels
				errorLabels = ""
			}
			
			crossValidation {
				# If greater than 0, the input file will be split into foldCount folds
				# Typically, 10-fold validation is used, if cross-validation is desired
				foldCount = 0
				
				# Used when evaluating in cross-validation
				# If provided, only a single fold will be processed, numbered from 0 to foldCount -1
				includeIndex = 0
				
				# Used when training in cross-validation
				# If provided, one of the folds will be excluded, numbered from 0 to foldCount - 1
				excludeIndex = 0
			}
			
						
			# Whether or not a separate file should be generated including the guessed output in a fairly compact format
			outputGuesses = false
			# If we do generate the output guesses, how many guesses (from the beam) to include per analysis
			outputGuessCount = 1
		}
		
		process {
			# processing options:
			# - output: Simply output what you read, usually changing the format
			# - loadParsingConstraints: Load the parsing constraints from a training corpus.
			# - posTagFeatureTester: Test pos-tag features on a subset of words in the training set.
			# - parseFeatureTester: Test parse features on the training set.
			option = output
			
			# Should an attempt be made to the predict the transitions that led to this configuration,
			# or should dependencies simply be added with null transitions. Outside of training, it is only
			# useful to predict transitions if we need the transition sequence, e.g. to check that the
			# given dependency tree is reachable via the selected transition system.
			predictTransitions = false
			
			# A path to a file containing a regex describing how to read a lexical entry from each line in the corpus
			# making it possible to maintain and/or transform morphosyntaxic attributes when processing a corpus
			corpusLexicalEntryRegex = ""
			
			posTagFeatureTester {
				# a set of ;-separated words to test pos-tag features on, other words will be skipped
				testWords = ""
			}
		}
		
		train {
			# The paths to various external resources, separated by semicolons
			externalResources = ""

			languageDetector {
				# A path to a file containing language detector feature descriptors
				features = ""
				
				# A path to a file listing, for each language, the path to its corpus.
				# Format is locale code, followed by a tab, followed by the path to a file containing the corpus.
				languageCorpusMap = ""
			}
			
			sentenceDetector {
				# A path to a file containing sentence detector feature descriptors
				features = ""
			}
			
			tokeniser {
				# if a sentence reader is provided, the text of sentences represented by the tokenised input is provided by this reader, one sentence per line.
				# otherwise, their text for use in tokenisation features is automatically reconstructed from the manually tokenised input, and the spacing may be erroneous
				sentenceReader = ""
				
				# A path to a file containing tokeniser feature descriptors
				features = ""
				
				# A path to a file containing patterns indicating which areas need further testing by the pattern tokeniser
				patterns = ""
			}
			
			posTagger {
				# A path to a file containing pos-tagger feature descriptors
				features = ""
			}
			
			parser {
				# A path to a file containing parser feature descriptors
				features = ""
			}
			
			# iterations at which the perceptron model should be saved
			# TODO: this really belongs in the machine-learning conf, and will be moved there eventually
			perceptronObservationPoints = []
		}
		
		# if set to true, the total time of talismane execution will be written to a separate file
		logStats = false
	}
}