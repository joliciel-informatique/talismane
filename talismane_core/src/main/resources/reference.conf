talismane {
  core {
    # This is a generic core configuration containing all the default values.
    # When adding a new configuration, consider inheriting from this
    generic {
      # The main command to execute:
      # - train: Train a model using a corpus, a feature set, a classifier + parameters, etc.
      # - analyse: Analyse a corpus and add annotations.
      # - evaluate: Evaluate an annotated corpus, by re-analysing the corpus and comparing the new annotations to the existing ones.
      # - process: Process an annotated corpus - Talismane simply reads the corpus using the appropriate corpus reader
      #    and passes the results to the appropriate processors.
      # - compare: Compare two annotated corpora.
      command = analyse
      
      # Which mode to run in:
      # - normal: Command line mode, reading from standard in or file, and writing to standard out or file.
      # - server: Server listening on port, and processing input as it comes.
      mode = normal
      
      # In server mode, which port to listen on
      port = 7272
      
      # Modules are used by all commands
      # If provided, it will override startModule and endModule for analyse/evaluate/compare
      # It is required for train|process
      # - languageDetector
      # - sentenceDetector
      # - tokeniser
      # - posTagger
      # - parser
      module = null
      
      # gives the base name for output files, if not overridden in code
      base-name = "talismane"
      
      # If provided, will add this suffix to all output files (both in analysis and evaluation). Useful when comparing configurations.
      # Also will replace the extension for all files when analysing a directory and retaining the directory structure
      suffix = null
          
      # if encoding is not provided, the default encoding for the system will be used
      encoding = null
      
      # inputEncoding and outputEncoding can override encoding
      input-encoding = null
      output-encoding = null
      
      # The locale in which we are analysing - must be overridden
      locale = null
      
      ### The list of lexicons to use
      # The lexicons are used to find lemmas and morphosyntaxic attributes for known words, and to feed various analysis features
      # The list can either be zip files with the ".zip" extension (assumed to be lexicons pre-serialized with the serializeLexicon command)
      # or text files (assumed to be on the local file system, and to represent lexicon properties files).
      lexicons = []
      
      # The paths to various external word lists (can be either folders or files)
      # These are used within regex-based filters and annotators, as well as certain features
      word-lists = []
      
      ### A path to a precompiled "diacriticizer", which adds diacritics to words in ALL UPPERCASE prior to analysis.
      # if left blank, a diacriticizer is automatically constructed from the lexicon, but this takes more time at startup.
      #diacriticizer = null
      
      # Path to a file giving lowercase preferences for all uppercase words with multiple diacriticized possibilities
      #lowercase-preferences = null
      
      # The paths to various external resources (can be either folders or files), used for building features
      external-resources = []
      
      # The default beam width for all modules
      beam-width = 1
      
      # A character (typically non-printing) which will mark a stop in the input stream and set-off analysis immediately. Must be a single character.
      end-block-char-code = "\f"
      
      # The block size to use when applying filters - if a text filter regex goes beyond the blocksize, Talismane will fail.
      # This is required because Talismane input is based on streaming, and in order to ensure filters are applied, we have to know when to stop reading and apply filters.
      # A larger block size will avoid errors due to regex matches that are too large, but will mean longer waits before Talismane returns results when streaming.
      # Set the block size = 0 if analysing individual files via the API, rather than streams via the command-line or API.
      # A block-size of 0 means no errors will get generated.
      block-size = 1000
      
      # How to handle the newline character, options are SPACE (will be replaced by a space) and SENTENCE_BREAK (will break sentences)
      newline = SENTENCE_BREAK
      
      # The different types of annotators need to be applied in specific points of execution,
      # depending on the command - they are applied at different stages depending on which modules
      # are called and when.
      # Specifically, when training, evaluating or simply analysing, they apply to different corpus-readers.
      # All annotators are simply lists of file references - the files contain "descriptors" in a format
      # specific to each annotator type.
      # It is important when training a model to distribute the model with the same annotators are were
      # applied to train it, as referenced by an appropriate configuration file.
      annotators {
        # Used to filter the raw text and transform it into text that can be processed
        # Among other things, text filters:
        # - tell us which sections to process and which sections to skip (e.g. when processing XML)
        # - fix encoding/escaping (e.g. &quot; transformed into a quote)
        # - impose sentence breaks (e.g. on double newlines, or on certain XML tags)
        # - indicate that a sentence-boundary cannot occur within a certain word (e.g. Mr. Smith)
        # - remove duplicate white space
        text-annotators = []
        
        # Used to prepare a sentence for tokenisation and pos-tagging. In particular:
        # - Mark deterministic token boundaries (e.g. around "aujourd'hui" in French)
        # - Indicate the analysis text when different from raw text (e.g. replace the number 12 by "#NUMBER#")
        # - Impose pos-tags on a particular word
        # - Add arbitrary tags to the text, which are inherited by the tokens and handled by downstream systems
        sentence-annotators = []
      }
      
      analysis {
        # at which stage should analysis start
        start-module = sentenceDetector
        
        # at which stage should analysis end
        end-module = parser
              
        # If true, the input file is processed from the very start (e.g. TXT files). If false, we wait until a text filter tells us to start processing (e.g. XML files).
        process-by-default = true
        
        # Should analysis stop if an analysis error occurs, or should it simply skip the sentence and continue
        stop-on-error = true
        
        # If false, will only take into account the output for the end module
        # If true, will take into account output for all modules
        output-intermediate-modules = false
      }
      
      input {
        # The max sentences to process. If <= 0 will be ignored.
        sentence-count = 0
      
        # The index of the first sentence to process. Earlier sentences will be skipped.
        start-sentence = 0

        # regexes indicating lines to be skipped
        skip-line-patterns = [
          "#.*"
        ]
          
        cross-validation {
          # If greater than 0, the input file will be split into foldCount folds
          # Typically, 10-fold validation is used, if cross-validation is desired
          fold-count = -1
          
          # Used when evaluating in cross-validation
          # If provided, only a single fold will be processed, numbered from 0 to foldCount -1
          include-index = -1
          
          # Used when training in cross-validation
          # If provided, one of the folds will be excluded, numbered from 0 to foldCount - 1
          exclude-index = -1
        }      
      }
        
      output {
        ### Parameters controlling how data is output
        # a built-in output template. Options include: standard, with_location, with_prob, with_comments
        built-in-template = "standard"
        
        # A string to insert between sections marked for output (e.g. XML tags to be kept in the analysed output).
        # The String NEWLINE is interpreted as "\n". Otherwise, used literally.
        output-divider = ""
        
        # if set to true, the total time of talismane execution will be written to a separate file
        log-execution-time = false
        
        # if true, will spew out a file full of low-level analysis details for certain model types
        include-details = false
      }
      
      language-detector {
        # Path to the language detection model
        model = null
        
        input = ${talismane.core.generic.input} {
          # the class used to read the corpus
          corpus-reader = com.joliciel.talismane.languageDetector.TextPerLineCorpusReader
        }
        
        train = ${talismane.core.generic.language-detector.input} {        
          # A path to a file containing sentence-detector feature descriptors
          features = null
          
          machine-learning = ${talismane.machine-learning}
          
          # A path to a file listing, for each language, the path to its corpus.
          # Format is locale code, followed by a tab, followed by the path to a file containing the corpus.
          language-corpus-map = null
        }
      }
      
      sentence-detector {
        # Path to the sentence detection model
        model = null
        
        output = ${talismane.core.generic.output} {
          # a list of processors to apply
          # - FreemarkerSentenceWriter:  Output using a freemarker template.
          # Others may be defined, as long as:
          # - they're on the classpath,
          # - they implement com.joliciel.talismane.sentenceDetector.SentenceProcessor,
          # - they implement a constructor with the signatures (Writer, TalismaneSession) for the main output,
          #    (File outDir, TalismaneSession) for additional output, or (TalismaneSession) for no output.
          # The first processor implementing (Writer, TalismaneSession) will write to the main output file.
          processors = [
            com.joliciel.talismane.sentenceDetector.FreemarkerSentenceWriter
          ]
          
          # if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
          template = null
        }
        
        input = ${talismane.core.generic.input} {
          # the class used to read the training corpus
          corpus-reader = com.joliciel.talismane.sentenceDetector.SentencePerLineCorpusReader
        }
        
        train = ${talismane.core.generic.sentence-detector.input} {        
          # A path to a file containing sentence-detector feature descriptors
          features = null
          
          machine-learning = ${talismane.machine-learning}
        }
        
        evaluate = ${talismane.core.generic.sentence-detector.input} {
        }
      }
      
      tokeniser {
        # A regex giving default separators for tokens.
        # Note that Unicode character class will be applied
        separators = "[\\s\\p{Punct}«»\\_‒–—―`‛“”„‟′″‴‹›‘’‚\\*\ufeff]"
        
        ### The tokeniser itself. Must extend com.joliciel.talismane.Tokeniser and have a constructor taking a TalismaneSession.
        # - com.joliciel.talismane.tokeniser.SimpleTokeniser:
        #      a deterministic tokeniser purely based on filters
        # - com.joliciel.talismane.tokeniser.patterns.PatternTokeniser:
        #      a probabilistic tokeniser using supervised machine learning to take tokenisation decisions in areas where certain patterns are matched
        tokeniser = com.joliciel.talismane.tokeniser.SimpleTokeniser
        
              
        # Applied after tokenisation to normalise token text
        filters = [
        	#com.joliciel.talismane.tokeniser.filters.DiacriticRemover
        	#com.joliciel.talismane.tokeniser.filters.LowercaseFilter
        	#com.joliciel.talismane.tokeniser.filters.LowercaseKnownFirstWordFilter
        	#com.joliciel.talismane.tokeniser.filters.LowercaseKnownWordFilter
        	#com.joliciel.talismane.tokeniser.filters.QuoteNormaliser
        	#com.joliciel.talismane.tokeniser.filters.UppercaseSeriesFilter
        ]
        
        # If using a pattern tokeniser, the path to the tokeniser model
        model = null
              
        # The beam width used by the tokeniser
        beam-width = 1
              
        output = ${talismane.core.generic.output} {
          # a list of processors to apply
          # - FreemarkerTokenWriter:  Output using a freemarker template.
          # Others may be defined, as long as:
          # - they're on the classpath,
          # - they implement com.joliciel.talismane.tokeniser.TokenSequenceProcessor,
          # - they implement a constructor with the signatures (Writer, TalismaneSession) for the main output,
          #    (File outDir, TalismaneSession) for additional output, or (TalismaneSession) for no output.
          # The first processor implementing (Writer, TalismaneSession) will write to the main output file.
          processors = [
            com.joliciel.talismane.tokeniser.output.FreemarkerTokenWriter
          ]
          
          # if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
          template = null
        }
        
        input = ${talismane.core.generic.input} {
          # the class used to read the training corpus
          corpus-reader = com.joliciel.talismane.tokeniser.TokenRegexBasedCorpusReader

          # the regex used to read tokens from the evaluation corpus
          input-pattern = "%INDEX%\t%TOKEN%"
            
          # if a sentence file is provided, the text of sentences represented by the tokenised input is provided by this file, one sentence per line.
          # otherwise, their text for use in tokenisation features is automatically reconstructed from the manually tokenised input, and the spacing may be erroneous
          sentence-file = null
          
          # See core.talismane.parser.input.corpus-rules
          corpus-rules = []
        }
        
        train = ${talismane.core.generic.tokeniser.input} {        
          # A path to a file containing tokeniser feature descriptors
          features = null
          
          # A path to a file containing patterns indicating which areas need further testing by the pattern tokeniser
          patterns = null
          
          machine-learning = ${talismane.machine-learning}
        }
        
        evaluate = ${talismane.core.generic.tokeniser.input} {
          # A list of observers to apply when evaluating.
          # Others may be defined, as long as:
          # - they're on the classpath,
          # - they implement com.joliciel.talismane.tokeniser.evaluate.TokenEvaluationObserver,
          # - they implement a constructor with the signatures (File outDir, TalismaneSession) for output,
          #   or (TalismaneSession) for no output.
          observers = [
            # to write f-scores of the various patterns
            com.joliciel.talismane.tokeniser.evaluate.TokenFScoreCalculator
            
            # writes out the corpus with errors highlighted
            # com.joliciel.talismane.tokeniser.evaluate.TokenEvaluationCorpusWriter
          ]
        }
      }
      
      pos-tagger {
        # The pos-tagger itself. Must extend com.joliciel.talismane.posTagger.PosTagger and have a constructor taking a TalismaneSession
        pos-tagger = com.joliciel.talismane.posTagger.ForwardStatisticalPosTagger
        
        # The pos-tag set to use
        pos-tag-set = null
        
        pos-tag-map {
          # for each lexicon name in the lexicon list, optionally list a pos-tag mapper path mapping the lexicon entries to pos-tags
          # if no entry is found, will use the default pos-tag mapper which maps Categories directly to pos-tags
        }
          
        # A path to the pos-tagger model
        model = null
        
        # Paths to files containing pos-tagger rules to apply
        rules = []
        
        # The beam width used by the pos-tagger
        beam-width = ${talismane.core.generic.beam-width}
        
        # should the tokeniser beam be propagated to the pos-tagger
        propagate-tokeniser-beam = false
        
        output = ${talismane.core.generic.output} {
          # a list of processors to apply
          # - FreemarkerPosTagWriter:  Output using a freemarker template.
          # - PosTagFeatureTester: Test pos-tag features on a subset of words in the training set.
          # Others may be defined, as long as:
          # - they're on the classpath,
          # - they implement com.joliciel.talismane.posTagger.output.PosTagSequenceProcessor,
          # - they implement a constructor with the signatures (Writer, TalismaneSession) for the main output,
          #    (File outDir, TalismaneSession) for additional output, or (TalismaneSession) for no output.
          # The first processor implementing (Writer, TalismaneSession) will write to the main output file.
          processors = [
            com.joliciel.talismane.posTagger.output.FreemarkerPosTagWriter
            # com.joliciel.talismane.posTagger.output.PosTagFeatureTester
          ]
        
          # if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
          template = null
          
          # a list of words to test pos-tag features on, other words will be skipped
          test-words = []
        }
        
        input = ${talismane.core.generic.input} {
          # the class used to read the training corpus
          corpus-reader = com.joliciel.talismane.posTagger.PosTagRegexBasedCorpusReader

          # the regex used to read tokens from the evaluation corpus
          input-pattern = "%INDEX%\t%TOKEN%\t%LEMMA%\t%POSTAG%\t%CATEGORY%\t%MORPHOLOGY%"
                    
          # A path to a file containing a regex describing how to read a lexical entry from each line in the corpus
          # making it possible to maintain and/or transform morphosyntaxic attributes when processing a corpus
          corpus-lexical-entry-regex = null
          
          # if a sentence file is provided, the text of sentences represented by the tokenised input is provided by this file, one sentence per line.
          # allows Talismane to know the correct original spacing.
          sentence-file = null
          
          # See core.talismane.parser.input.corpus-rules
          corpus-rules = []
        }
        
        train = ${talismane.core.generic.pos-tagger.input} {
          # A path to a file containing pos-tagger feature descriptors
          features = null
          
          machine-learning = ${talismane.machine-learning}
        }
        
        evaluate = ${talismane.core.generic.pos-tagger.input} {
          # if true, will add files ending with "_unknown.csv" and "_known.csv" splitting pos-tagging f-scores into known and unknown words
          include-unknown-word-results = false
          
          # If we do generate the output guesses, how many guesses (from the beam) to include per analysis
          output-guess-count = 1
          
          # Start-module: tokeniser or posTagger
          # This allows us to evaluate a sequence of several modules in a row
          start-module = posTagger
          
          # A list of observers to apply when evaluating.
          # Others may be defined, as long as:
          # - they're on the classpath,
          # - they implement com.joliciel.talismane.posTagger.evaluate.PosTagEvaluationObserver,
          # - they implement a constructor with the signatures (File outDir, TalismaneSession) for output,
          #   or (TalismaneSession) for no output.
          observers = [
            # to write f-scores of the various dependency labels
            # respects include-unknown-word-results
            com.joliciel.talismane.posTagger.evaluate.PosTagFScoreCalculator
            
            # Generate the guessed output in a fairly compact format - respects the output-guess-count
            # com.joliciel.talismane.posTagger.evaluate.PosTagEvaluationSentenceWriter
            
            # add a file ending with ".lexiconCoverage.csv" giving lexicon word coverage
            # com.joliciel.talismane.posTagger.evaluate.PosTagLexicalCoverageTester
          ]
        }
      }
      
      parser {
        # The parser itself. Must extend com.joliciel.talismane.parser.Parser and have a constructor taking a TalismaneSession
        parser = com.joliciel.talismane.parser.TransitionBasedParser
        
        # The transition system used when no model has been provided: ArcEager or ShiftReduce
        transition-system = "ArcEager"
        
        # The list of possible dependency labels when no model has been provided
        dependency-labels = null

        # A path to the parser model
        model = null
        
        # Paths to files containing parser rules to apply
        rules = []
        
        # The beam width used by the parser
        beam-width = ${talismane.core.generic.beam-width}
        
        # should the pos-tagger beam be propagated to the parser
        propagate-pos-tagger-beam = true
        
        # How long we will attempt to parse a sentence before leaving the parse as is, in seconds. A value of 0 means the parsing will continue indefinitely.
        # If analysis jumps out because of time-out, there will be a parse-forest instead of a parse-tree,
        # with several nodes left unattached
        max-analysis-time = 60
        
        # The minimum amount of remaining free memory to continue a parse, in kilobytes.
        # Will be ignored is set to 0.
        # If analysis jumps out because of free memory descends below this limit,
        # there will be a parse-forest instead of a parse-tree,
        # with several nodes left unattached
        min-free-memory = 64

        ### During the beam search, how to determine which parse configurations to compare to each other in the same beam
        # * transitionCount: Comparison based on number of transitions applied.
        # * bufferSize: Comparison based on number of elements remaining on the buffer
        # * stackAndBufferSize: Comparison based on number of elements remaining on both the stack and the buffer.
        # * dependencyCount:  Comparison based on number of dependencies created.
        comparison-strategy = bufferSize
        
        # If true, we stop as soon as the beam contains n terminal configurations,
        # where n is the beam width, rather than waiting for all paths to end.
        early-stop = false
        
        output = ${talismane.core.generic.output} {
          # a list of processors to apply
          # - FreemarkerParseWriter: Output using a freemarker template.
          # - ParseFeatureTester: Test parse features on the training set.
          # - TransitionLogWriter: writes the list of transitions that were actually applied, one at a time.
          # Others may be defined, as long as:
          # - they're on the classpath,
          # - they implement com.joliciel.talismane.parser.output.ParseConfigurationProcessor,
          # - they implement a constructor with the signatures (Writer, TalismaneSession) for the main output,
          #    (File outDir, TalismaneSession) for additional output, or (TalismaneSession) for no output.
          # The first processor implementing (Writer, TalismaneSession) will write to the main output file.
          processors = [
            com.joliciel.talismane.parser.output.FreemarkerParseWriter
            # com.joliciel.talismane.parser.output.ParseFeatureTester
            # com.joliciel.talismane.parser.output.TransitionLogWriter
          ]
          
          # if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
          template = null
        }
            
        input = ${talismane.core.generic.input} {
          # the class used to read the training corpus
          corpus-reader = com.joliciel.talismane.parser.ParserRegexBasedCorpusReader

          # the regex used to read tokens from the evaluation corpus
          input-pattern = "%INDEX%\t%TOKEN%\t%LEMMA%\t%POSTAG%\t%CATEGORY%\t%MORPHOLOGY%\t%NON_PROJ_GOVERNOR%\t%NON_PROJ_LABEL%\t%GOVERNOR%\t%LABEL%"
          
          # Should an attempt be made to the predict the transitions that led to this configuration,
          # or should dependencies simply be added with null transitions. Outside of training, it is only
          # useful to predict transitions if we need the transition sequence, e.g. to check that the
          # given dependency tree is reachable via the selected transition system.
          predict-transitions = false
          
          # A path to a file containing a regex describing how to read a lexical entry from each line in the corpus
          # making it possible to maintain and/or transform morphosyntaxic attributes when processing a corpus
          corpus-lexical-entry-regex = null
   
          # if a sentence file is provided, the text of sentences represented by the tokenised input is provided by this file, one sentence per line.
          # allows Talismane to know the correct original spacing.
          sentence-file = null

          # A list of corpus transformation rules, which can change the value any element read from a pre-annotated input corpus
          # based an a conjunction of regexes matching any set of elements on the same line.
          # All rules will be matched prior to applying changes, and the first matching rule to change a particular element wins.
          # The set of possible elements are the same as those found in the input-pattern.
          corpus-rules = [
          ## for example, this rule states that any row where the POSTAG is two single quotes will be transformed into a double-quote, and assigned the POSTAG "P"
          #  {
          #    criteria = { POSTAG = "(``|'')" }
          #    actions = {
          #      POSTAG = "P"
          #      TOKEN = "\""
          #    }
          #  },
          ## This rule states taht any row where the POSTAG and TOKEN both match one of a set of values will have the POSTAG set to P
          #  {
          #    criteria = {
          #      POSTAG = "[,:.()]"
          #      TOKEN = "[,:.()]"
          #    }
          #    actions = { POSTAG = "P" }
          #  }
          ]
        }
        
        train = ${talismane.core.generic.parser.input} {
          # A path to a file containing parser feature descriptors
          features = null
          
          # When training, skip any sentences which cannot be parsed (rather than raising an exception)
          skip-impossible-sentences = true
          
          predict-transitions = true
          
          machine-learning = ${talismane.machine-learning}
        }
        
        evaluate = ${talismane.core.generic.parser.input} {
          # If true, takes both governor and dependency label into account when determining errors. If false, only the governor is taken into account.
          labeled-evaluation = true
          
          # If true, evaluate the projective labels/governors. If false, evaluate the non-projective labels/governors.
          projective = true
          
          # Label to be skipped when calculating distance f-scores - for arbitrary distance attachments in the case of punctuation
          skip-label = null
          
          # If included, will output in the transition log, at the end of each parse, the correct sentence and erroneous sentence
          # if the error concerned any one of these comma-separated dependency labels
          error-labels = []

          # If we do generate the output guesses, how many guesses (from the beam) to include per analysis
          output-guess-count = 1
          
          # Start-module: tokeniser, posTagger or parser
          # This allows us to evaluate a sequence of several modules in a row
          start-module = parser
          
          # A list of observers to apply when evaluating.
          # Others may be defined, as long as:
          # - they're on the classpath,
          # - they implement com.joliciel.talismane.parser.evaluate.ParseEvaluationObserver,
          # - they implement a constructor with the signatures (File outDir, TalismaneSession) for output,
          #   or (TalismaneSession) for no output.
          observers = [
            # to write f-scores of the various dependency labels
            com.joliciel.talismane.parser.evaluate.ParserFScoreCalculator
            
            # Generate the guessed output in a fairly compact format - respects the output-guess-count
            # com.joliciel.talismane.parser.evaluate.ParseEvaluationSentenceWriter
            
            # Calculates and writes the f-score for each separate governor to dependent distance during a parse evaluation
            # com.joliciel.talismane.parser.evaluate.ParserFScoreCalculatorByDistance
            
            # Output average time for sentences of different lengths, making it possible to see if analysis is performing in linear time
            # respects the skip-label
            # com.joliciel.talismane.parser.evaluate.ParseTimeByLengthObserver
            
            # Log of full transitions in the evaluation output - respects the error-labels
            # com.joliciel.talismane.parser.TransitionLogWriter
          ]
        }
      }
      
      csv {
        # Which character should separate cells in the CSV files generated
        separator = "\t"
      
        # Which encoding to generate CSV files in. If null will use the default encoding for the current OS.
        encoding = null
        
        # Which locale to use for CSV number formatting. If null will use the default locale for the current OS.
        locale = null
      }
      
      conll {
        # when reading/writing CoNLL format, should we replace spaces with underscores (e.g. in multi-word expressions)
        spaces-to-underscores = true
      }
    }
  }
}
