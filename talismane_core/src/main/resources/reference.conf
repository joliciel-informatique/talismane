talismane {
	core {
		# The main command to execute:
		# - train: Train a model using a corpus, a feature set, a classifier + parameters, etc.
		# - analyse: Analyse a corpus and add annotations.
		# - evaluate: Evaluate an annotated corpus, by re-analysing the corpus and comparing the new annotations to the existing ones.
		# - process: Process an annotated corpus - Talismane simply reads the corpus using the appropriate corpus reader
		#    and passes the results to the appropriate processors.
		# - compare: Compare two annotated corpora.
		command = analyse
		
		# Which mode to run in:
		# - normal: Command line mode, reading from standard in or file, and writing to standard out or file.
		# - server: Server listening on port, and processing input as it comes.
		mode = normal
		
		# In server mode, which port to listen on
		port = 7272
		
		# Modules are used by all commands
		# If provided, it will override startModule and endModule for analyse/evaluate/compare
		# It is required for train|process
		# - language|languageDetector
		# - sentence|sentenceDetector
		# - tokenise|tokeniser
		# - postag|posTagger
		# - parse|parser
		#module = null
		
		# if inFile and inDir are not specified, Talismane will read from STDIN
		# use inFile if a specific file is being read
		#in-file = null
		# use inDir if a full directory needs to be processed
		#in-dir = null
		
		# if outFile and outDir are not specified, Talismane will write to STDOUT
		# use outFile to write all output to a single file
		#out-file = null
		# use outDir to write output to a directory - in combination with inDir, will write one file out per file read in
			
		# If provided, will add this suffix to all output files (both in analysis and evaluation). Useful when comparing configurations.
		suffix = ""
		
		#suffix = null
				
		# if encoding is not provided, the default encoding for the system will be used
		#encoding = null
		
		# inputEncoding and outputEncoding can override encoding
		#input-encoding = null
		#output-encoding = null
		
		# The locale in which we are analysing - required if not provided in the language pack
		#locale = null
		
		### The list of lexicons to use
		# The lexicons are used to find lemmas and morphosyntaxic attributes for known words, and to feed various analysis features
		lexicons = []
		
		# The paths to various external word lists (can be either folders or files)
		# These are used within regex-based filters and annotators, as well as certain features
		word-lists = []
		
		### A path to a precompiled "diacriticizer", which adds diacritics to words in ALL UPPERCASE prior to analysis.
		# if left blank, a diacriticizer is automatically constructed from the lexicon, but this takes more time at startup.
		#diacriticizer = null
		
		# Path to a file giving lowercase preferences for all uppercase words with multiple diacriticized possibilities
		#lowercase-preferences = null
		
		# The paths to various external resources (can be either folders or files), used for building features
		external-resources = []
		
		
		analysis {
			# Annotators used to filter the raw text and transform it into text that needs to be processed
			text-filters = []
			
			# Annotators to be run before full analysis chain, but after raw text has been filtered
			pre-annotators = []
			
			# at which stage should analysis start
			start-module = sentence
			
			# at which stage should analysis end
			end-module = parser
			
			# Annotators to be run after full analysis chain
			post-annotators = []
		}
		
		sentence-detector {
			pre-annotators = []
			post-annotators = []
			
			# Path to the sentence detection model
			#model = null
		}
		
		tokeniser {
			pre-annotators = []
			post-annotators = []
			
			### The tokeniser type:
			# - simple: a deterministic tokeniser purely based on filters
			# - pattern: a probabilistic tokeniser using supervised machine learning to take tokenisation decisions in areas where certain patterns are matched
			type = "simple"
			
			# If using a probablistic tokeniser, the path to the tokeniser model
			#model = null
			
			### If using a pattern tokeniser, the method: Interval or Compound
			pattern-tokeniser-type = "Compound"
			
			beam-width = 1
			
			# should the tokeniser beam be propagated to the pos-tagger
			propagate-beam = false
			
			# A regex for reading previously tokenised input, with various placeholders.
			preannotated-input-pattern = ".+\t%TOKEN%"
			
			# if true, will spew out a file full of low-level analysis details for certain model types
			include-details = false
			
			training {
				# if a sentence reader is provided, the text of sentences represented by the tokenised input is provided by this reader, one sentence per line.
				# otherwise, their text for use in tokenisation features is automatically reconstructed from the manually tokenised input, and the spacing may be erroneous
				#sentenceReader = null
				
				# A path to a file containing tokeniser feature descriptors
				#features = null
				
				# A path to a file containing patterns indicating which areas need further testing by the pattern tokeniser
				#patterns = null
			}
		}
		
		pos-tagger {
			# The pos-tag set used when no model has been provided
			# pos-tag-set
			
			# A path to the pos-tagger model
			#model = null
		}
		
		parser {
			# The transition system used when no model has been provided: ArcEager or ShiftReduce
			transition-system = "ArcEager"
			
			# The list of possible dependency labels when no model has been provided
			#dependency-labels = null
			
			# A path to the parser model
			#model = null
		}
		
		analyse {
			# at which stage should analysis start
			startModule = sentence
			
			# at which stage should analysis end
			endModule = parser
			
			### Paths for finding the models
			#languageModel = null
			#sentenceModel = null
			#posTaggerModel = null
			#parserModel = null
			
			############################################
			### Parameters controlling how data is input
			# If one of these is provided, it will override the default handling of input
			# A regex for reading input, with various placeholders
			# The default for tokenised text (pos-tagger analysis or tokeniser evaluation) is: ".+\t%TOKEN%"
			# The default for pos-tagged text (parser analysis or pos-tagger evaluation) is: ".*\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t"
			# The default for parsed text (parser evaluation) is: "%INDEX%\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t%NON_PROJ_GOVERNOR%\t%NON_PROJ_LABEL%\t%GOVERNOR%\t%LABEL%"
			#inputPattern = null
			# File path to a regex
			#inputPatternFile = null
			
			### Paths to files containing filters
			# If the associated replace parameter is set to true, the model filters are replaced by this list, otherwise this list is added before the model filters
			textFilters = []
			tokenFilters = []
			tokenFiltersReplace = false
			tokenSequenceFilters = []
			tokenSequenceFiltersReplace = false
			posTagSequenceFilters = []
			posTagSequenceFiltersReplace = false
			
			# How to handle the newline character, options are SPACE (will be replaced by a space) and SENTENCE_BREAK (will break sentences)
			newline = SENTENCE_BREAK
			
			# If true, the input file is processed from the very start (e.g. TXT files). If false, we wait until a text filter tells us to start processing (e.g. XML files).
			processByDefault = true
			
			# A character (typically non-printing) which will mark a stop in the input stream and set-off analysis immediately. Must be a single character.
			endBlockCharCode = "\f"
			
			# The block size to use when applying filters - if a text filter regex goes beyond the blocksize, Talismane will fail.
			# This is required because Talismane input is based on streaming, and in order to ensure filters are applied, we have to know when to stop reading and apply filters.
			# A larger block size will avoid errors due to regex matches that are too large, but will mean longer waits before Talismane returns results when streaming.
			blockSize = 1000
			
			# The max sentences to process. If <= 0 will be ignored.
			sentenceCount = 0
			
			# The first sentence index to process. Earlier sentences will be skipped.
			startSentence = 0

			############################################
			### Parameters controlling how data is output
			# a built-in output template. Options include: default, with_location, with_prob, with_comments
			builtInTemplate = "default"
			
			# if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
			#template = null

			# if true, will spew out a file full of low-level analysis details for certain model types
			includeDetails = false
			
			### Paths to files containing rules
			posTaggerRules = []
			parserRules = []
			
			# A string to insert between sections marked for output (e.g. XML tags to be kept in the analysed output).
			# The String NEWLINE is interpreted as "\n". Otherwise, used literally.
			outputDivider = ""
			
			#############################################
			### Parameters controlling the analysis process itself
			
			### various parameters defining beam search behaviour
			# how wide should the beam be in the beam search - wider beams give better analysis but slow the analysis linearly
			beamWidth = 1
			
			# we can override the beamWidth for individual models, by setting these to positive values
			posTaggerBeamWidth = -1
			parserBeamWidth = -1

			# whether or not to propagate the beam from the pos-tagger to the parser, enabling the parser to correct pos-tagging errors
			propagateBeam = true
			
			# whether or not to propagate the beam from the tokeniser to the pos-tagger and parser, enabling them to correct tokenisation errors
			propagateTokeniserBeam = false
			
			# If true, feature descriptors will be converted into a single dynamically generated class
			dynamiseFeatures = false
			
			parser {
				# How long we will attempt to parse a sentence before leaving the parse as is, in seconds. A value of 0 means the parsing will continue indefinitely.
				# If analysis jumps out because of time-out, there will be a parse-forest instead of a parse-tree,
				# with several nodes left unattached
				maxAnalysisTime = 60
				
				# The minimum amount of remaining free memory to continue a parse, in kilobytes.
				# Will be ignored is set to 0.
				# If analysis jumps out because of free memory descends below this limit,
				# there will be a parse-forest instead of a parse-tree,
				# with several nodes left unattached
				minFreeMemory = 64

				### During the beam search, how to determine which parse configurations to compare to each other in the same beam
				# * transitionCount: Comparison based on number of transitions applied.
				# * bufferSize: Comparison based on number of elements remaining on the buffer
				# * stackAndBufferSize: Comparison based on number of elements remaining on both the stack and the buffer.
				# * dependencyCount:  Comparison based on number of dependencies created.
				comparisonStrategy = bufferSize
				
				# If true, we stop as soon as the beam contains n terminal configurations,
				# where n is the beam width, rather than waiting for all paths to end.
				earlyStop = false

			}
		}
		
		train {
			# The paths to various external resources (can be either folders or files), separated by semicolons
			externalResources = []
			
			languageDetector {
				# A path to a file containing language detector feature descriptors
				#features = null
				
				# A path to a file listing, for each language, the path to its corpus.
				# Format is locale code, followed by a tab, followed by the path to a file containing the corpus.
				#languageCorpusMap = null
			}
			
			sentenceDetector {
				# A path to a file containing sentence detector feature descriptors
				#features = null
			}
			
			tokeniser {
				# the regex used to read tokens from the training corpus
				readerRegex = ".+\t%TOKEN%"
				
				# if a sentence reader is provided, the text of sentences represented by the tokenised input is provided by this reader, one sentence per line.
				# otherwise, their text for use in tokenisation features is automatically reconstructed from the manually tokenised input, and the spacing may be erroneous
				#sentenceReader = null
				
				# A path to a file containing tokeniser feature descriptors
				#features = null
				
				# A path to a file containing patterns indicating which areas need further testing by the pattern tokeniser
				#patterns = null
			}
			
			posTagger {
				# the regex used to read tokens from the training corpus
				readerRegex = ".*\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t"
				
				# A path to a file containing pos-tagger feature descriptors
				#features = null
			}
			
			parser {
				# the regex used to read tokens from the training corpus
				readerRegex = "%INDEX%\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t%NON_PROJ_GOVERNOR%\t%NON_PROJ_LABEL%\t%GOVERNOR%\t%LABEL%"

				# A path to a file containing parser feature descriptors
				#features = null
			}
			
			# iterations at which the perceptron model should be saved
			# TODO: this really belongs in the machine-learning conf, and will be moved there eventually
			perceptronObservationPoints = []
		}
		
		evaluate {
			# The file to evaluate against the inputFile
			#evaluationFile = null
			
			# If one of these is provided, it will override the default handling of evaluation input
			# Note that this is distinct from analyse.inputPattern, which indicates how the "gold" file will be read
			# A regex for reading input, with various placeholders
			#evaluationPattern = null
			# File path to a regex
			#evaluationPatternFile = null
			
			csv {
				# Which character should separate cells in the CSV files generated
				separator = "\t"
			
				# Which encoding to generate CSV files in. If blank will use the default encoding for the current OS.
				#encoding = null
				
				# Which locale to use for CSV number formatting. If blank will use the default locale for the current OS.
				#locale = null
			}
			
			tokeniser {
				# the regex used to read tokens from the evaluation corpus
				readerRegex = ${talismane.core.train.tokeniser.readerRegex}
			}
			
			posTagger {
				# the regex used to read tokens from the evaluation corpus
				readerRegex = ${talismane.core.train.posTagger.readerRegex}

				# if true, will add files ending with "_unknown.csv" and "_known.csv" splitting pos-tagging f-scores into known and unknown words
				includeUnknownWordResults = false
				
				# if true, will add a file ending with ".lexiconCoverage.csv" giving lexicon word coverage
				includeLexiconCoverage = false
			}
			
			parser {
				# the regex used to read tokens from the evaluation corpus
				readerRegex = ${talismane.core.train.parser.readerRegex}

				# If true, takes both governor and dependency label into account when determining errors. If false, only the governor is taken into account.
				labeledEvaluation = true
				
				# if true, will output a file giving average time for sentences of different lengths, making it possible to see if analysis is performing in linear time
				includeTimePerToken = false
				
				### Distance f-score output
				# If true, calculates and writes the f-score for each separate governor to dependent distance during a parse evaluation
				includeDistanceFScores = false
				
				# Label to be skipped when calculating distance f-scores - for arbitrary distance attachments in the case of punctuation
				#skipLabel = null
				
				### Output the transitions applied
				# If true, writes the list of transitions that were actually applied, one at a time.
				includeTransitionLog = false
				
				# If included, will output in the transition log, at the end of each parse, the correct sentence and erroneous sentence
				# if the error concerned any one of these comma-separated dependency labels
				errorLabels = []
			}
			
			crossValidation {
				# If greater than 0, the input file will be split into foldCount folds
				# Typically, 10-fold validation is used, if cross-validation is desired
				foldCount = 0
				
				# Used when evaluating in cross-validation
				# If provided, only a single fold will be processed, numbered from 0 to foldCount -1
				includeIndex = 0
				
				# Used when training in cross-validation
				# If provided, one of the folds will be excluded, numbered from 0 to foldCount - 1
				excludeIndex = 0
			}
			
						
			# Whether or not a separate file should be generated including the guessed output in a fairly compact format
			outputGuesses = false
			# If we do generate the output guesses, how many guesses (from the beam) to include per analysis
			outputGuessCount = 1
		}
		
		process {
			# processing options:
			# - output: Simply output what you read, usually changing the format
			# - loadParsingConstraints: Load the parsing constraints from a training corpus.
			# - posTagFeatureTester: Test pos-tag features on a subset of words in the training set.
			# - parseFeatureTester: Test parse features on the training set.
			option = output
			
			# Should an attempt be made to the predict the transitions that led to this configuration,
			# or should dependencies simply be added with null transitions. Outside of training, it is only
			# useful to predict transitions if we need the transition sequence, e.g. to check that the
			# given dependency tree is reachable via the selected transition system.
			predictTransitions = false
			
			# A path to a file containing a regex describing how to read a lexical entry from each line in the corpus
			# making it possible to maintain and/or transform morphosyntaxic attributes when processing a corpus
			#corpusLexicalEntryRegex = null
			
			posTagFeatureTester {
				# a list of words to test pos-tag features on, other words will be skipped
				testWords = []
			}
		}
		
		# if set to true, the total time of talismane execution will be written to a separate file
		logStats = false
	}
}