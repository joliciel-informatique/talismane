talismane {
	core {
		# The main command to execute:
		# - train: Train a model using a corpus, a feature set, a classifier + parameters, etc.
		# - analyse: Analyse a corpus and add annotations.
		# - evaluate: Evaluate an annotated corpus, by re-analysing the corpus and comparing the new annotations to the existing ones.
		# - process: Process an annotated corpus - Talismane simply reads the corpus using the appropriate corpus reader
		#    and passes the results to the appropriate processors.
		# - compare: Compare two annotated corpora.
		command = analyse
		
		# Which mode to run in:
		# - normal: Command line mode, reading from standard in or file, and writing to standard out or file.
		# - server: Server listening on port, and processing input as it comes.
		mode = normal
		
		# In server mode, which port to listen on
		port = 7272
		
		# Modules are used by all commands
		# If provided, it will override startModule and endModule for analyse/evaluate/compare
		# It is required for train|process
		# - languageDetector
		# - sentenceDetector
		# - tokeniser
		# - posTagger
		# - parser
		module = parser
		
		# if inFile and inDir are not specified, Talismane will read from STDIN
		# use inFile if a specific file is being read
		#in-file = null
		# use inDir if a full directory needs to be processed
		#in-dir = null
		
		# if out-file and out-dir are not specified, Talismane will write to STDOUT
		# use out-file to write all output to a single file
		#out-file = null
		# use out-dir to write output to a directory - in combination with in-dir, will write one file out per file read in
		#out-dir = null
		
		# If provided, will add this suffix to all output files (both in analysis and evaluation). Useful when comparing configurations.
		suffix = ""
				
		# if encoding is not provided, the default encoding for the system will be used
		#encoding = null
		
		# inputEncoding and outputEncoding can override encoding
		#input-encoding = null
		#output-encoding = null
		
		# The locale in which we are analysing - required if not provided in the language pack
		#locale = null
		
		### The list of lexicons to use
		# The lexicons are used to find lemmas and morphosyntaxic attributes for known words, and to feed various analysis features
		lexicons = []
		
		# The paths to various external word lists (can be either folders or files)
		# These are used within regex-based filters and annotators, as well as certain features
		word-lists = []
		
		### A path to a precompiled "diacriticizer", which adds diacritics to words in ALL UPPERCASE prior to analysis.
		# if left blank, a diacriticizer is automatically constructed from the lexicon, but this takes more time at startup.
		#diacriticizer = null
		
		# Path to a file giving lowercase preferences for all uppercase words with multiple diacriticized possibilities
		#lowercase-preferences = null
		
		# The paths to various external resources (can be either folders or files), used for building features
		external-resources = []
		
		# The default beam width for all modules
		beam-width = 1
		
		# A character (typically non-printing) which will mark a stop in the input stream and set-off analysis immediately. Must be a single character.
		end-block-char-code = "\f"
		
		analysis {
			# Annotators used to filter the raw text and transform it into text that needs to be processed
			text-filters = []
			
			# Annotators to be run before full analysis chain, but after raw text has been filtered
			pre-annotators = []
			
			# at which stage should analysis start
			start-module = sentence
			
			# at which stage should analysis end
			end-module = parser
			
			# Annotators to be run after full analysis chain
			post-annotators = []
			
			# The block size to use when applying filters - if a text filter regex goes beyond the blocksize, Talismane will fail.
			# This is required because Talismane input is based on streaming, and in order to ensure filters are applied, we have to know when to stop reading and apply filters.
			# A larger block size will avoid errors due to regex matches that are too large, but will mean longer waits before Talismane returns results when streaming.
			block-size = 1000
			
			# How to handle the newline character, options are SPACE (will be replaced by a space) and SENTENCE_BREAK (will break sentences)
			newline = SENTENCE_BREAK
			
			# If true, the input file is processed from the very start (e.g. TXT files). If false, we wait until a text filter tells us to start processing (e.g. XML files).
			process-by-default = true
			
			# Should analysis stop if an analysis error occurs, or should it simply skip the sentence and continue
			stop-on-error = true
		}
		
		input {
			# The max sentences to process. If <= 0 will be ignored.
			sentence-count = 0
		
			# The index of the first sentence to process. Earlier sentences will be skipped.
			start-sentence = 0
			
			cross-validation {
				# If greater than 0, the input file will be split into foldCount folds
				# Typically, 10-fold validation is used, if cross-validation is desired
				fold-count = 0
				
				# Used when evaluating in cross-validation
				# If provided, only a single fold will be processed, numbered from 0 to foldCount -1
				include-index = 0
				
				# Used when training in cross-validation
				# If provided, one of the folds will be excluded, numbered from 0 to foldCount - 1
				exclude-index = 0
			}			
		}
			
		output {
			### Parameters controlling how data is output
			# a built-in output template. Options include: standard, with_location, with_prob, with_comments
			built-in-template = "standard"
			
			# A string to insert between sections marked for output (e.g. XML tags to be kept in the analysed output).
			# The String NEWLINE is interpreted as "\n". Otherwise, used literally.
			output-divider = ""
			
			# processing options:
			# - output: Simply output what you read, usually changing the format
			# - loadParsingConstraints: Load the parsing constraints from a training corpus.
			# - posTagFeatureTester: Test pos-tag features on a subset of words in the training set.
			# - parseFeatureTester: Test parse features on the training set.
			option = output
			
			# if set to true, the total time of talismane execution will be written to a separate file
			log-execution-time = false
			
			# if true, will spew out a file full of low-level analysis details for certain model types
			include-details = false
		}
		
		language-detector {
			# Path to the language detection model
			#model = null
			
			input = ${talismane.core.input} {
				# the class used to read the corpus
				corpus-reader = com.joliciel.talismane.languageDetector.TextPerLineCorpusReader
			}
			
			train = ${talismane.core.language-detector.input} {				
				# A path to a file containing sentence-detector feature descriptors
				#features = null
				
				machine-learning = ${talismane.machine-learning}
				
				# A path to a file listing, for each language, the path to its corpus.
				# Format is locale code, followed by a tab, followed by the path to a file containing the corpus.
				#language-corpus-map = null
			}
		}
		
		sentence-detector {
			pre-annotators = []

			# Path to the sentence detection model
			#model = null
			
			output = ${talismane.core.output} {
				# if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
				#template = null
			}
			
			input = ${talismane.core.input} {
				# the class used to read the training corpus
				corpus-reader = com.joliciel.talismane.sentenceDetector.SentencePerLineCorpusReader
			}
			
			train = ${talismane.core.sentence-detector.input} {				
				# A path to a file containing sentence-detector feature descriptors
				#features = null
				
				machine-learning = ${talismane.machine-learning}
			}
			
			evaluate = ${talismane.core.sentence-detector.input} {
				# The file to evaluate against the in-file
				#eval-file = null
			}
		}
		
		tokeniser {
			pre-annotators = []
			token-sequence-filters = []
			
			### The tokeniser type:
			# - simple: a deterministic tokeniser purely based on filters
			# - pattern: a probabilistic tokeniser using supervised machine learning to take tokenisation decisions in areas where certain patterns are matched
			type = "simple"
			
			# If using a pattern tokeniser, the path to the tokeniser model
			#model = null
						
			# The beam width used by the tokeniser
			beam-width = 1
						
			output = ${talismane.core.output} {
				# if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
				#template = null
			}
			
			input = ${talismane.core.input} {
				# the class used to read the training corpus
				corpus-reader = com.joliciel.talismane.tokeniser.TokenRegexBasedCorpusReader

				# the regex used to read tokens from the evaluation corpus
				preannotated-pattern = ".+\t%TOKEN%"
				
				pre-annotators = ${talismane.core.tokeniser.pre-annotators}
				token-sequence-filters = ${talismane.core.tokeniser.token-sequence-filters}
				
				# if a sentence file is provided, the text of sentences represented by the tokenised input is provided by this file, one sentence per line.
				# otherwise, their text for use in tokenisation features is automatically reconstructed from the manually tokenised input, and the spacing may be erroneous
				#sentence-file = null
			}
			
			train = ${talismane.core.tokeniser.input} {				
				# A path to a file containing tokeniser feature descriptors
				#features = null
				
				# A path to a file containing patterns indicating which areas need further testing by the pattern tokeniser
				#patterns = null
				
				machine-learning = ${talismane.machine-learning}
			}
			
			evaluate = ${talismane.core.tokeniser.input} {
				# The file to evaluate against the in-file
				#eval-file = null
			}
		}
		
		pos-tagger {
			# The pos-tag set used when no model has been provided
			# pos-tag-set
			
			pre-annotators = []
			token-sequence-filters = []
			postag-sequence-filters = []
			
			# A path to the pos-tagger model
			#model = null
			
			# Paths to files containing pos-tagger rules to apply
			rules = []
			
			# The beam width used by the pos-tagger
			beam-width = ${talismane.core.beam-width}
			
			# should the tokeniser beam be propagated to the pos-tagger
			propagate-tokeniser-beam = false
			
			output = ${talismane.core.output} {
				# if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
				#template = null
				
				# a list of words to test pos-tag features on, other words will be skipped
				test-words = []
			}
			
			input = ${talismane.core.input} {
				# the class used to read the training corpus
				corpus-reader = com.joliciel.talismane.posTagger.PosTagRegexBasedCorpusReader

				# the regex used to read tokens from the evaluation corpus
				preannotated-pattern = ".*\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t"
				
				# A path to a file containing a regex describing how to read a lexical entry from each line in the corpus
				# making it possible to maintain and/or transform morphosyntaxic attributes when processing a corpus
				#corpus-lexical-entry-regex = null
				
				pre-annotators = ${talismane.core.pos-tagger.pre-annotators}
				token-sequence-filters = ${talismane.core.pos-tagger.token-sequence-filters}
				postag-sequence-filters = ${talismane.core.pos-tagger.postag-sequence-filters}
			}
			
			train = ${talismane.core.pos-tagger.input} {
				# A path to a file containing pos-tagger feature descriptors
				#features = null
				
				machine-learning = ${talismane.machine-learning}
			}
			
			evaluate = ${talismane.core.pos-tagger.input} {
				# The file to evaluate against the in-file
				#eval-file = null
				
				# if true, will add files ending with "_unknown.csv" and "_known.csv" splitting pos-tagging f-scores into known and unknown words
				include-unknown-word-results = false
				
				# if true, will add a file ending with ".lexiconCoverage.csv" giving lexicon word coverage
				include-lexicon-coverage = false
				
				# Whether or not a separate file should be generated including the guessed output in a fairly compact format
				output-guesses = false
				# If we do generate the output guesses, how many guesses (from the beam) to include per analysis
				output-guess-count = 1
				
				# Start-module: tokeniser or pos-tagger
				start-module = ${talismane.core.module}
			}
		}
		
		parser {
			# The transition system used when no model has been provided: ArcEager or ShiftReduce
			transition-system = "ArcEager"
			
			# The list of possible dependency labels when no model has been provided
			#dependency-labels = null
			
			pre-annotators = []
			token-sequence-filters = []
			postag-sequence-filters = []

			# A path to the parser model
			#model = null
			
			# Paths to files containing parser rules to apply
			rules = []
			
			# The beam width used by the parser
			beam-width = ${talismane.core.beam-width}
			
			# should the pos-tagger beam be propagated to the parser
			propagate-pos-tagger-beam = true
			
			# How long we will attempt to parse a sentence before leaving the parse as is, in seconds. A value of 0 means the parsing will continue indefinitely.
			# If analysis jumps out because of time-out, there will be a parse-forest instead of a parse-tree,
			# with several nodes left unattached
			max-analysis-time = 60
			
			# The minimum amount of remaining free memory to continue a parse, in kilobytes.
			# Will be ignored is set to 0.
			# If analysis jumps out because of free memory descends below this limit,
			# there will be a parse-forest instead of a parse-tree,
			# with several nodes left unattached
			min-free-memory = 64

			### During the beam search, how to determine which parse configurations to compare to each other in the same beam
			# * transitionCount: Comparison based on number of transitions applied.
			# * bufferSize: Comparison based on number of elements remaining on the buffer
			# * stackAndBufferSize: Comparison based on number of elements remaining on both the stack and the buffer.
			# * dependencyCount:  Comparison based on number of dependencies created.
			comparison-strategy = bufferSize
			
			# If true, we stop as soon as the beam contains n terminal configurations,
			# where n is the beam width, rather than waiting for all paths to end.
			early-stop = false
			
			output = ${talismane.core.output} {
				# if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
				#template = null
				
				### Output the transitions applied
				# If true, writes the list of transitions that were actually applied, one at a time.
				include-transition-log = false
			}
			
			# If true, feature descriptors will be converted into a single dynamically generated class
			# possibly affecting performance (though current testing does not indicate this)
			dynamise-features = false
					
			input = ${talismane.core.input} {
				# the class used to read the training corpus
				corpus-reader = com.joliciel.talismane.parser.ParserRegexBasedCorpusReader

				# the regex used to read tokens from the evaluation corpus
				preannotated-pattern = "%INDEX%\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t%NON_PROJ_GOVERNOR%\t%NON_PROJ_LABEL%\t%GOVERNOR%\t%LABEL%"
				
				# Should an attempt be made to the predict the transitions that led to this configuration,
				# or should dependencies simply be added with null transitions. Outside of training, it is only
				# useful to predict transitions if we need the transition sequence, e.g. to check that the
				# given dependency tree is reachable via the selected transition system.
				# Choices are: yes, no and depends, where depends means yes for training and no otherwise.
				predict-transitions = depends
				
				# A path to a file containing a regex describing how to read a lexical entry from each line in the corpus
				# making it possible to maintain and/or transform morphosyntaxic attributes when processing a corpus
				#corpus-lexical-entry-regex = null
				
				pre-annotators = ${talismane.core.parser.pre-annotators}
				token-sequence-filters = ${talismane.core.parser.token-sequence-filters}
				postag-sequence-filters = ${talismane.core.parser.postag-sequence-filters}
			}
			
			train = ${talismane.core.parser.input} {
				# A path to a file containing parser feature descriptors
				#features = null
				
				machine-learning = ${talismane.machine-learning}
			}
			
			evaluate = ${talismane.core.parser.input} {
				# The file to evaluate against the in-file
				#eval-file = null
				
				# If true, takes both governor and dependency label into account when determining errors. If false, only the governor is taken into account.
				labeled-evaluation = true
				
				# if true, will output a file giving average time for sentences of different lengths, making it possible to see if analysis is performing in linear time
				include-time-per-token = false
				
				### Distance f-score output
				# If true, calculates and writes the f-score for each separate governor to dependent distance during a parse evaluation
				include-distance-fscores = false
				
				# Label to be skipped when calculating distance f-scores - for arbitrary distance attachments in the case of punctuation
				#skip-label = null
				
				# If included, will output in the transition log, at the end of each parse, the correct sentence and erroneous sentence
				# if the error concerned any one of these comma-separated dependency labels
				error-labels = []
				
				# Whether or not a separate file should be generated including the guessed output in a fairly compact format
				output-guesses = false
				# If we do generate the output guesses, how many guesses (from the beam) to include per analysis
				output-guess-count = 1
				
				# Start-module: tokeniser, pos-tagger or parser
				start-module = ${talismane.core.module}
			}
		}
		
		csv {
			# Which character should separate cells in the CSV files generated
			separator = "\t"
		
			# Which encoding to generate CSV files in. If blank will use the default encoding for the current OS.
			#encoding = null
			
			# Which locale to use for CSV number formatting. If blank will use the default locale for the current OS.
			#locale = null
		}
		
		conll {
			# when reading/writing CoNLL format, should we replace spaces with underscores (e.g. in multi-word expressions)
			spaces-to-underscores = true
		}
	}
}
