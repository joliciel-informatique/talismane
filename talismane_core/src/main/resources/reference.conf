talismane {
  core {
    # The main command to execute:
    # - train: Train a model using a corpus, a feature set, a classifier + parameters, etc.
    # - analyse: Analyse a corpus and add annotations.
    # - evaluate: Evaluate an annotated corpus, by re-analysing the corpus and comparing the new annotations to the existing ones.
    # - process: Process an annotated corpus - Talismane simply reads the corpus using the appropriate corpus reader
    #    and passes the results to the appropriate processors.
    # - compare: Compare two annotated corpora.
    command = analyse
    
    # Which mode to run in:
    # - normal: Command line mode, reading from standard in or file, and writing to standard out or file.
    # - server: Server listening on port, and processing input as it comes.
    mode = normal
    
    # In server mode, which port to listen on
    port = 7272
    
    # Modules are used by all commands
    # If provided, it will override startModule and endModule for analyse/evaluate/compare
    # It is required for train|process
    # - languageDetector
    # - sentenceDetector
    # - tokeniser
    # - posTagger
    # - parser
    # module = null
    
    # gives the base name for output files, if not overridden in code
    base-name = "talismane"
    
    # If provided, will add this suffix to all output files (both in analysis and evaluation). Useful when comparing configurations.
    suffix = ""
        
    # if encoding is not provided, the default encoding for the system will be used
    #encoding = null
    
    # inputEncoding and outputEncoding can override encoding
    #input-encoding = null
    #output-encoding = null
    
    # The locale in which we are analysing - required if not provided in the language pack
    #locale = null
    
    ### The list of lexicons to use
    # The lexicons are used to find lemmas and morphosyntaxic attributes for known words, and to feed various analysis features
    # The list can either be zip files with the ".zip" extension (assumed to be lexicons pre-serialized with the serializeLexicon command)
    # or text files (assumed to be on the local file system, and to represent lexicon properties files).
    lexicons = []
    
    # The paths to various external word lists (can be either folders or files)
    # These are used within regex-based filters and annotators, as well as certain features
    word-lists = []
    
    ### A path to a precompiled "diacriticizer", which adds diacritics to words in ALL UPPERCASE prior to analysis.
    # if left blank, a diacriticizer is automatically constructed from the lexicon, but this takes more time at startup.
    #diacriticizer = null
    
    # Path to a file giving lowercase preferences for all uppercase words with multiple diacriticized possibilities
    #lowercase-preferences = null
    
    # The paths to various external resources (can be either folders or files), used for building features
    external-resources = []
    
    # The default beam width for all modules
    beam-width = 1
    
    # A character (typically non-printing) which will mark a stop in the input stream and set-off analysis immediately. Must be a single character.
    end-block-char-code = "\f"
    
    # The block size to use when applying filters - if a text filter regex goes beyond the blocksize, Talismane will fail.
    # This is required because Talismane input is based on streaming, and in order to ensure filters are applied, we have to know when to stop reading and apply filters.
    # A larger block size will avoid errors due to regex matches that are too large, but will mean longer waits before Talismane returns results when streaming.
    block-size = 1000
    
    # How to handle the newline character, options are SPACE (will be replaced by a space) and SENTENCE_BREAK (will break sentences)
    newline = SENTENCE_BREAK
    
    # The different types of annotators need to be applied in specific points of execution,
    # depending on the command - they are applied at different stages depending on which modules
    # are called and when.
    # Specifically, when training, evaluating or simply analysing, they apply to different corpus-readers.
    # All annotators are simply lists of file references - the files contain "descriptors" in a format
    # specific to each annotator type.
    # It is important when training a model to distribute the model with the same annotators are were
    # applied to train it, as referenced by an appropriate configuration file.
    annotators {
      # Used to filter the raw text and transform it into text that can be processed
      # Among other things, text filters:
      # - tell us which sections to process and which sections to skip (e.g. when processing XML)
      # - fix encoding/escaping (e.g. &quot; transformed into a quote)
      # - impose sentence breaks (e.g. on double newlines, or on certain XML tags)
      # - indicate that a sentence-boundary cannot occur within a certain word (e.g. Mr. Smith)
      # - remove duplicate white space
      text-annotators = []
      
      # Used to prepare a sentence for tokenisation and pos-tagging. In particular:
      # - Mark deterministic token boundaries (e.g. around "aujourd'hui" in French)
      # - Indicate the analysis text when different from raw text (e.g. replace the number 12 by "#NUMBER#")
      # - Impose pos-tags on a particular word
      # - Add arbitrary tags to the text, which are inherited by the tokens and handled by downstream systems
      sentence-annotators = []
    }
    
    analysis {
      # at which stage should analysis start
      start-module = sentenceDetector
      
      # at which stage should analysis end
      end-module = parser
            
      # If true, the input file is processed from the very start (e.g. TXT files). If false, we wait until a text filter tells us to start processing (e.g. XML files).
      process-by-default = true
      
      # Should analysis stop if an analysis error occurs, or should it simply skip the sentence and continue
      stop-on-error = true
      
      # If false, will only take into account the output for the end module
      # If true, will take into account output for all modules
      output-intermediate-modules = false
    }
    
    input {
      # The max sentences to process. If <= 0 will be ignored.
      sentence-count = 0
    
      # The index of the first sentence to process. Earlier sentences will be skipped.
      start-sentence = 0
      
      cross-validation {
        # If greater than 0, the input file will be split into foldCount folds
        # Typically, 10-fold validation is used, if cross-validation is desired
        fold-count = -1
        
        # Used when evaluating in cross-validation
        # If provided, only a single fold will be processed, numbered from 0 to foldCount -1
        include-index = -1
        
        # Used when training in cross-validation
        # If provided, one of the folds will be excluded, numbered from 0 to foldCount - 1
        exclude-index = -1
      }      
    }
      
    output {
      ### Parameters controlling how data is output
      # a built-in output template. Options include: standard, with_location, with_prob, with_comments
      built-in-template = "standard"
      
      # A string to insert between sections marked for output (e.g. XML tags to be kept in the analysed output).
      # The String NEWLINE is interpreted as "\n". Otherwise, used literally.
      output-divider = ""
      
      # if set to true, the total time of talismane execution will be written to a separate file
      log-execution-time = false
      
      # if true, will spew out a file full of low-level analysis details for certain model types
      include-details = false
    }
    
    language-detector {
      # Path to the language detection model
      #model = null
      
      input = ${talismane.core.input} {
        # the class used to read the corpus
        corpus-reader = com.joliciel.talismane.languageDetector.TextPerLineCorpusReader
      }
      
      train = ${talismane.core.language-detector.input} {        
        # A path to a file containing sentence-detector feature descriptors
        #features = null
        
        machine-learning = ${talismane.machine-learning}
        
        # A path to a file listing, for each language, the path to its corpus.
        # Format is locale code, followed by a tab, followed by the path to a file containing the corpus.
        #language-corpus-map = null
      }
    }
    
    sentence-detector {
      # Path to the sentence detection model
      #model = null
      
      output = ${talismane.core.output} {
        # if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
        #template = null
      }
      
      input = ${talismane.core.input} {
        # the class used to read the training corpus
        corpus-reader = com.joliciel.talismane.sentenceDetector.SentencePerLineCorpusReader
      }
      
      train = ${talismane.core.sentence-detector.input} {        
        # A path to a file containing sentence-detector feature descriptors
        #features = null
        
        machine-learning = ${talismane.machine-learning}
      }
      
      evaluate = ${talismane.core.sentence-detector.input} {
      }
    }
    
    tokeniser {
      # A regex giving default separators for tokens.
      # Note that Unicode character class will be applied
      separators = "[\\s\\p{Punct}«»\\_‒–—―‛“”„‟′″‴‹›‘’‚\\*\ufeff]"
      
      ### The tokeniser type:
      # - simple: a deterministic tokeniser purely based on filters
      # - pattern: a probabilistic tokeniser using supervised machine learning to take tokenisation decisions in areas where certain patterns are matched
      type = "simple"
      
      # If using a pattern tokeniser, the path to the tokeniser model
      #model = null
            
      # The beam width used by the tokeniser
      beam-width = 1
            
      output = ${talismane.core.output} {
        # if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
        #template = null
      }
      
      input = ${talismane.core.input} {
        # the class used to read the training corpus
        corpus-reader = com.joliciel.talismane.tokeniser.TokenRegexBasedCorpusReader

        # the regex used to read tokens from the evaluation corpus
        preannotated-pattern = ".+\t%TOKEN%"
          
        # if a sentence file is provided, the text of sentences represented by the tokenised input is provided by this file, one sentence per line.
        # otherwise, their text for use in tokenisation features is automatically reconstructed from the manually tokenised input, and the spacing may be erroneous
        #sentence-file = null
      }
      
      train = ${talismane.core.tokeniser.input} {        
        # A path to a file containing tokeniser feature descriptors
        #features = null
        
        # A path to a file containing patterns indicating which areas need further testing by the pattern tokeniser
        #patterns = null
        
        machine-learning = ${talismane.machine-learning}
      }
      
      evaluate = ${talismane.core.tokeniser.input} {
      }
    }
    
    pos-tagger {
      # The pos-tag set used when no model has been provided
      # pos-tag-set
        
      # A path to the pos-tagger model
      #model = null
      
      # Paths to files containing pos-tagger rules to apply
      rules = []
      
      # The beam width used by the pos-tagger
      beam-width = ${talismane.core.beam-width}
      
      # should the tokeniser beam be propagated to the pos-tagger
      propagate-tokeniser-beam = false
      
      output = ${talismane.core.output} {
        # a list of processors to apply
        # - output: Simply output what you read, usually changing the format
        # - posTagFeatureTester: Test pos-tag features on a subset of words in the training set.
        processors = [ output ]
      
        # if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
        #template = null
        
        # a list of words to test pos-tag features on, other words will be skipped
        test-words = []
      }
      
      input = ${talismane.core.input} {
        # the class used to read the training corpus
        corpus-reader = com.joliciel.talismane.posTagger.PosTagRegexBasedCorpusReader

        # the regex used to read tokens from the evaluation corpus
        preannotated-pattern = ".*\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t"
        
        # A path to a file containing a regex describing how to read a lexical entry from each line in the corpus
        # making it possible to maintain and/or transform morphosyntaxic attributes when processing a corpus
        #corpus-lexical-entry-regex = null
        
        # if a sentence file is provided, the text of sentences represented by the tokenised input is provided by this file, one sentence per line.
        # allows Talismane to know the correct original spacing.
        #sentence-file = null
      }
      
      train = ${talismane.core.pos-tagger.input} {
        # A path to a file containing pos-tagger feature descriptors
        #features = null
        
        machine-learning = ${talismane.machine-learning}
      }
      
      evaluate = ${talismane.core.pos-tagger.input} {
        # if true, will add files ending with "_unknown.csv" and "_known.csv" splitting pos-tagging f-scores into known and unknown words
        include-unknown-word-results = false
        
        # if true, will add a file ending with ".lexiconCoverage.csv" giving lexicon word coverage
        include-lexicon-coverage = false
        
        # Whether or not a separate file should be generated including the guessed output in a fairly compact format
        output-guesses = false
        # If we do generate the output guesses, how many guesses (from the beam) to include per analysis
        output-guess-count = 1
        
        # Start-module: tokeniser or posTagger
        # This allows us to evaluate a sequence of several modules in a row
        start-module = posTagger
      }
    }
    
    parser {
      # The transition system used when no model has been provided: ArcEager or ShiftReduce
      transition-system = "ArcEager"
      
      # The list of possible dependency labels when no model has been provided
      #dependency-labels = null

      # A path to the parser model
      #model = null
      
      # Paths to files containing parser rules to apply
      rules = []
      
      # The beam width used by the parser
      beam-width = ${talismane.core.beam-width}
      
      # should the pos-tagger beam be propagated to the parser
      propagate-pos-tagger-beam = true
      
      # How long we will attempt to parse a sentence before leaving the parse as is, in seconds. A value of 0 means the parsing will continue indefinitely.
      # If analysis jumps out because of time-out, there will be a parse-forest instead of a parse-tree,
      # with several nodes left unattached
      max-analysis-time = 60
      
      # The minimum amount of remaining free memory to continue a parse, in kilobytes.
      # Will be ignored is set to 0.
      # If analysis jumps out because of free memory descends below this limit,
      # there will be a parse-forest instead of a parse-tree,
      # with several nodes left unattached
      min-free-memory = 64

      ### During the beam search, how to determine which parse configurations to compare to each other in the same beam
      # * transitionCount: Comparison based on number of transitions applied.
      # * bufferSize: Comparison based on number of elements remaining on the buffer
      # * stackAndBufferSize: Comparison based on number of elements remaining on both the stack and the buffer.
      # * dependencyCount:  Comparison based on number of dependencies created.
      comparison-strategy = bufferSize
      
      # If true, we stop as soon as the beam contains n terminal configurations,
      # where n is the beam width, rather than waiting for all paths to end.
      early-stop = false
      
      output = ${talismane.core.output} {
        # a list of processors to apply
        # - output: Simply output what you read, usually changing the format
        # - parseFeatureTester: Test parse features on the training set.
        # - transitionLog: writes the list of transitions that were actually applied, one at a time.
        processors = [output]
        
        # if provided, overrides the builtInTemplate by a specific a FreeMarker template for writing the output
        #template = null
      }
          
      input = ${talismane.core.input} {
        # the class used to read the training corpus
        corpus-reader = com.joliciel.talismane.parser.ParserRegexBasedCorpusReader

        # the regex used to read tokens from the evaluation corpus
        preannotated-pattern = "%INDEX%\t%TOKEN%\t.*\t%POSTAG%\t.*\t.*\t%NON_PROJ_GOVERNOR%\t%NON_PROJ_LABEL%\t%GOVERNOR%\t%LABEL%"
        
        # Should an attempt be made to the predict the transitions that led to this configuration,
        # or should dependencies simply be added with null transitions. Outside of training, it is only
        # useful to predict transitions if we need the transition sequence, e.g. to check that the
        # given dependency tree is reachable via the selected transition system.
        predict-transitions = false
        
        # A path to a file containing a regex describing how to read a lexical entry from each line in the corpus
        # making it possible to maintain and/or transform morphosyntaxic attributes when processing a corpus
        #corpus-lexical-entry-regex = null
 
        # if a sentence file is provided, the text of sentences represented by the tokenised input is provided by this file, one sentence per line.
        # allows Talismane to know the correct original spacing.
        #sentence-file = null
      }
      
      train = ${talismane.core.parser.input} {
        # A path to a file containing parser feature descriptors
        #features = null
        
        # When training, skip any sentences which cannot be parsed (rather than raising an exception)
        skip-impossible-sentences = true
        
        predict-transitions = true
        
        machine-learning = ${talismane.machine-learning}
      }
      
      evaluate = ${talismane.core.parser.input} {
        # If true, takes both governor and dependency label into account when determining errors. If false, only the governor is taken into account.
        labeled-evaluation = true
        
        # if true, will output a file giving average time for sentences of different lengths, making it possible to see if analysis is performing in linear time
        include-time-per-token = false
        
        ### Distance f-score output
        # If true, calculates and writes the f-score for each separate governor to dependent distance during a parse evaluation
        include-distance-fscores = false
        
        # Label to be skipped when calculating distance f-scores - for arbitrary distance attachments in the case of punctuation
        #skip-label = null
        
        # Whether or not to include a limited log of full transitions in the evaluation output
        include-transition-log = false
        
        # If included, will output in the transition log, at the end of each parse, the correct sentence and erroneous sentence
        # if the error concerned any one of these comma-separated dependency labels
        error-labels = []
        
        # Whether or not a separate file should be generated including the guessed output in a fairly compact format
        output-guesses = false
        # If we do generate the output guesses, how many guesses (from the beam) to include per analysis
        output-guess-count = 1
        
        # Start-module: tokeniser, posTagger or parser
        # This allows us to evaluate a sequence of several modules in a row
        start-module = parser
      }
    }
    
    csv {
      # Which character should separate cells in the CSV files generated
      separator = "\t"
    
      # Which encoding to generate CSV files in. If blank will use the default encoding for the current OS.
      #encoding = null
      
      # Which locale to use for CSV number formatting. If blank will use the default locale for the current OS.
      #locale = null
    }
    
    conll {
      # when reading/writing CoNLL format, should we replace spaces with underscores (e.g. in multi-word expressions)
      spaces-to-underscores = true
    }
  }
}
